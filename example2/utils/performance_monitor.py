#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ç›‘æ§å·¥å…·
ç›‘æ§å†…å­˜ä½¿ç”¨ã€å¤„ç†æ—¶é—´å’Œç³»ç»Ÿèµ„æº
"""

import time
import psutil
import threading
from typing import Dict, Any, Optional, Callable
from dataclasses import dataclass, field
from contextlib import contextmanager


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    duration: Optional[float] = None
    
    # å†…å­˜æŒ‡æ ‡
    memory_start: float = 0.0
    memory_peak: float = 0.0
    memory_end: float = 0.0
    
    # CPUæŒ‡æ ‡
    cpu_percent: float = 0.0
    
    # è‡ªå®šä¹‰æŒ‡æ ‡
    custom_metrics: Dict[str, Any] = field(default_factory=dict)
    
    def finish(self):
        """å®Œæˆæ€§èƒ½æµ‹é‡"""
        self.end_time = time.time()
        self.duration = self.end_time - self.start_time
        self.memory_end = self._get_memory_usage()
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, name: str = "Unknown", auto_start: bool = True):
        self.name = name
        self.metrics = PerformanceMetrics()
        self.is_monitoring = False
        self.monitor_thread = None
        self.callbacks = []
        
        if auto_start:
            self.start()
    
    def start(self):
        """å¼€å§‹ç›‘æ§"""
        if self.is_monitoring:
            return
        
        self.metrics = PerformanceMetrics()
        self.metrics.memory_start = self.metrics._get_memory_usage()
        self.metrics.memory_peak = self.metrics.memory_start
        
        self.is_monitoring = True
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        
        print(f"ğŸ” å¼€å§‹æ€§èƒ½ç›‘æ§: {self.name}")
    
    def stop(self) -> PerformanceMetrics:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»“æœ"""
        if not self.is_monitoring:
            return self.metrics
        
        self.is_monitoring = False
        self.metrics.finish()
        
        # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=1.0)
        
        # è°ƒç”¨å›è°ƒå‡½æ•°
        for callback in self.callbacks:
            try:
                callback(self.metrics)
            except Exception as e:
                print(f"æ€§èƒ½ç›‘æ§å›è°ƒé”™è¯¯: {e}")
        
        self._print_summary()
        return self.metrics
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.is_monitoring:
            try:
                # æ›´æ–°å†…å­˜å³°å€¼
                current_memory = self.metrics._get_memory_usage()
                self.metrics.memory_peak = max(self.metrics.memory_peak, current_memory)
                
                # æ›´æ–°CPUä½¿ç”¨ç‡
                self.metrics.cpu_percent = psutil.cpu_percent(interval=None)
                
                time.sleep(0.1)  # 100msé—´éš”
                
            except Exception as e:
                print(f"æ€§èƒ½ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                break
    
    def add_custom_metric(self, key: str, value: Any):
        """æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡"""
        self.metrics.custom_metrics[key] = value
    
    def add_callback(self, callback: Callable[[PerformanceMetrics], None]):
        """æ·»åŠ æ€§èƒ½ç›‘æ§å®Œæˆå›è°ƒ"""
        self.callbacks.append(callback)
    
    def _print_summary(self):
        """æ‰“å°æ€§èƒ½æ€»ç»“"""
        print(f"\nğŸ“Š æ€§èƒ½ç›‘æ§ç»“æœ: {self.name}")
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {self.metrics.duration:.2f}ç§’")
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: å¼€å§‹{self.metrics.memory_start:.1f}MB, "
              f"å³°å€¼{self.metrics.memory_peak:.1f}MB, "
              f"ç»“æŸ{self.metrics.memory_end:.1f}MB")
        print(f"ğŸ–¥ï¸  å¹³å‡CPU: {self.metrics.cpu_percent:.1f}%")
        
        if self.metrics.custom_metrics:
            print("ğŸ“ˆ è‡ªå®šä¹‰æŒ‡æ ‡:")
            for key, value in self.metrics.custom_metrics.items():
                print(f"   {key}: {value}")
        print()


@contextmanager
def performance_monitor(name: str = "Operation", 
                       print_summary: bool = True,
                       callback: Optional[Callable[[PerformanceMetrics], None]] = None):
    """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    monitor = PerformanceMonitor(name, auto_start=False)
    
    if callback:
        monitor.add_callback(callback)
    
    try:
        monitor.start()
        yield monitor
    finally:
        metrics = monitor.stop()
        if not print_summary:
            # å¦‚æœä¸æ‰“å°æ€»ç»“ï¼Œè‡³å°‘æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            print(f"âœ… {name} å®Œæˆ: {metrics.duration:.2f}ç§’, "
                  f"å†…å­˜å³°å€¼: {metrics.memory_peak:.1f}MB")


class FileProcessingMonitor(PerformanceMonitor):
    """æ–‡ä»¶å¤„ç†ä¸“ç”¨ç›‘æ§å™¨"""
    
    def __init__(self, file_path: str, file_size: Optional[int] = None):
        self.file_path = file_path
        self.file_size = file_size or self._get_file_size()
        super().__init__(f"æ–‡ä»¶å¤„ç†: {Path(file_path).name}")
    
    def _get_file_size(self) -> int:
        """è·å–æ–‡ä»¶å¤§å°"""
        try:
            from pathlib import Path
            return Path(self.file_path).stat().st_size
        except Exception:
            return 0
    
    def update_progress(self, processed_bytes: int):
        """æ›´æ–°å¤„ç†è¿›åº¦"""
        if self.file_size > 0:
            progress = (processed_bytes / self.file_size) * 100
            self.add_custom_metric("progress_percent", progress)
            self.add_custom_metric("processed_bytes", processed_bytes)
    
    def _print_summary(self):
        """æ‰“å°æ–‡ä»¶å¤„ç†æ€»ç»“"""
        super()._print_summary()
        
        if self.file_size > 0:
            throughput = self.file_size / (1024 * 1024) / self.metrics.duration  # MB/s
            print(f"ğŸ“ æ–‡ä»¶å¤§å°: {self.file_size / (1024 * 1024):.1f}MB")
            print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {throughput:.1f}MB/s")


def benchmark_function(func: Callable, *args, **kwargs) -> tuple:
    """
    å¯¹å‡½æ•°è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    
    Returns:
        (å‡½æ•°è¿”å›å€¼, æ€§èƒ½æŒ‡æ ‡)
    """
    with performance_monitor(f"å‡½æ•°: {func.__name__}") as monitor:
        result = func(*args, **kwargs)
        
        # æ·»åŠ å‡½æ•°ç›¸å…³æŒ‡æ ‡
        monitor.add_custom_metric("function_name", func.__name__)
        monitor.add_custom_metric("args_count", len(args))
        monitor.add_custom_metric("kwargs_count", len(kwargs))
    
    return result, monitor.metrics


def compare_performance(functions: Dict[str, Callable], *args, **kwargs) -> Dict[str, PerformanceMetrics]:
    """
    æ¯”è¾ƒå¤šä¸ªå‡½æ•°çš„æ€§èƒ½
    
    Args:
        functions: {åç§°: å‡½æ•°} å­—å…¸
        *args, **kwargs: ä¼ é€’ç»™å‡½æ•°çš„å‚æ•°
    
    Returns:
        {åç§°: æ€§èƒ½æŒ‡æ ‡} å­—å…¸
    """
    results = {}
    
    print(f"ğŸ å¼€å§‹æ€§èƒ½æ¯”è¾ƒæµ‹è¯•ï¼Œå…±{len(functions)}ä¸ªå‡½æ•°")
    
    for name, func in functions.items():
        print(f"\næµ‹è¯•: {name}")
        try:
            _, metrics = benchmark_function(func, *args, **kwargs)
            results[name] = metrics
        except Exception as e:
            print(f"âŒ {name} æµ‹è¯•å¤±è´¥: {e}")
            continue
    
    # æ‰“å°æ¯”è¾ƒç»“æœ
    print("\n" + "="*60)
    print("æ€§èƒ½æ¯”è¾ƒç»“æœ:")
    print("="*60)
    
    # æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
    sorted_results = sorted(results.items(), key=lambda x: x[1].duration)
    
    for i, (name, metrics) in enumerate(sorted_results):
        rank = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else f"{i+1}."
        print(f"{rank} {name:20} {metrics.duration:8.2f}s  {metrics.memory_peak:8.1f}MB")
    
    return results


# è£…é¥°å™¨ï¼šè‡ªåŠ¨æ€§èƒ½ç›‘æ§
def monitor_performance(name: Optional[str] = None, print_summary: bool = True):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func: Callable):
        def wrapper(*args, **kwargs):
            monitor_name = name or f"å‡½æ•°: {func.__name__}"
            with performance_monitor(monitor_name, print_summary):
                return func(*args, **kwargs)
        return wrapper
    return decorator


# æµ‹è¯•å’Œç¤ºä¾‹
if __name__ == "__main__":
    import numpy as np
    from pathlib import Path
    
    print("ğŸ§ª æ€§èƒ½ç›‘æ§å·¥å…·æµ‹è¯•")
    
    # æµ‹è¯•1: åŸºæœ¬æ€§èƒ½ç›‘æ§
    with performance_monitor("çŸ©é˜µè®¡ç®—æµ‹è¯•") as monitor:
        # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
        data = np.random.random((1000, 1000))
        result = np.dot(data, data.T)
        
        monitor.add_custom_metric("matrix_size", "1000x1000")
        monitor.add_custom_metric("result_shape", result.shape)
    
    # æµ‹è¯•2: å‡½æ•°åŸºå‡†æµ‹è¯•
    @monitor_performance("è£…é¥°å™¨æµ‹è¯•")
    def test_function():
        time.sleep(0.5)
        return "æµ‹è¯•å®Œæˆ"
    
    result = test_function()
    print(f"å‡½æ•°è¿”å›: {result}")
    
    # æµ‹è¯•3: æ€§èƒ½æ¯”è¾ƒ
    def method_a():
        return sum(range(100000))
    
    def method_b():
        return sum(i for i in range(100000))
    
    def method_c():
        return np.sum(np.arange(100000))
    
    compare_performance({
        "ä¼ ç»Ÿå¾ªç¯": method_a,
        "ç”Ÿæˆå™¨": method_b,
        "NumPy": method_c
    })
    
    print("âœ… æ€§èƒ½ç›‘æ§å·¥å…·æµ‹è¯•å®Œæˆ")
