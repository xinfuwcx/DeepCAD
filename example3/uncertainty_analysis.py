"""
ä¸ç¡®å®šæ€§åˆ†æç³»ç»Ÿ
Uncertainty Analysis System

å®ç°è´å¶æ–¯æ¨ç†ã€è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿå’Œåœ°è´¨æ¨¡å‹çš„ä¸ç¡®å®šæ€§åˆ†æ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats, optimize, integrate
from scipy.spatial.distance import cdist
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel
from sklearn.model_selection import cross_val_score
import json
from typing import Dict, List, Tuple, Optional, Any, Union, Callable
from dataclasses import dataclass
from enum import Enum
import warnings
warnings.filterwarnings('ignore')


@dataclass
class UncertaintyParams:
    """ä¸ç¡®å®šæ€§å‚æ•°"""
    measurement_error: float    # æµ‹é‡è¯¯å·®æ ‡å‡†å·®
    model_error: float         # æ¨¡å‹è¯¯å·®æ ‡å‡†å·®
    prior_mean: float          # å…ˆéªŒå‡å€¼
    prior_std: float           # å…ˆéªŒæ ‡å‡†å·®
    correlation_length: float  # ç›¸å…³é•¿åº¦


class BayesianInference:
    """è´å¶æ–¯æ¨ç†å™¨"""
    
    def __init__(self, prior_distribution: str = "normal"):
        """
        åˆå§‹åŒ–è´å¶æ–¯æ¨ç†å™¨
        
        Args:
            prior_distribution: å…ˆéªŒåˆ†å¸ƒç±»å‹ ("normal", "uniform", "exponential")
        """
        self.prior_distribution = prior_distribution
        self.posterior_samples = None
        self.evidence = None
        self.hyperparameters = {}
        
        print(f"âœ“ è´å¶æ–¯æ¨ç†å™¨å·²åˆå§‹åŒ– (å…ˆéªŒ: {prior_distribution})")
    
    def set_prior(self, **kwargs):
        """è®¾ç½®å…ˆéªŒåˆ†å¸ƒå‚æ•°"""
        self.hyperparameters = kwargs
        
        if self.prior_distribution == "normal":
            if 'mean' not in kwargs or 'std' not in kwargs:
                raise ValueError("æ­£æ€å…ˆéªŒéœ€è¦ mean å’Œ std å‚æ•°")
        elif self.prior_distribution == "uniform":
            if 'low' not in kwargs or 'high' not in kwargs:
                raise ValueError("å‡åŒ€å…ˆéªŒéœ€è¦ low å’Œ high å‚æ•°")
        elif self.prior_distribution == "exponential":
            if 'scale' not in kwargs:
                raise ValueError("æŒ‡æ•°å…ˆéªŒéœ€è¦ scale å‚æ•°")
        
        print(f"âœ“ å…ˆéªŒåˆ†å¸ƒå·²è®¾ç½®: {self.prior_distribution} {kwargs}")
    
    def log_prior(self, parameters: np.ndarray) -> float:
        """è®¡ç®—å‚æ•°çš„å¯¹æ•°å…ˆéªŒæ¦‚ç‡"""
        if self.prior_distribution == "normal":
            mean = self.hyperparameters['mean']
            std = self.hyperparameters['std']
            return -0.5 * np.sum(((parameters - mean) / std) ** 2)
            
        elif self.prior_distribution == "uniform":
            low = self.hyperparameters['low']
            high = self.hyperparameters['high']
            if np.all((parameters >= low) & (parameters <= high)):
                return 0.0
            else:
                return -np.inf
                
        elif self.prior_distribution == "exponential":
            scale = self.hyperparameters['scale']
            if np.all(parameters >= 0):
                return np.sum(-parameters / scale - np.log(scale))
            else:
                return -np.inf
        
        return -np.inf
    
    def log_likelihood(self, parameters: np.ndarray, data: Dict) -> float:
        """è®¡ç®—å¯¹æ•°ä¼¼ç„¶å‡½æ•°"""
        observed_values = data['observations']
        predicted_values = data['predictions'](parameters)
        measurement_error = data.get('error', 1.0)
        
        # å‡è®¾é«˜æ–¯ä¼¼ç„¶
        residuals = observed_values - predicted_values
        log_likelihood = -0.5 * np.sum((residuals / measurement_error) ** 2)
        log_likelihood -= 0.5 * len(observed_values) * np.log(2 * np.pi * measurement_error**2)
        
        return log_likelihood
    
    def log_posterior(self, parameters: np.ndarray, data: Dict) -> float:
        """è®¡ç®—å¯¹æ•°åéªŒæ¦‚ç‡"""
        log_prior_val = self.log_prior(parameters)
        if log_prior_val == -np.inf:
            return -np.inf
            
        log_likelihood_val = self.log_likelihood(parameters, data)
        return log_prior_val + log_likelihood_val
    
    def metropolis_hastings(self, data: Dict, n_samples: int = 10000, 
                           initial_params: np.ndarray = None,
                           proposal_std: float = 0.1) -> np.ndarray:
        """
        ä½¿ç”¨Metropolis-Hastingsç®—æ³•è¿›è¡ŒMCMCé‡‡æ ·
        
        Args:
            data: åŒ…å«è§‚æµ‹æ•°æ®å’Œé¢„æµ‹å‡½æ•°çš„å­—å…¸
            n_samples: é‡‡æ ·æ•°é‡
            initial_params: åˆå§‹å‚æ•°
            proposal_std: æè®®åˆ†å¸ƒæ ‡å‡†å·®
            
        Returns:
            åéªŒæ ·æœ¬
        """
        print(f"ğŸ”„ å¼€å§‹MCMCé‡‡æ · ({n_samples} æ ·æœ¬)...")
        
        if initial_params is None:
            if self.prior_distribution == "normal":
                initial_params = np.array([self.hyperparameters['mean']])
            else:
                initial_params = np.array([1.0])  # é»˜è®¤åˆå§‹å€¼
        
        n_params = len(initial_params)
        samples = np.zeros((n_samples, n_params))
        current_params = initial_params.copy()
        current_log_posterior = self.log_posterior(current_params, data)
        
        n_accepted = 0
        
        for i in range(n_samples):
            if i % max(1, n_samples//10) == 0:
                print(f"  è¿›åº¦: {i}/{n_samples} ({100*i/n_samples:.1f}%), æ¥å—ç‡: {100*n_accepted/(i+1):.1f}%")
            
            # ç”Ÿæˆæè®®å‚æ•°
            proposal = current_params + np.random.normal(0, proposal_std, n_params)
            proposal_log_posterior = self.log_posterior(proposal, data)
            
            # Metropolis-Hastingsæ¥å—å‡†åˆ™
            log_alpha = proposal_log_posterior - current_log_posterior
            alpha = min(1.0, np.exp(log_alpha))
            
            if np.random.random() < alpha:
                current_params = proposal.copy()
                current_log_posterior = proposal_log_posterior
                n_accepted += 1
            
            samples[i] = current_params.copy()
        
        acceptance_rate = n_accepted / n_samples
        print(f"âœ“ MCMCé‡‡æ ·å®Œæˆ")
        print(f"  æ€»æ¥å—ç‡: {100*acceptance_rate:.1f}%")
        
        self.posterior_samples = samples
        return samples
    
    def compute_posterior_statistics(self, burn_in: int = 1000) -> Dict:
        """è®¡ç®—åéªŒç»Ÿè®¡é‡"""
        if self.posterior_samples is None:
            raise ValueError("éœ€è¦å…ˆè¿›è¡ŒMCMCé‡‡æ ·")
        
        # å»é™¤burn-inæ ·æœ¬
        samples = self.posterior_samples[burn_in:]
        
        statistics = {
            'mean': np.mean(samples, axis=0),
            'std': np.std(samples, axis=0),
            'median': np.median(samples, axis=0),
            'quantiles': {
                '2.5%': np.percentile(samples, 2.5, axis=0),
                '25%': np.percentile(samples, 25, axis=0),
                '75%': np.percentile(samples, 75, axis=0),
                '97.5%': np.percentile(samples, 97.5, axis=0)
            },
            'effective_sample_size': self._compute_ess(samples),
            'rhat': self._compute_rhat(samples) if samples.shape[0] > 100 else 1.0
        }
        
        print("âœ“ åéªŒç»Ÿè®¡é‡:")
        print(f"  å‡å€¼: {statistics['mean']}")
        print(f"  æ ‡å‡†å·®: {statistics['std']}")
        print(f"  95%ç½®ä¿¡åŒºé—´: [{statistics['quantiles']['2.5%'][0]:.3f}, {statistics['quantiles']['97.5%'][0]:.3f}]")
        
        return statistics
    
    def _compute_ess(self, samples: np.ndarray) -> float:
        """è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ•°"""
        # ç®€åŒ–çš„æœ‰æ•ˆæ ·æœ¬æ•°ä¼°è®¡
        n_samples = samples.shape[0]
        
        # è®¡ç®—è‡ªç›¸å…³å‡½æ•°
        autocorr = np.correlate(samples.flatten(), samples.flatten(), mode='full')
        autocorr = autocorr[autocorr.size // 2:]
        autocorr = autocorr / autocorr[0]  # å½’ä¸€åŒ–
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªè´Ÿå€¼æˆ–è¶³å¤Ÿå°çš„å€¼
        cutoff = 1
        for i in range(1, min(len(autocorr), n_samples//4)):
            if autocorr[i] <= 0.05:  # 5%é˜ˆå€¼
                cutoff = i
                break
        
        # è®¡ç®—è‡ªç›¸å…³æ—¶é—´
        tau = 1 + 2 * np.sum(autocorr[1:cutoff])
        ess = n_samples / (2 * tau + 1)
        
        return max(1, ess)
    
    def _compute_rhat(self, samples: np.ndarray) -> float:
        """è®¡ç®—Rhatæ”¶æ•›è¯Šæ–­ç»Ÿè®¡é‡"""
        # ç®€åŒ–çš„Rhatè®¡ç®— (éœ€è¦å¤šæ¡é“¾)
        # è¿™é‡Œä½¿ç”¨å•é“¾çš„ç®€åŒ–ç‰ˆæœ¬
        n = samples.shape[0]
        first_half = samples[:n//2]
        second_half = samples[n//2:]
        
        w = 0.5 * (np.var(first_half, axis=0) + np.var(second_half, axis=0))
        b = 0.5 * (np.mean(first_half, axis=0) - np.mean(second_half, axis=0))**2
        
        var_est = ((n//2 - 1) * w + b) / (n//2)
        rhat = np.sqrt(var_est / w) if w > 0 else 1.0
        
        return float(rhat)


class MonteCarloUncertainty:
    """è’™ç‰¹å¡æ´›ä¸ç¡®å®šæ€§åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è’™ç‰¹å¡æ´›åˆ†æå™¨"""
        self.parameter_distributions = {}
        self.correlation_matrix = None
        self.samples = None
        self.results = None
        
        print("âœ“ è’™ç‰¹å¡æ´›ä¸ç¡®å®šæ€§åˆ†æå™¨å·²åˆå§‹åŒ–")
    
    def add_uncertain_parameter(self, name: str, distribution: str, **kwargs):
        """
        æ·»åŠ ä¸ç¡®å®šå‚æ•°
        
        Args:
            name: å‚æ•°åç§°
            distribution: åˆ†å¸ƒç±»å‹ ("normal", "uniform", "triangular", "lognormal")
            **kwargs: åˆ†å¸ƒå‚æ•°
        """
        if distribution == "normal":
            dist = stats.norm(loc=kwargs['mean'], scale=kwargs['std'])
        elif distribution == "uniform":
            dist = stats.uniform(loc=kwargs['low'], scale=kwargs['high']-kwargs['low'])
        elif distribution == "triangular":
            c = (kwargs['mode'] - kwargs['low']) / (kwargs['high'] - kwargs['low'])
            dist = stats.triang(c=c, loc=kwargs['low'], scale=kwargs['high']-kwargs['low'])
        elif distribution == "lognormal":
            dist = stats.lognorm(s=kwargs['sigma'], scale=np.exp(kwargs['mu']))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å¸ƒç±»å‹: {distribution}")
        
        self.parameter_distributions[name] = dist
        print(f"âœ“ æ·»åŠ ä¸ç¡®å®šå‚æ•°: {name} ({distribution})")
    
    def set_correlation_matrix(self, param_names: List[str], 
                              correlation_matrix: np.ndarray):
        """è®¾ç½®å‚æ•°é—´çš„ç›¸å…³çŸ©é˜µ"""
        if len(param_names) != correlation_matrix.shape[0]:
            raise ValueError("å‚æ•°æ•°é‡ä¸ç›¸å…³çŸ©é˜µç»´åº¦ä¸åŒ¹é…")
        
        # æ£€æŸ¥ç›¸å…³çŸ©é˜µçš„æœ‰æ•ˆæ€§
        if not np.allclose(correlation_matrix, correlation_matrix.T):
            raise ValueError("ç›¸å…³çŸ©é˜µå¿…é¡»æ˜¯å¯¹ç§°çš„")
        
        eigenvals = np.linalg.eigvals(correlation_matrix)
        if np.any(eigenvals <= 0):
            raise ValueError("ç›¸å…³çŸ©é˜µå¿…é¡»æ˜¯æ­£å®šçš„")
        
        self.correlation_matrix = correlation_matrix
        self.correlated_params = param_names
        
        print(f"âœ“ ç›¸å…³çŸ©é˜µå·²è®¾ç½® ({len(param_names)}x{len(param_names)})")
    
    def generate_samples(self, n_samples: int) -> np.ndarray:
        """ç”Ÿæˆå‚æ•°æ ·æœ¬"""
        print(f"ğŸ”„ ç”Ÿæˆè’™ç‰¹å¡æ´›æ ·æœ¬ ({n_samples} ä¸ª)...")
        
        param_names = list(self.parameter_distributions.keys())
        n_params = len(param_names)
        
        # ç”Ÿæˆç‹¬ç«‹æ ·æœ¬
        samples = np.zeros((n_samples, n_params))
        
        for i, param_name in enumerate(param_names):
            dist = self.parameter_distributions[param_name]
            samples[:, i] = dist.rvs(n_samples)
        
        # å¦‚æœè®¾ç½®äº†ç›¸å…³æ€§ï¼Œåº”ç”¨ç›¸å…³å˜æ¢
        if self.correlation_matrix is not None:
            # è½¬æ¢ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒ
            normal_samples = np.zeros_like(samples)
            for i, param_name in enumerate(param_names):
                if param_name in self.correlated_params:
                    idx = self.correlated_params.index(param_name)
                    dist = self.parameter_distributions[param_name]
                    # è½¬æ¢ä¸ºæ ‡å‡†æ­£æ€
                    normal_samples[:, i] = stats.norm.ppf(dist.cdf(samples[:, i]))
            
            # åº”ç”¨Choleskyåˆ†è§£å¼•å…¥ç›¸å…³æ€§
            try:
                L = np.linalg.cholesky(self.correlation_matrix)
                corr_indices = [param_names.index(name) for name in self.correlated_params]
                
                correlated_normal = (L @ normal_samples[:, corr_indices].T).T
                
                # è½¬æ¢å›åŸåˆ†å¸ƒ
                for i, param_name in enumerate(self.correlated_params):
                    param_idx = param_names.index(param_name)
                    dist = self.parameter_distributions[param_name]
                    normal_vals = correlated_normal[:, i]
                    samples[:, param_idx] = dist.ppf(stats.norm.cdf(normal_vals))
                    
            except np.linalg.LinAlgError:
                print("  è­¦å‘Š: Choleskyåˆ†è§£å¤±è´¥ï¼Œä½¿ç”¨ç‹¬ç«‹æ ·æœ¬")
        
        self.samples = samples
        
        print("âœ“ æ ·æœ¬ç”Ÿæˆå®Œæˆ")
        return samples
    
    def propagate_uncertainty(self, model_function: Callable, 
                             samples: np.ndarray = None) -> np.ndarray:
        """
        ä¼ æ’­ä¸ç¡®å®šæ€§
        
        Args:
            model_function: æ¨¡å‹å‡½æ•°ï¼Œæ¥å—å‚æ•°æ•°ç»„è¿”å›ç»“æœ
            samples: å‚æ•°æ ·æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä¹‹å‰ç”Ÿæˆçš„æ ·æœ¬
            
        Returns:
            æ¨¡å‹è¾“å‡ºçš„æ ·æœ¬
        """
        if samples is None:
            if self.samples is None:
                raise ValueError("éœ€è¦å…ˆç”Ÿæˆæ ·æœ¬æˆ–æä¾›æ ·æœ¬")
            samples = self.samples
        
        print(f"ğŸ”„ ä¼ æ’­ä¸ç¡®å®šæ€§ ({len(samples)} ä¸ªæ ·æœ¬)...")
        
        n_samples = samples.shape[0]
        results = []
        
        for i, sample in enumerate(samples):
            if i % max(1, n_samples//10) == 0:
                print(f"  è¿›åº¦: {i}/{n_samples} ({100*i/n_samples:.1f}%)")
            
            try:
                result = model_function(sample)
                results.append(result)
            except Exception as e:
                print(f"  è­¦å‘Š: æ ·æœ¬ {i} è®¡ç®—å¤±è´¥: {e}")
                results.append(np.nan)
        
        results = np.array(results)
        self.results = results
        
        print("âœ“ ä¸ç¡®å®šæ€§ä¼ æ’­å®Œæˆ")
        return results
    
    def compute_sensitivity_indices(self, samples: np.ndarray, 
                                  results: np.ndarray) -> Dict:
        """è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ ‡"""
        print("ğŸ”„ è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ ‡...")
        
        param_names = list(self.parameter_distributions.keys())
        n_params = len(param_names)
        
        # è¿‡æ»¤æœ‰æ•ˆç»“æœ
        valid_mask = ~np.isnan(results)
        valid_samples = samples[valid_mask]
        valid_results = results[valid_mask]
        
        if len(valid_results) < 10:
            print("  è­¦å‘Š: æœ‰æ•ˆæ ·æœ¬æ•°é‡å¤ªå°‘")
            return {}
        
        # è®¡ç®—æ€»æ–¹å·®
        total_variance = np.var(valid_results)
        
        sensitivity_indices = {}
        
        # ä¸€é˜¶æ•æ„Ÿæ€§æŒ‡æ ‡ (Spearmanç›¸å…³ç³»æ•°)
        for i, param_name in enumerate(param_names):
            correlation, p_value = stats.spearmanr(
                valid_samples[:, i], valid_results
            )
            
            # ä¸€é˜¶æ•æ„Ÿæ€§æŒ‡æ ‡ (ç®€åŒ–ç‰ˆ)
            first_order_si = correlation**2 if not np.isnan(correlation) else 0
            
            sensitivity_indices[param_name] = {
                'first_order': first_order_si,
                'correlation': correlation,
                'p_value': p_value
            }
        
        # æ€»æ•æ„Ÿæ€§æŒ‡æ ‡ (é€šè¿‡æ–¹å·®åˆ†è§£ä¼°è®¡)
        for i, param_name in enumerate(param_names):
            # å°†å‚æ•°åˆ†ä¸ºé«˜ä½ä¸¤ç»„
            param_values = valid_samples[:, i]
            median_val = np.median(param_values)
            
            low_mask = param_values <= median_val
            high_mask = param_values > median_val
            
            if np.sum(low_mask) > 5 and np.sum(high_mask) > 5:
                low_var = np.var(valid_results[low_mask])
                high_var = np.var(valid_results[high_mask])
                within_group_var = (low_var + high_var) / 2
                
                total_si = 1 - within_group_var / total_variance
                sensitivity_indices[param_name]['total'] = max(0, total_si)
        
        print("âœ“ æ•æ„Ÿæ€§åˆ†æå®Œæˆ")
        
        # æŒ‰æ•æ„Ÿæ€§æ’åº
        sorted_params = sorted(sensitivity_indices.keys(), 
                             key=lambda x: sensitivity_indices[x]['first_order'], 
                             reverse=True)
        
        print("  æ•æ„Ÿæ€§æ’åº:")
        for i, param in enumerate(sorted_params[:3]):
            si = sensitivity_indices[param]['first_order']
            print(f"    {i+1}. {param}: {si:.3f}")
        
        return sensitivity_indices
    
    def compute_uncertainty_statistics(self, results: np.ndarray = None) -> Dict:
        """è®¡ç®—ä¸ç¡®å®šæ€§ç»Ÿè®¡é‡"""
        if results is None:
            if self.results is None:
                raise ValueError("éœ€è¦å…ˆè¿›è¡Œä¸ç¡®å®šæ€§ä¼ æ’­")
            results = self.results
        
        # è¿‡æ»¤æœ‰æ•ˆç»“æœ
        valid_results = results[~np.isnan(results)]
        
        if len(valid_results) == 0:
            return {'error': 'æ— æœ‰æ•ˆç»“æœ'}
        
        statistics = {
            'mean': np.mean(valid_results),
            'std': np.std(valid_results),
            'variance': np.var(valid_results),
            'median': np.median(valid_results),
            'min': np.min(valid_results),
            'max': np.max(valid_results),
            'skewness': stats.skew(valid_results),
            'kurtosis': stats.kurtosis(valid_results),
            'percentiles': {
                '1%': np.percentile(valid_results, 1),
                '5%': np.percentile(valid_results, 5),
                '25%': np.percentile(valid_results, 25),
                '75%': np.percentile(valid_results, 75),
                '95%': np.percentile(valid_results, 95),
                '99%': np.percentile(valid_results, 99)
            },
            'coefficient_of_variation': np.std(valid_results) / np.mean(valid_results) if np.mean(valid_results) != 0 else np.inf,
            'valid_samples': len(valid_results),
            'total_samples': len(results)
        }
        
        print("âœ“ ä¸ç¡®å®šæ€§ç»Ÿè®¡é‡:")
        print(f"  å‡å€¼ Â± æ ‡å‡†å·®: {statistics['mean']:.3f} Â± {statistics['std']:.3f}")
        print(f"  95%ç½®ä¿¡åŒºé—´: [{statistics['percentiles']['2.5%']:.3f}, {statistics['percentiles']['97.5%']:.3f}]")
        print(f"  å˜å¼‚ç³»æ•°: {statistics['coefficient_of_variation']:.3f}")
        
        return statistics


class GeologicalUncertaintyAnalyzer:
    """åœ°è´¨æ¨¡å‹ä¸ç¡®å®šæ€§åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åœ°è´¨ä¸ç¡®å®šæ€§åˆ†æå™¨"""
        self.bayesian_inference = BayesianInference()
        self.monte_carlo = MonteCarloUncertainty()
        
        # åœ°è´¨å‚æ•°ä¸ç¡®å®šæ€§
        self.geological_parameters = {}
        self.spatial_uncertainty = None
        
        print("âœ“ åœ°è´¨æ¨¡å‹ä¸ç¡®å®šæ€§åˆ†æå™¨å·²åˆå§‹åŒ–")
    
    def add_geological_uncertainty(self, param_name: str, uncertainty_type: str,
                                 spatial_data: np.ndarray = None, **kwargs):
        """
        æ·»åŠ åœ°è´¨å‚æ•°ä¸ç¡®å®šæ€§
        
        Args:
            param_name: å‚æ•°åç§° ('density', 'thickness', 'depth', etc.)
            uncertainty_type: ä¸ç¡®å®šæ€§ç±»å‹ ('measurement', 'interpolation', 'model')
            spatial_data: ç©ºé—´æ•°æ®ç‚¹ [[x,y,z,value], ...]
            **kwargs: ä¸ç¡®å®šæ€§å‚æ•°
        """
        self.geological_parameters[param_name] = {
            'type': uncertainty_type,
            'spatial_data': spatial_data,
            'params': kwargs
        }
        
        # è‡ªåŠ¨æ·»åŠ åˆ°è’™ç‰¹å¡æ´›åˆ†æå™¨
        if uncertainty_type == 'measurement':
            # æµ‹é‡ä¸ç¡®å®šæ€§ - æ­£æ€åˆ†å¸ƒ
            mean_val = kwargs.get('mean', 0)
            std_val = kwargs.get('std', 1)
            self.monte_carlo.add_uncertain_parameter(
                param_name, 'normal', mean=mean_val, std=std_val
            )
        elif uncertainty_type == 'interpolation':
            # æ’å€¼ä¸ç¡®å®šæ€§ - åŸºäºå…‹é‡Œé‡‘æ–¹å·®
            if spatial_data is not None:
                self._add_spatial_uncertainty(param_name, spatial_data, **kwargs)
        
        print(f"âœ“ åœ°è´¨ä¸ç¡®å®šæ€§å·²æ·»åŠ : {param_name} ({uncertainty_type})")
    
    def _add_spatial_uncertainty(self, param_name: str, spatial_data: np.ndarray, **kwargs):
        """æ·»åŠ ç©ºé—´ä¸ç¡®å®šæ€§"""
        # ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹å›å½’ä¼°è®¡ç©ºé—´ä¸ç¡®å®šæ€§
        coords = spatial_data[:, :3]  # x, y, z
        values = spatial_data[:, 3]   # å‚æ•°å€¼
        
        # é…ç½®é«˜æ–¯è¿‡ç¨‹
        length_scale = kwargs.get('length_scale', 1000)  # ç›¸å…³é•¿åº¦
        variance = kwargs.get('variance', np.var(values))
        
        kernel = RBF(length_scale=length_scale) * variance + WhiteKernel(noise_level=0.1)
        
        gp = GaussianProcessRegressor(
            kernel=kernel,
            n_restarts_optimizer=3,
            random_state=42
        )
        
        try:
            gp.fit(coords, values)
            
            # å­˜å‚¨é«˜æ–¯è¿‡ç¨‹æ¨¡å‹
            self.geological_parameters[param_name]['gp_model'] = gp
            
            print(f"  é«˜æ–¯è¿‡ç¨‹æ¨¡å‹å·²æ‹Ÿåˆ: {param_name}")
            print(f"    é•¿åº¦å°ºåº¦: {gp.kernel_.k1.length_scale:.1f}")
            print(f"    æ–¹å·®: {gp.kernel_.k1.k2.constant_value:.3f}")
            
        except Exception as e:
            print(f"  è­¦å‘Š: é«˜æ–¯è¿‡ç¨‹æ‹Ÿåˆå¤±è´¥ ({e})")
    
    def predict_with_uncertainty(self, param_name: str, 
                                prediction_points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        åœ¨æŒ‡å®šç‚¹é¢„æµ‹å‚æ•°å€¼åŠå…¶ä¸ç¡®å®šæ€§
        
        Args:
            param_name: å‚æ•°åç§°
            prediction_points: é¢„æµ‹ç‚¹åæ ‡ [[x1,y1,z1], [x2,y2,z2], ...]
            
        Returns:
            (å‡å€¼é¢„æµ‹, æ ‡å‡†å·®é¢„æµ‹)
        """
        if param_name not in self.geological_parameters:
            raise ValueError(f"å‚æ•° {param_name} æœªå®šä¹‰")
        
        param_info = self.geological_parameters[param_name]
        
        if 'gp_model' in param_info:
            # ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹é¢„æµ‹
            gp_model = param_info['gp_model']
            mean_pred, std_pred = gp_model.predict(prediction_points, return_std=True)
            
            return mean_pred, std_pred
        else:
            # ä½¿ç”¨ç®€å•çš„ç©ºé—´æ’å€¼ + ä¸ç¡®å®šæ€§ä¼°è®¡
            spatial_data = param_info['spatial_data']
            if spatial_data is None:
                raise ValueError(f"å‚æ•° {param_name} ç¼ºå°‘ç©ºé—´æ•°æ®")
            
            coords = spatial_data[:, :3]
            values = spatial_data[:, 3]
            
            # ç®€å•çš„åè·ç¦»åŠ æƒæ’å€¼
            predictions = []
            uncertainties = []
            
            for point in prediction_points:
                distances = np.linalg.norm(coords - point, axis=1)
                
                # é¿å…é™¤é›¶
                distances = np.maximum(distances, 1e-10)
                
                # åè·ç¦»æƒé‡
                weights = 1 / distances**2
                weights /= np.sum(weights)
                
                # æ’å€¼é¢„æµ‹
                pred_val = np.sum(weights * values)
                
                # ä¸ç¡®å®šæ€§ä¼°è®¡ (åŸºäºæœ€è¿‘é‚»æ–¹å·®)
                k_nearest = min(5, len(values))
                nearest_indices = np.argsort(distances)[:k_nearest]
                local_variance = np.var(values[nearest_indices])
                uncertainty = np.sqrt(local_variance)
                
                predictions.append(pred_val)
                uncertainties.append(uncertainty)
            
            return np.array(predictions), np.array(uncertainties)
    
    def analyze_model_uncertainty(self, geological_model: Dict, 
                                 analysis_points: np.ndarray,
                                 n_realizations: int = 100) -> Dict:
        """
        åˆ†æåœ°è´¨æ¨¡å‹çš„æ•´ä½“ä¸ç¡®å®šæ€§
        
        Args:
            geological_model: åœ°è´¨æ¨¡å‹å­—å…¸
            analysis_points: åˆ†æç‚¹åæ ‡
            n_realizations: å®ç°æ¬¡æ•°
            
        Returns:
            ä¸ç¡®å®šæ€§åˆ†æç»“æœ
        """
        print(f"ğŸ”„ åˆ†æåœ°è´¨æ¨¡å‹ä¸ç¡®å®šæ€§ ({n_realizations} æ¬¡å®ç°)...")
        
        results = {
            'realizations': [],
            'statistics': {},
            'uncertainty_maps': {}
        }
        
        # ç”Ÿæˆå¤šä¸ªåœ°è´¨æ¨¡å‹å®ç°
        for i in range(n_realizations):
            if i % max(1, n_realizations//10) == 0:
                print(f"  è¿›åº¦: {i}/{n_realizations} ({100*i/n_realizations:.1f}%)")
            
            try:
                # ä¸ºæ¯ä¸ªä¸ç¡®å®šå‚æ•°ç”Ÿæˆéšæœºå€¼
                uncertain_params = {}
                for param_name, param_info in self.geological_parameters.items():
                    if param_info['type'] == 'measurement':
                        # ä»åˆ†å¸ƒä¸­é‡‡æ ·
                        dist = self.monte_carlo.parameter_distributions[param_name]
                        uncertain_params[param_name] = dist.rvs()
                    elif param_info['type'] == 'interpolation':
                        # ä½¿ç”¨æ¡ä»¶é«˜æ–¯è¿‡ç¨‹é‡‡æ ·
                        if 'gp_model' in param_info:
                            gp_model = param_info['gp_model']
                            # ç®€åŒ–: åœ¨åˆ†æç‚¹ç”Ÿæˆä¸€ä¸ªéšæœºå®ç°
                            samples = gp_model.sample_y(analysis_points, n_samples=1)
                            uncertain_params[param_name] = samples.flatten()
                
                # ä½¿ç”¨ä¸ç¡®å®šå‚æ•°è¿è¡Œåœ°è´¨æ¨¡å‹
                realization = self._run_geological_model_realization(
                    geological_model, analysis_points, uncertain_params
                )
                
                results['realizations'].append(realization)
                
            except Exception as e:
                print(f"  è­¦å‘Š: å®ç° {i} å¤±è´¥: {e}")
        
        # è®¡ç®—ç»Ÿè®¡é‡
        if len(results['realizations']) > 0:
            realizations_array = np.array(results['realizations'])
            
            results['statistics'] = {
                'mean': np.mean(realizations_array, axis=0),
                'std': np.std(realizations_array, axis=0),
                'percentiles': {
                    '5%': np.percentile(realizations_array, 5, axis=0),
                    '25%': np.percentile(realizations_array, 25, axis=0),
                    '75%': np.percentile(realizations_array, 75, axis=0),
                    '95%': np.percentile(realizations_array, 95, axis=0)
                },
                'probability_maps': {}
            }
            
            # è®¡ç®—æ¦‚ç‡å›¾ (ä¾‹å¦‚: æŸå²©æ€§å­˜åœ¨æ¦‚ç‡)
            unique_values = np.unique(realizations_array)
            for value in unique_values:
                prob_map = np.mean(realizations_array == value, axis=0)
                results['statistics']['probability_maps'][f'prob_{value}'] = prob_map
        
        print("âœ“ åœ°è´¨æ¨¡å‹ä¸ç¡®å®šæ€§åˆ†æå®Œæˆ")
        print(f"  æˆåŠŸå®ç°: {len(results['realizations'])}/{n_realizations}")
        
        return results
    
    def _run_geological_model_realization(self, geological_model: Dict,
                                        analysis_points: np.ndarray,
                                        uncertain_params: Dict) -> np.ndarray:
        """è¿è¡Œå•æ¬¡åœ°è´¨æ¨¡å‹å®ç°"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹å®ç°
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨å®Œæ•´çš„åœ°è´¨å»ºæ¨¡æµç¨‹
        
        n_points = len(analysis_points)
        
        # æ¨¡æ‹Ÿåœ°è´¨æ¨¡å‹è¾“å‡º (å²©æ€§ID)
        base_lithology = np.ones(n_points, dtype=int)
        
        # æ ¹æ®ä¸ç¡®å®šå‚æ•°ä¿®æ”¹ç»“æœ
        for param_name, param_value in uncertain_params.items():
            if param_name == 'density':
                # åŸºäºå¯†åº¦è°ƒæ•´å²©æ€§åˆ†å¸ƒ (ç¤ºä¾‹é€»è¾‘)
                if isinstance(param_value, (int, float)):
                    if param_value > 2.5:
                        base_lithology[n_points//3:] = 2  # é«˜å¯†åº¦å²©çŸ³
                else:
                    high_density_mask = param_value > 2.5
                    base_lithology[high_density_mask] = 2
        
        return base_lithology


def create_demo_uncertainty_analysis():
    """åˆ›å»ºä¸ç¡®å®šæ€§åˆ†ææ¼”ç¤º"""
    print("ğŸ“Š åˆ›å»ºä¸ç¡®å®šæ€§åˆ†ææ¼”ç¤º...")
    
    # åˆ›å»ºè´å¶æ–¯æ¨ç†æ¼”ç¤º
    print("\n1. è´å¶æ–¯æ¨ç†æ¼”ç¤º")
    bayesian = BayesianInference("normal")
    bayesian.set_prior(mean=2.5, std=0.2)  # å¯†åº¦å…ˆéªŒ
    
    # æ¨¡æ‹Ÿè§‚æµ‹æ•°æ®
    np.random.seed(42)
    true_density = 2.7
    n_obs = 20
    measurement_error = 0.1
    observations = true_density + np.random.normal(0, measurement_error, n_obs)
    
    # å®šä¹‰ç®€å•çš„é¢„æµ‹å‡½æ•°
    def prediction_function(params):
        return np.full(n_obs, params[0])  # å¸¸æ•°æ¨¡å‹
    
    data = {
        'observations': observations,
        'predictions': prediction_function,
        'error': measurement_error
    }
    
    # MCMCé‡‡æ ·
    samples = bayesian.metropolis_hastings(data, n_samples=5000, 
                                         initial_params=np.array([2.5]),
                                         proposal_std=0.05)
    
    # è®¡ç®—åéªŒç»Ÿè®¡
    posterior_stats = bayesian.compute_posterior_statistics(burn_in=1000)
    
    # åˆ›å»ºè’™ç‰¹å¡æ´›æ¼”ç¤º
    print("\n2. è’™ç‰¹å¡æ´›ä¸ç¡®å®šæ€§åˆ†ææ¼”ç¤º")
    mc_analyzer = MonteCarloUncertainty()
    
    # æ·»åŠ ä¸ç¡®å®šå‚æ•°
    mc_analyzer.add_uncertain_parameter('density', 'normal', mean=2.6, std=0.2)
    mc_analyzer.add_uncertain_parameter('thickness', 'uniform', low=50, high=150)
    mc_analyzer.add_uncertain_parameter('depth', 'triangular', low=100, mode=200, high=300)
    
    # è®¾ç½®å‚æ•°ç›¸å…³æ€§
    correlation_matrix = np.array([
        [1.0, 0.3, -0.2],  # density vs thickness vs depth
        [0.3, 1.0, 0.1],
        [-0.2, 0.1, 1.0]
    ])
    mc_analyzer.set_correlation_matrix(['density', 'thickness', 'depth'], correlation_matrix)
    
    # ç”Ÿæˆæ ·æœ¬
    samples_mc = mc_analyzer.generate_samples(n_samples=1000)
    
    # å®šä¹‰åœ°è´¨æ¨¡å‹å‡½æ•°
    def geological_model(params):
        density, thickness, depth = params
        # ç®€åŒ–çš„åœ°è´¨å“åº”å‡½æ•°
        gravity_response = density * thickness * (1 / depth)
        return gravity_response
    
    # ä¼ æ’­ä¸ç¡®å®šæ€§
    model_results = mc_analyzer.propagate_uncertainty(geological_model, samples_mc)
    
    # è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ ‡
    sensitivity = mc_analyzer.compute_sensitivity_indices(samples_mc, model_results)
    
    # è®¡ç®—ä¸ç¡®å®šæ€§ç»Ÿè®¡
    uncertainty_stats = mc_analyzer.compute_uncertainty_statistics(model_results)
    
    # åˆ›å»ºåœ°è´¨ä¸ç¡®å®šæ€§åˆ†ææ¼”ç¤º
    print("\n3. åœ°è´¨æ¨¡å‹ä¸ç¡®å®šæ€§åˆ†ææ¼”ç¤º")
    geo_analyzer = GeologicalUncertaintyAnalyzer()
    
    # ç”Ÿæˆæ¨¡æ‹Ÿç©ºé—´æ•°æ®
    n_points = 50
    coords = np.random.uniform(0, 1000, (n_points, 3))
    density_values = 2.5 + 0.3 * np.sin(coords[:, 0] / 200) + np.random.normal(0, 0.1, n_points)
    spatial_data = np.column_stack([coords, density_values])
    
    # æ·»åŠ åœ°è´¨ä¸ç¡®å®šæ€§
    geo_analyzer.add_geological_uncertainty(
        'spatial_density', 'interpolation', 
        spatial_data=spatial_data,
        length_scale=300, variance=0.1
    )
    
    # é¢„æµ‹ç‚¹
    pred_points = np.random.uniform(0, 1000, (100, 3))
    
    # é¢„æµ‹åŠä¸ç¡®å®šæ€§
    try:
        mean_pred, std_pred = geo_analyzer.predict_with_uncertainty('spatial_density', pred_points)
        
        print(f"  ç©ºé—´é¢„æµ‹å®Œæˆ:")
        print(f"    é¢„æµ‹å‡å€¼èŒƒå›´: {mean_pred.min():.3f} - {mean_pred.max():.3f}")
        print(f"    ä¸ç¡®å®šæ€§èŒƒå›´: {std_pred.min():.3f} - {std_pred.max():.3f}")
    except Exception as e:
        print(f"  ç©ºé—´é¢„æµ‹å¤±è´¥: {e}")
    
    print("\nğŸ‰ ä¸ç¡®å®šæ€§åˆ†ææ¼”ç¤ºå®Œæˆ!")
    
    return {
        'bayesian': bayesian,
        'posterior_stats': posterior_stats,
        'monte_carlo': mc_analyzer,
        'mc_samples': samples_mc,
        'mc_results': model_results,
        'sensitivity': sensitivity,
        'uncertainty_stats': uncertainty_stats,
        'geological_analyzer': geo_analyzer
    }


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demo_results = create_demo_uncertainty_analysis()
    
    # å¯è§†åŒ–ç»“æœ
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # 1. åéªŒåˆ†å¸ƒ
    if demo_results['bayesian'].posterior_samples is not None:
        samples = demo_results['bayesian'].posterior_samples[1000:]  # å»é™¤burn-in
        axes[0, 0].hist(samples.flatten(), bins=50, alpha=0.7, density=True)
        axes[0, 0].axvline(demo_results['posterior_stats']['mean'][0], color='r', 
                          linestyle='--', label='åéªŒå‡å€¼')
        axes[0, 0].set_xlabel('å¯†åº¦ (g/cmÂ³)')
        axes[0, 0].set_ylabel('æ¦‚ç‡å¯†åº¦')
        axes[0, 0].set_title('è´å¶æ–¯åéªŒåˆ†å¸ƒ')
        axes[0, 0].legend()
    
    # 2. å‚æ•°æ ·æœ¬æ•£ç‚¹å›¾
    samples_mc = demo_results['mc_samples']
    scatter = axes[0, 1].scatter(samples_mc[:, 0], samples_mc[:, 1], 
                                c=samples_mc[:, 2], alpha=0.6, cmap='viridis')
    axes[0, 1].set_xlabel('å¯†åº¦ (g/cmÂ³)')
    axes[0, 1].set_ylabel('åšåº¦ (m)')
    axes[0, 1].set_title('å‚æ•°æ ·æœ¬åˆ†å¸ƒ')
    plt.colorbar(scatter, ax=axes[0, 1], label='æ·±åº¦ (m)')
    
    # 3. æ¨¡å‹è¾“å‡ºåˆ†å¸ƒ
    mc_results = demo_results['mc_results']
    valid_results = mc_results[~np.isnan(mc_results)]
    axes[0, 2].hist(valid_results, bins=50, alpha=0.7, density=True)
    axes[0, 2].set_xlabel('æ¨¡å‹è¾“å‡º')
    axes[0, 2].set_ylabel('æ¦‚ç‡å¯†åº¦')
    axes[0, 2].set_title('æ¨¡å‹è¾“å‡ºä¸ç¡®å®šæ€§')
    
    # 4. æ•æ„Ÿæ€§åˆ†æ
    sensitivity = demo_results['sensitivity']
    param_names = list(sensitivity.keys())
    first_order_si = [sensitivity[p]['first_order'] for p in param_names]
    
    bars = axes[1, 0].bar(param_names, first_order_si)
    axes[1, 0].set_ylabel('ä¸€é˜¶æ•æ„Ÿæ€§æŒ‡æ ‡')
    axes[1, 0].set_title('å‚æ•°æ•æ„Ÿæ€§åˆ†æ')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # 5. ä¸ç¡®å®šæ€§ç»Ÿè®¡
    stats = demo_results['uncertainty_stats']
    percentiles = [1, 5, 25, 50, 75, 95, 99]
    values = [stats['percentiles'][f'{p}%'] for p in percentiles]
    
    axes[1, 1].plot(percentiles, values, 'o-')
    axes[1, 1].set_xlabel('ç™¾åˆ†ä½æ•°')
    axes[1, 1].set_ylabel('æ¨¡å‹è¾“å‡º')
    axes[1, 1].set_title('ä¸ç¡®å®šæ€§åˆ†ä½æ•°')
    axes[1, 1].grid(True)
    
    # 6. æ”¶æ•›è¯Šæ–­
    if demo_results['bayesian'].posterior_samples is not None:
        samples = demo_results['bayesian'].posterior_samples
        running_mean = np.cumsum(samples.flatten()) / np.arange(1, len(samples.flatten()) + 1)
        axes[1, 2].plot(running_mean)
        axes[1, 2].axhline(demo_results['posterior_stats']['mean'][0], 
                          color='r', linestyle='--', label='åéªŒå‡å€¼')
        axes[1, 2].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[1, 2].set_ylabel('ç´¯ç§¯å‡å€¼')
        axes[1, 2].set_title('MCMCæ”¶æ•›è¯Šæ–­')
        axes[1, 2].legend()
    
    plt.tight_layout()
    plt.savefig('example3/uncertainty_analysis_results.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š ä¸ç¡®å®šæ€§åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: example3/uncertainty_analysis_results.png")