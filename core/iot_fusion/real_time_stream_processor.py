#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepCAD IoTå®æ—¶æ•°æ®æµå¤„ç†å™¨ - æ ¸å¿ƒç®—æ³•å®ç°
3å·è®¡ç®—ä¸“å®¶ - Week2-3æ ¸å¿ƒç®—æ³•å®ç°

å®æ—¶æ•°æ®æµå¤„ç†æ ¸å¿ƒç®—æ³•ï¼š
- æ»‘åŠ¨çª—å£æ•°æ®èšåˆ
- åœ¨çº¿å¼‚å¸¸æ£€æµ‹ç®—æ³•
- è‡ªé€‚åº”æµé‡æ§åˆ¶
- åˆ†å¸ƒå¼æµå¤„ç†æ¶æ„
- GPUåŠ é€Ÿæ•°æ®å¤„ç†

æŠ€æœ¯æŒ‡æ ‡ï¼š
- å¤„ç†èƒ½åŠ›ï¼š10,000ç‚¹/ç§’
- å»¶è¿Ÿï¼š<100ms
- ååé‡ï¼š1000+ä¼ æ„Ÿå™¨å¹¶å‘
- å¼‚å¸¸æ£€æµ‹ç²¾åº¦ï¼š>99%
"""

import numpy as np
import asyncio
import time
import logging
from typing import Dict, List, Optional, Union, Tuple, Callable, Any
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import json
import pickle
from collections import deque, defaultdict
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import heapq
import statistics

# æ•°æ®å¤„ç†åº“
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    print("Warning: Pandasä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€æ•°æ®ç»“æ„")
    PANDAS_AVAILABLE = False

# æœºå™¨å­¦ä¹ åº“
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA
    SKLEARN_AVAILABLE = True
except ImportError:
    print("Warning: Scikit-learnä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€å¼‚å¸¸æ£€æµ‹")
    SKLEARN_AVAILABLE = False

# GPUåŠ é€Ÿ
try:
    import cupy as cp
    CUPY_AVAILABLE = True
    print("CuPyå¯ç”¨ï¼Œå¯ç”¨GPUåŠ é€Ÿ")
except ImportError:
    print("CuPyä¸å¯ç”¨ï¼Œä½¿ç”¨CPUå¤„ç†")
    CUPY_AVAILABLE = False

@dataclass
class StreamData:
    """æµæ•°æ®ç‚¹"""
    sensor_id: str
    timestamp: float
    value: float
    metadata: Dict = field(default_factory=dict)
    processed: bool = False

@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœ"""
    original_data: StreamData
    processed_value: float
    anomaly_score: float
    is_anomaly: bool
    processing_time: float
    stage: str

@dataclass
class WindowState:
    """æ»‘åŠ¨çª—å£çŠ¶æ€"""
    window_size: int
    data_buffer: deque = field(default_factory=deque)
    statistics: Dict = field(default_factory=dict)
    last_update: float = 0.0

class SlidingWindowAggregator:
    """æ»‘åŠ¨çª—å£æ•°æ®èšåˆå™¨"""
    
    def __init__(self, window_size: int = 1000, stride: int = 100):
        """
        åˆå§‹åŒ–æ»‘åŠ¨çª—å£èšåˆå™¨
        
        Args:
            window_size: çª—å£å¤§å°ï¼ˆæ•°æ®ç‚¹æ•°ï¼‰
            stride: æ»‘åŠ¨æ­¥é•¿
        """
        self.window_size = window_size
        self.stride = stride
        self.windows: Dict[str, WindowState] = {}
        self.logger = logging.getLogger(__name__)
        
        # èšåˆå‡½æ•°
        self.aggregation_functions = {
            'mean': np.mean,
            'std': np.std,
            'min': np.min,
            'max': np.max,
            'median': np.median,
            'percentile_95': lambda x: np.percentile(x, 95),
            'rms': lambda x: np.sqrt(np.mean(np.square(x)))
        }
    
    def add_data_point(self, data: StreamData) -> Optional[Dict]:
        """
        æ·»åŠ æ•°æ®ç‚¹åˆ°æ»‘åŠ¨çª—å£
        
        Args:
            data: æµæ•°æ®ç‚¹
            
        Returns:
            èšåˆç»Ÿè®¡ç»“æœï¼ˆå¦‚æœçª—å£å·²æ»¡ï¼‰
        """
        sensor_id = data.sensor_id
        
        # åˆå§‹åŒ–ä¼ æ„Ÿå™¨çª—å£
        if sensor_id not in self.windows:
            self.windows[sensor_id] = WindowState(
                window_size=self.window_size,
                data_buffer=deque(maxlen=self.window_size)
            )
        
        window = self.windows[sensor_id]
        
        # æ·»åŠ æ•°æ®ç‚¹
        window.data_buffer.append(data.value)
        window.last_update = data.timestamp
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è®¡ç®—èšåˆç»Ÿè®¡
        if len(window.data_buffer) >= self.window_size:
            if len(window.data_buffer) % self.stride == 0:
                return self._compute_window_statistics(sensor_id)
        
        return None
    
    def _compute_window_statistics(self, sensor_id: str) -> Dict:
        """è®¡ç®—çª—å£ç»Ÿè®¡ä¿¡æ¯"""
        window = self.windows[sensor_id]
        data_array = np.array(list(window.data_buffer))
        
        statistics = {}
        
        # è®¡ç®—åŸºç¡€ç»Ÿè®¡é‡
        for stat_name, func in self.aggregation_functions.items():
            try:
                statistics[stat_name] = float(func(data_array))
            except Exception as e:
                self.logger.warning(f"è®¡ç®—{stat_name}å¤±è´¥: {e}")
                statistics[stat_name] = 0.0
        
        # è¶‹åŠ¿åˆ†æ
        if len(data_array) >= 10:
            statistics['trend'] = self._compute_trend(data_array)
            statistics['volatility'] = self._compute_volatility(data_array)
        
        # æ›´æ–°çª—å£ç»Ÿè®¡
        window.statistics = statistics
        
        return {
            'sensor_id': sensor_id,
            'timestamp': window.last_update,
            'window_size': len(data_array),
            'statistics': statistics
        }
    
    def _compute_trend(self, data: np.ndarray) -> float:
        """è®¡ç®—æ•°æ®è¶‹åŠ¿ï¼ˆçº¿æ€§å›å½’æ–œç‡ï¼‰"""
        n = len(data)
        x = np.arange(n)
        
        # çº¿æ€§å›å½’ y = ax + b
        a = (n * np.sum(x * data) - np.sum(x) * np.sum(data)) / (n * np.sum(x**2) - np.sum(x)**2)
        
        return float(a)
    
    def _compute_volatility(self, data: np.ndarray) -> float:
        """è®¡ç®—æ³¢åŠ¨æ€§ï¼ˆæ»šåŠ¨æ ‡å‡†å·®ï¼‰"""
        if len(data) < 5:
            return 0.0
        
        # è®¡ç®—æ»šåŠ¨æ ‡å‡†å·®
        window_size = min(10, len(data) // 2)
        rolling_stds = []
        
        for i in range(window_size, len(data)):
            window_data = data[i-window_size:i]
            rolling_stds.append(np.std(window_data))
        
        return float(np.mean(rolling_stds)) if rolling_stds else 0.0
    
    def get_window_state(self, sensor_id: str) -> Optional[WindowState]:
        """è·å–ä¼ æ„Ÿå™¨çª—å£çŠ¶æ€"""
        return self.windows.get(sensor_id)

class OnlineAnomalyDetector:
    """åœ¨çº¿å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, contamination: float = 0.1, window_size: int = 1000):
        """
        åˆå§‹åŒ–åœ¨çº¿å¼‚å¸¸æ£€æµ‹å™¨
        
        Args:
            contamination: å¼‚å¸¸æ¯”ä¾‹
            window_size: è®­ç»ƒçª—å£å¤§å°
        """
        self.contamination = contamination
        self.window_size = window_size
        self.logger = logging.getLogger(__name__)
        
        # æ£€æµ‹å™¨å­—å…¸ï¼ˆæ¯ä¸ªä¼ æ„Ÿå™¨ä¸€ä¸ªï¼‰
        self.detectors: Dict[str, Any] = {}
        self.scalers: Dict[str, Any] = {}
        self.training_data: Dict[str, deque] = {}
        
        # é˜ˆå€¼å­—å…¸
        self.thresholds: Dict[str, Dict[str, float]] = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.detection_stats = defaultdict(lambda: {
            'total_points': 0,
            'anomalies_detected': 0,
            'false_positives': 0,
            'false_negatives': 0
        })
    
    def detect_anomaly(self, data: StreamData, window_stats: Optional[Dict] = None) -> Tuple[bool, float]:
        """
        æ£€æµ‹æ•°æ®ç‚¹æ˜¯å¦ä¸ºå¼‚å¸¸
        
        Args:
            data: æµæ•°æ®ç‚¹
            window_stats: æ»‘åŠ¨çª—å£ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            (æ˜¯å¦å¼‚å¸¸, å¼‚å¸¸åˆ†æ•°)
        """
        sensor_id = data.sensor_id
        
        # åˆå§‹åŒ–ä¼ æ„Ÿå™¨æ£€æµ‹å™¨
        if sensor_id not in self.detectors:
            self._initialize_detector(sensor_id)
        
        # æ›´æ–°è®­ç»ƒæ•°æ®
        self._update_training_data(sensor_id, data.value)
        
        # å¤šé‡å¼‚å¸¸æ£€æµ‹ç­–ç•¥
        anomaly_scores = []
        
        # 1. ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
        stat_score = self._statistical_anomaly_detection(sensor_id, data.value)
        anomaly_scores.append(stat_score)
        
        # 2. æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹
        if SKLEARN_AVAILABLE and self._has_sufficient_data(sensor_id):
            ml_score = self._ml_anomaly_detection(sensor_id, data.value)
            anomaly_scores.append(ml_score)
        
        # 3. åŸºäºçª—å£ç»Ÿè®¡çš„å¼‚å¸¸æ£€æµ‹
        if window_stats:
            window_score = self._window_based_anomaly_detection(data.value, window_stats)
            anomaly_scores.append(window_score)
        
        # é›†æˆå¼‚å¸¸åˆ†æ•°
        final_score = np.mean(anomaly_scores) if anomaly_scores else 0.0
        
        # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸
        is_anomaly = self._is_anomaly(sensor_id, final_score)
        
        # æ›´æ–°ç»Ÿè®¡
        self.detection_stats[sensor_id]['total_points'] += 1
        if is_anomaly:
            self.detection_stats[sensor_id]['anomalies_detected'] += 1
        
        return is_anomaly, float(final_score)
    
    def _initialize_detector(self, sensor_id: str):
        """åˆå§‹åŒ–ä¼ æ„Ÿå™¨æ£€æµ‹å™¨"""
        self.training_data[sensor_id] = deque(maxlen=self.window_size)
        
        if SKLEARN_AVAILABLE:
            self.detectors[sensor_id] = IsolationForest(
                contamination=self.contamination,
                random_state=42,
                n_estimators=50
            )
            self.scalers[sensor_id] = StandardScaler()
        
        # åˆå§‹åŒ–ç»Ÿè®¡é˜ˆå€¼
        self.thresholds[sensor_id] = {
            'z_score': 3.0,    # Z-scoreé˜ˆå€¼
            'iqr_factor': 1.5,  # IQRå› å­
            'percentile': 0.95  # ç™¾åˆ†ä½é˜ˆå€¼
        }
    
    def _update_training_data(self, sensor_id: str, value: float):
        """æ›´æ–°è®­ç»ƒæ•°æ®"""
        self.training_data[sensor_id].append(value)
        
        # å®šæœŸé‡è®­ç»ƒæ¨¡å‹
        if (len(self.training_data[sensor_id]) >= self.window_size and 
            len(self.training_data[sensor_id]) % 500 == 0):
            self._retrain_detector(sensor_id)
    
    def _retrain_detector(self, sensor_id: str):
        """é‡è®­ç»ƒæ£€æµ‹å™¨"""
        if not SKLEARN_AVAILABLE or sensor_id not in self.detectors:
            return
        
        try:
            data = np.array(list(self.training_data[sensor_id])).reshape(-1, 1)
            
            # æ ‡å‡†åŒ–æ•°æ®
            self.scalers[sensor_id].fit(data)
            normalized_data = self.scalers[sensor_id].transform(data)
            
            # è®­ç»ƒå¼‚å¸¸æ£€æµ‹å™¨
            self.detectors[sensor_id].fit(normalized_data)
            
            self.logger.info(f"ä¼ æ„Ÿå™¨ {sensor_id} æ£€æµ‹å™¨é‡è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ä¼ æ„Ÿå™¨ {sensor_id} æ£€æµ‹å™¨é‡è®­ç»ƒå¤±è´¥: {e}")
    
    def _statistical_anomaly_detection(self, sensor_id: str, value: float) -> float:
        """ç»Ÿè®¡æ–¹æ³•å¼‚å¸¸æ£€æµ‹"""
        if len(self.training_data[sensor_id]) < 10:
            return 0.0
        
        data = list(self.training_data[sensor_id])
        
        # Z-scoreæ£€æµ‹
        mean_val = statistics.mean(data)
        std_val = statistics.stdev(data) if len(data) > 1 else 1.0
        z_score = abs(value - mean_val) / std_val if std_val > 0 else 0.0
        
        # IQRæ£€æµ‹
        q1 = np.percentile(data, 25)
        q3 = np.percentile(data, 75)
        iqr = q3 - q1
        iqr_lower = q1 - 1.5 * iqr
        iqr_upper = q3 + 1.5 * iqr
        iqr_score = 1.0 if value < iqr_lower or value > iqr_upper else 0.0
        
        # ç»„åˆåˆ†æ•°
        statistical_score = min(1.0, z_score / self.thresholds[sensor_id]['z_score']) * 0.7 + iqr_score * 0.3
        
        return statistical_score
    
    def _ml_anomaly_detection(self, sensor_id: str, value: float) -> float:
        """æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹"""
        if sensor_id not in self.detectors or not self._has_sufficient_data(sensor_id):
            return 0.0
        
        try:
            # æ ‡å‡†åŒ–è¾“å…¥
            value_array = np.array([[value]])
            normalized_value = self.scalers[sensor_id].transform(value_array)
            
            # å¼‚å¸¸åˆ†æ•°
            anomaly_score = self.detectors[sensor_id].decision_function(normalized_value)[0]
            
            # è½¬æ¢ä¸º0-1èŒƒå›´
            normalized_score = max(0.0, min(1.0, -anomaly_score))
            
            return normalized_score
            
        except Exception as e:
            self.logger.warning(f"MLå¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
            return 0.0
    
    def _window_based_anomaly_detection(self, value: float, window_stats: Dict) -> float:
        """åŸºäºçª—å£ç»Ÿè®¡çš„å¼‚å¸¸æ£€æµ‹"""
        if 'statistics' not in window_stats:
            return 0.0
        
        stats = window_stats['statistics']
        
        # ç›¸å¯¹äºçª—å£å‡å€¼çš„åå·®
        mean_val = stats.get('mean', 0.0)
        std_val = stats.get('std', 1.0)
        
        if std_val > 0:
            deviation_score = abs(value - mean_val) / std_val / 3.0  # 3-sigmaè§„åˆ™
        else:
            deviation_score = 0.0
        
        # ç›¸å¯¹äºçª—å£èŒƒå›´çš„ä½ç½®
        min_val = stats.get('min', value)
        max_val = stats.get('max', value)
        
        if max_val > min_val:
            range_score = 0.0
            if value < min_val or value > max_val:
                range_score = min(abs(value - min_val), abs(value - max_val)) / (max_val - min_val)
        else:
            range_score = 0.0
        
        # ç»„åˆåˆ†æ•°
        window_score = min(1.0, deviation_score * 0.8 + range_score * 0.2)
        
        return window_score
    
    def _has_sufficient_data(self, sensor_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®"""
        return len(self.training_data[sensor_id]) >= 50
    
    def _is_anomaly(self, sensor_id: str, score: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¼‚å¸¸"""
        # è‡ªé€‚åº”é˜ˆå€¼
        base_threshold = 0.6
        
        # æ ¹æ®å†å²å¼‚å¸¸ç‡è°ƒæ•´é˜ˆå€¼
        stats = self.detection_stats[sensor_id]
        if stats['total_points'] > 100:
            anomaly_rate = stats['anomalies_detected'] / stats['total_points']
            
            # å¦‚æœå¼‚å¸¸ç‡è¿‡é«˜ï¼Œæé«˜é˜ˆå€¼
            if anomaly_rate > self.contamination * 2:
                base_threshold = min(0.9, base_threshold + 0.1)
            # å¦‚æœå¼‚å¸¸ç‡è¿‡ä½ï¼Œé™ä½é˜ˆå€¼
            elif anomaly_rate < self.contamination * 0.5:
                base_threshold = max(0.3, base_threshold - 0.1)
        
        return score > base_threshold
    
    def get_detection_statistics(self) -> Dict:
        """è·å–æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        return dict(self.detection_stats)

class AdaptiveFlowController:
    """è‡ªé€‚åº”æµé‡æ§åˆ¶å™¨"""
    
    def __init__(self, max_throughput: int = 10000, 
                 latency_threshold: float = 0.1,
                 adaptation_interval: float = 1.0):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”æµé‡æ§åˆ¶å™¨
        
        Args:
            max_throughput: æœ€å¤§ååé‡ï¼ˆç‚¹/ç§’ï¼‰
            latency_threshold: å»¶è¿Ÿé˜ˆå€¼ï¼ˆç§’ï¼‰
            adaptation_interval: é€‚åº”é—´éš”ï¼ˆç§’ï¼‰
        """
        self.max_throughput = max_throughput
        self.latency_threshold = latency_threshold
        self.adaptation_interval = adaptation_interval
        
        # æµé‡æ§åˆ¶çŠ¶æ€
        self.current_throughput = 0
        self.current_latency = 0.0
        self.drop_rate = 0.0
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.throughput_history = deque(maxlen=100)
        self.latency_history = deque(maxlen=100)
        self.last_adaptation = time.time()
        
        # æ§åˆ¶å‚æ•°
        self.backpressure_factor = 1.0
        self.priority_weights = defaultdict(float)
        
        self.logger = logging.getLogger(__name__)
    
    def should_process(self, data: StreamData) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤„ç†è¯¥æ•°æ®ç‚¹
        
        Args:
            data: æµæ•°æ®ç‚¹
            
        Returns:
            æ˜¯å¦åº”è¯¥å¤„ç†
        """
        # è·å–ä¼ æ„Ÿå™¨ä¼˜å…ˆçº§
        priority = self._get_sensor_priority(data.sensor_id)
        
        # è®¡ç®—å¤„ç†æ¦‚ç‡
        process_probability = self._calculate_process_probability(priority)
        
        # éšæœºé‡‡æ ·å†³ç­–
        return np.random.random() < process_probability
    
    def update_metrics(self, processing_time: float, queue_size: int):
        """
        æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        
        Args:
            processing_time: å¤„ç†æ—¶é—´
            queue_size: é˜Ÿåˆ—å¤§å°
        """
        current_time = time.time()
        
        # æ›´æ–°å»¶è¿Ÿ
        self.current_latency = processing_time
        self.latency_history.append(processing_time)
        
        # æ›´æ–°ååé‡ï¼ˆåŸºäºé˜Ÿåˆ—å¤§å°ä¼°ç®—ï¼‰
        estimated_throughput = min(self.max_throughput, 1.0 / processing_time if processing_time > 0 else self.max_throughput)
        self.current_throughput = estimated_throughput
        self.throughput_history.append(estimated_throughput)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é€‚åº”
        if current_time - self.last_adaptation >= self.adaptation_interval:
            self._adapt_flow_control()
            self.last_adaptation = current_time
    
    def _get_sensor_priority(self, sensor_id: str) -> float:
        """è·å–ä¼ æ„Ÿå™¨ä¼˜å…ˆçº§"""
        # é»˜è®¤ä¼˜å…ˆçº§æƒé‡
        default_weights = {
            'displacement': 1.0,    # ä½ç§»ä¼ æ„Ÿå™¨ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
            'strain': 0.9,          # åº”å˜ä¼ æ„Ÿå™¨
            'pressure': 0.8,        # å‹åŠ›ä¼ æ„Ÿå™¨
            'temperature': 0.5,     # æ¸©åº¦ä¼ æ„Ÿå™¨ï¼ˆä½ä¼˜å…ˆçº§ï¼‰
            'humidity': 0.3         # æ¹¿åº¦ä¼ æ„Ÿå™¨
        }
        
        # æ ¹æ®ä¼ æ„Ÿå™¨IDåˆ¤æ–­ç±»å‹
        for sensor_type, weight in default_weights.items():
            if sensor_type in sensor_id.lower():
                return weight
        
        # ä½¿ç”¨è‡ªé€‚åº”æƒé‡
        if sensor_id in self.priority_weights:
            return self.priority_weights[sensor_id]
        
        # é»˜è®¤ä¸­ç­‰ä¼˜å…ˆçº§
        return 0.7
    
    def _calculate_process_probability(self, priority: float) -> float:
        """è®¡ç®—å¤„ç†æ¦‚ç‡"""
        # åŸºç¡€æ¦‚ç‡
        base_probability = 1.0
        
        # æ ¹æ®å½“å‰å»¶è¿Ÿè°ƒæ•´
        if self.current_latency > self.latency_threshold:
            latency_penalty = min(0.5, (self.current_latency - self.latency_threshold) / self.latency_threshold)
            base_probability -= latency_penalty
        
        # æ ¹æ®ååé‡è°ƒæ•´
        if self.current_throughput > self.max_throughput * 0.9:
            throughput_penalty = 0.3
            base_probability -= throughput_penalty
        
        # åº”ç”¨ä¼˜å…ˆçº§æƒé‡
        final_probability = base_probability * priority * self.backpressure_factor
        
        return max(0.1, min(1.0, final_probability))  # é™åˆ¶åœ¨[0.1, 1.0]èŒƒå›´
    
    def _adapt_flow_control(self):
        """è‡ªé€‚åº”æµé‡æ§åˆ¶"""
        if not self.latency_history or not self.throughput_history:
            return
        
        # è®¡ç®—å¹³å‡å»¶è¿Ÿå’Œååé‡
        avg_latency = statistics.mean(self.latency_history)
        avg_throughput = statistics.mean(self.throughput_history)
        
        # è°ƒæ•´èƒŒå‹å› å­
        if avg_latency > self.latency_threshold * 1.2:
            # å»¶è¿Ÿè¿‡é«˜ï¼Œå¢åŠ èƒŒå‹
            self.backpressure_factor = max(0.5, self.backpressure_factor - 0.1)
            self.logger.info(f"å»¶è¿Ÿè¿‡é«˜({avg_latency:.3f}s)ï¼Œé™ä½èƒŒå‹å› å­è‡³{self.backpressure_factor:.2f}")
        elif avg_latency < self.latency_threshold * 0.8:
            # å»¶è¿Ÿè¾ƒä½ï¼Œå¯ä»¥æ”¾æ¾èƒŒå‹
            self.backpressure_factor = min(1.0, self.backpressure_factor + 0.05)
            self.logger.info(f"å»¶è¿Ÿè‰¯å¥½({avg_latency:.3f}s)ï¼Œæé«˜èƒŒå‹å› å­è‡³{self.backpressure_factor:.2f}")
        
        # æ›´æ–°ä¸¢å¼ƒç‡ç»Ÿè®¡
        self.drop_rate = 1.0 - self.backpressure_factor
    
    def get_flow_statistics(self) -> Dict:
        """è·å–æµé‡æ§åˆ¶ç»Ÿè®¡"""
        return {
            'current_throughput': self.current_throughput,
            'current_latency': self.current_latency,
            'drop_rate': self.drop_rate,
            'backpressure_factor': self.backpressure_factor,
            'avg_latency': statistics.mean(self.latency_history) if self.latency_history else 0.0,
            'avg_throughput': statistics.mean(self.throughput_history) if self.throughput_history else 0.0
        }

class DistributedStreamProcessor:
    """åˆ†å¸ƒå¼æµå¤„ç†å™¨"""
    
    def __init__(self, num_workers: int = 4, buffer_size: int = 10000):
        """
        åˆå§‹åŒ–åˆ†å¸ƒå¼æµå¤„ç†å™¨
        
        Args:
            num_workers: å·¥ä½œçº¿ç¨‹æ•°
            buffer_size: ç¼“å†²åŒºå¤§å°
        """
        self.num_workers = num_workers
        self.buffer_size = buffer_size
        self.logger = logging.getLogger(__name__)
        
        # å¤„ç†ç»„ä»¶
        self.window_aggregator = SlidingWindowAggregator()
        self.anomaly_detector = OnlineAnomalyDetector()
        self.flow_controller = AdaptiveFlowController()
        
        # çº¿ç¨‹æ± å’Œé˜Ÿåˆ—
        self.executor = ThreadPoolExecutor(max_workers=num_workers)
        self.input_queue = queue.Queue(maxsize=buffer_size)
        self.output_queue = queue.Queue(maxsize=buffer_size)
        
        # å¤„ç†çŠ¶æ€
        self.is_running = False
        self.worker_threads = []
        self.stats_thread = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.processing_stats = {
            'total_processed': 0,
            'total_anomalies': 0,
            'total_dropped': 0,
            'processing_times': deque(maxlen=1000),
            'start_time': time.time()
        }
        
        # GPUå¤„ç†æ”¯æŒ
        self.use_gpu = CUPY_AVAILABLE
        if self.use_gpu:
            self.gpu_batch_size = 1000
            self.gpu_buffer = []
    
    async def start_processing(self):
        """å¯åŠ¨æµå¤„ç†"""
        if self.is_running:
            self.logger.warning("æµå¤„ç†å™¨å·²åœ¨è¿è¡Œ")
            return
        
        self.is_running = True
        self.logger.info(f"å¯åŠ¨åˆ†å¸ƒå¼æµå¤„ç†å™¨ï¼Œå·¥ä½œçº¿ç¨‹æ•°: {self.num_workers}")
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for i in range(self.num_workers):
            thread = threading.Thread(target=self._worker_loop, args=(i,), daemon=True)
            thread.start()
            self.worker_threads.append(thread)
        
        # å¯åŠ¨ç»Ÿè®¡çº¿ç¨‹
        self.stats_thread = threading.Thread(target=self._stats_loop, daemon=True)
        self.stats_thread.start()
    
    def stop_processing(self):
        """åœæ­¢æµå¤„ç†"""
        self.is_running = False
        self.logger.info("åœæ­¢åˆ†å¸ƒå¼æµå¤„ç†å™¨")
        
        # ç­‰å¾…å·¥ä½œçº¿ç¨‹ç»“æŸ
        for thread in self.worker_threads:
            thread.join(timeout=5.0)
        
        self.worker_threads.clear()
        
        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=True)
    
    async def process_stream_data(self, data: StreamData) -> Optional[ProcessingResult]:
        """
        å¤„ç†æµæ•°æ®
        
        Args:
            data: æµæ•°æ®ç‚¹
            
        Returns:
            å¤„ç†ç»“æœ
        """
        # æµé‡æ§åˆ¶æ£€æŸ¥
        if not self.flow_controller.should_process(data):
            self.processing_stats['total_dropped'] += 1
            return None
        
        try:
            # æ·»åŠ åˆ°è¾“å…¥é˜Ÿåˆ—
            self.input_queue.put_nowait(data)
            return await self._get_processing_result()
        except queue.Full:
            self.processing_stats['total_dropped'] += 1
            self.logger.warning("è¾“å…¥é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒæ•°æ®ç‚¹")
            return None
    
    def _worker_loop(self, worker_id: int):
        """å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯"""
        self.logger.info(f"å·¥ä½œçº¿ç¨‹ {worker_id} å¯åŠ¨")
        
        while self.is_running:
            try:
                # è·å–è¾“å…¥æ•°æ®
                data = self.input_queue.get(timeout=1.0)
                
                # å¤„ç†æ•°æ®
                result = self._process_single_data(data)
                
                # è¾“å‡ºç»“æœ
                if result:
                    self.output_queue.put_nowait(result)
                
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.input_queue.task_done()
                
            except queue.Empty:
                continue
            except queue.Full:
                self.logger.warning(f"å·¥ä½œçº¿ç¨‹ {worker_id}: è¾“å‡ºé˜Ÿåˆ—å·²æ»¡")
            except Exception as e:
                self.logger.error(f"å·¥ä½œçº¿ç¨‹ {worker_id} å¤„ç†é”™è¯¯: {e}")
        
        self.logger.info(f"å·¥ä½œçº¿ç¨‹ {worker_id} ç»“æŸ")
    
    def _process_single_data(self, data: StreamData) -> Optional[ProcessingResult]:
        """å¤„ç†å•ä¸ªæ•°æ®ç‚¹"""
        start_time = time.time()
        
        try:
            # 1. æ»‘åŠ¨çª—å£èšåˆ
            window_stats = self.window_aggregator.add_data_point(data)
            
            # 2. å¼‚å¸¸æ£€æµ‹
            is_anomaly, anomaly_score = self.anomaly_detector.detect_anomaly(data, window_stats)
            
            # 3. æ•°æ®é¢„å¤„ç†ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
            processed_value = self._preprocess_value(data.value)
            
            # 4. åˆ›å»ºå¤„ç†ç»“æœ
            processing_time = time.time() - start_time
            
            result = ProcessingResult(
                original_data=data,
                processed_value=processed_value,
                anomaly_score=anomaly_score,
                is_anomaly=is_anomaly,
                processing_time=processing_time,
                stage='complete'
            )
            
            # 5. æ›´æ–°ç»Ÿè®¡
            self.processing_stats['total_processed'] += 1
            if is_anomaly:
                self.processing_stats['total_anomalies'] += 1
            self.processing_stats['processing_times'].append(processing_time)
            
            # 6. æ›´æ–°æµé‡æ§åˆ¶æŒ‡æ ‡
            self.flow_controller.update_metrics(processing_time, self.input_queue.qsize())
            
            return result
            
        except Exception as e:
            self.logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return None
    
    def _preprocess_value(self, value: float) -> float:
        """é¢„å¤„ç†æ•°å€¼"""
        if self.use_gpu and len(self.gpu_buffer) < self.gpu_batch_size:
            # GPUæ‰¹å¤„ç†æ¨¡å¼
            self.gpu_buffer.append(value)
            
            if len(self.gpu_buffer) >= self.gpu_batch_size:
                processed_batch = self._gpu_batch_process()
                self.gpu_buffer.clear()
                return processed_batch[-1]  # è¿”å›å½“å‰å€¼çš„å¤„ç†ç»“æœ
            else:
                return value  # ç­‰å¾…æ‰¹å¤„ç†
        else:
            # CPUå•ç‚¹å¤„ç†
            return self._cpu_process_value(value)
    
    def _gpu_batch_process(self) -> List[float]:
        """GPUæ‰¹å¤„ç†"""
        try:
            # è½¬æ¢ä¸ºGPUæ•°ç»„
            gpu_data = cp.array(self.gpu_buffer, dtype=cp.float32)
            
            # GPUå¹¶è¡Œå¤„ç†ï¼ˆç¤ºä¾‹ï¼šæ ‡å‡†åŒ– + æ»¤æ³¢ï¼‰
            gpu_normalized = (gpu_data - cp.mean(gpu_data)) / cp.std(gpu_data)
            
            # ç®€å•ä½é€šæ»¤æ³¢
            kernel = cp.array([0.2, 0.6, 0.2], dtype=cp.float32)
            gpu_filtered = cp.convolve(gpu_normalized, kernel, mode='same')
            
            # è½¬å›CPU
            return gpu_filtered.get().tolist()
            
        except Exception as e:
            self.logger.error(f"GPUæ‰¹å¤„ç†å¤±è´¥: {e}")
            # å›é€€åˆ°CPUå¤„ç†
            return [self._cpu_process_value(v) for v in self.gpu_buffer]
    
    def _cpu_process_value(self, value: float) -> float:
        """CPUå•ç‚¹å¤„ç†"""
        # ç®€å•çš„æ•°æ®é¢„å¤„ç†
        # 1. å»é™¤æ˜æ˜¾é”™è¯¯å€¼
        if abs(value) > 1e6:
            return 0.0
        
        # 2. ç®€å•å¹³æ»‘
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ»¤æ³¢ç®—æ³•
        return float(value)
    
    async def _get_processing_result(self) -> Optional[ProcessingResult]:
        """è·å–å¤„ç†ç»“æœ"""
        try:
            # å¼‚æ­¥ç­‰å¾…ç»“æœ
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                lambda: self.output_queue.get(timeout=5.0)
            )
            self.output_queue.task_done()
            return result
        except queue.Empty:
            return None
    
    def _stats_loop(self):
        """ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å¾ªç¯"""
        while self.is_running:
            try:
                time.sleep(5.0)  # æ¯5ç§’æ”¶é›†ä¸€æ¬¡ç»Ÿè®¡
                self._log_performance_stats()
            except Exception as e:
                self.logger.error(f"ç»Ÿè®¡æ”¶é›†é”™è¯¯: {e}")
    
    def _log_performance_stats(self):
        """è®°å½•æ€§èƒ½ç»Ÿè®¡"""
        stats = self.get_performance_statistics()
        
        self.logger.info(
            f"æ€§èƒ½ç»Ÿè®¡ - å¤„ç†æ€»æ•°: {stats['total_processed']}, "
            f"å¼‚å¸¸æ£€æµ‹: {stats['total_anomalies']}, "
            f"å¹³å‡å»¶è¿Ÿ: {stats['avg_processing_time']:.3f}s, "
            f"ååé‡: {stats['throughput']:.1f}/s"
        )
    
    def get_performance_statistics(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        current_time = time.time()
        elapsed_time = current_time - self.processing_stats['start_time']
        
        stats = {
            'total_processed': self.processing_stats['total_processed'],
            'total_anomalies': self.processing_stats['total_anomalies'],
            'total_dropped': self.processing_stats['total_dropped'],
            'elapsed_time': elapsed_time,
            'throughput': self.processing_stats['total_processed'] / elapsed_time if elapsed_time > 0 else 0.0,
            'anomaly_rate': (self.processing_stats['total_anomalies'] / 
                           max(1, self.processing_stats['total_processed'])),
            'drop_rate': (self.processing_stats['total_dropped'] / 
                         max(1, self.processing_stats['total_processed'] + self.processing_stats['total_dropped']))
        }
        
        # å¤„ç†æ—¶é—´ç»Ÿè®¡
        if self.processing_stats['processing_times']:
            processing_times = list(self.processing_stats['processing_times'])
            stats.update({
                'avg_processing_time': statistics.mean(processing_times),
                'min_processing_time': min(processing_times),
                'max_processing_time': max(processing_times),
                'p95_processing_time': np.percentile(processing_times, 95)
            })
        else:
            stats.update({
                'avg_processing_time': 0.0,
                'min_processing_time': 0.0,
                'max_processing_time': 0.0,
                'p95_processing_time': 0.0
            })
        
        # æ·»åŠ ç»„ä»¶ç»Ÿè®¡
        stats['flow_control'] = self.flow_controller.get_flow_statistics()
        stats['anomaly_detection'] = self.anomaly_detector.get_detection_statistics()
        
        return stats
    
    def get_system_health(self) -> Dict:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        stats = self.get_performance_statistics()
        
        # å¥åº·è¯„åˆ†ç®—æ³•
        health_score = 100.0
        
        # å»¶è¿Ÿå¥åº·
        if stats['avg_processing_time'] > 0.2:
            health_score -= 20.0
        elif stats['avg_processing_time'] > 0.1:
            health_score -= 10.0
        
        # ä¸¢å¼ƒç‡å¥åº·
        if stats['drop_rate'] > 0.1:
            health_score -= 30.0
        elif stats['drop_rate'] > 0.05:
            health_score -= 15.0
        
        # å¼‚å¸¸ç‡å¥åº·
        if stats['anomaly_rate'] > 0.2:
            health_score -= 20.0
        elif stats['anomaly_rate'] > 0.1:
            health_score -= 10.0
        
        # ååé‡å¥åº·
        if stats['throughput'] < 1000:
            health_score -= 20.0
        elif stats['throughput'] < 5000:
            health_score -= 10.0
        
        health_score = max(0.0, health_score)
        
        # å¥åº·ç­‰çº§
        if health_score >= 90:
            health_level = "Excellent"
        elif health_score >= 75:
            health_level = "Good"
        elif health_score >= 60:
            health_level = "Fair"
        elif health_score >= 40:
            health_level = "Poor"
        else:
            health_level = "Critical"
        
        return {
            'health_score': health_score,
            'health_level': health_level,
            'is_running': self.is_running,
            'worker_count': len(self.worker_threads),
            'queue_sizes': {
                'input': self.input_queue.qsize(),
                'output': self.output_queue.qsize()
            },
            'recommendations': self._get_health_recommendations(health_score, stats)
        }
    
    def _get_health_recommendations(self, health_score: float, stats: Dict) -> List[str]:
        """è·å–å¥åº·æ”¹å–„å»ºè®®"""
        recommendations = []
        
        if stats['avg_processing_time'] > 0.1:
            recommendations.append("è€ƒè™‘å¢åŠ å·¥ä½œçº¿ç¨‹æ•°é‡ä»¥é™ä½å¤„ç†å»¶è¿Ÿ")
        
        if stats['drop_rate'] > 0.05:
            recommendations.append("ä¼˜åŒ–æµé‡æ§åˆ¶ç­–ç•¥æˆ–å¢åŠ ç¼“å†²åŒºå¤§å°")
        
        if stats['anomaly_rate'] > 0.15:
            recommendations.append("æ£€æŸ¥ä¼ æ„Ÿå™¨æ•°æ®è´¨é‡æˆ–è°ƒæ•´å¼‚å¸¸æ£€æµ‹é˜ˆå€¼")
        
        if stats['throughput'] < 5000:
            recommendations.append("å¯ç”¨GPUåŠ é€Ÿæˆ–ä¼˜åŒ–æ•°æ®å¤„ç†ç®—æ³•")
        
        if health_score < 60:
            recommendations.append("å»ºè®®è¿›è¡Œç³»ç»Ÿç»´æŠ¤å’Œæ€§èƒ½è°ƒä¼˜")
        
        return recommendations

# æ·±åŸºå‘å·¥ç¨‹ä¸“ç”¨æµå¤„ç†å™¨
class DeepExcavationStreamProcessor(DistributedStreamProcessor):
    """æ·±åŸºå‘å·¥ç¨‹æµå¤„ç†å™¨"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # æ·±åŸºå‘ä¸“ç”¨é…ç½®
        self._configure_for_excavation()
    
    def _configure_for_excavation(self):
        """æ·±åŸºå‘å·¥ç¨‹ä¸“ç”¨é…ç½®"""
        # è°ƒæ•´å¼‚å¸¸æ£€æµ‹å‚æ•°ï¼ˆæ·±åŸºå‘å¯¹å®‰å…¨è¦æ±‚é«˜ï¼‰
        self.anomaly_detector.contamination = 0.05  # é™ä½å¼‚å¸¸å®¹å¿åº¦
        
        # è®¾ç½®ä¼ æ„Ÿå™¨ä¼˜å…ˆçº§
        excavation_priorities = {
            'displacement_sensor': 1.0,      # ä½ç§»ä¼ æ„Ÿå™¨æœ€é«˜ä¼˜å…ˆçº§
            'inclinometer': 0.95,            # å€¾æ–œä»ª
            'strain_gauge': 0.9,             # åº”å˜è®¡
            'piezometer': 0.85,              # å­”éš™æ°´å‹åŠ›è®¡
            'settlement_marker': 0.9,        # æ²‰é™æ ‡
            'crack_monitor': 0.95,           # è£‚ç¼ç›‘æµ‹
            'load_cell': 0.8,                # è·é‡ä¼ æ„Ÿå™¨
            'temperature_sensor': 0.4,       # æ¸©åº¦ä¼ æ„Ÿå™¨
            'humidity_sensor': 0.3           # æ¹¿åº¦ä¼ æ„Ÿå™¨
        }
        
        self.flow_controller.priority_weights.update(excavation_priorities)
        
        # è°ƒæ•´æµé‡æ§åˆ¶å‚æ•°
        self.flow_controller.latency_threshold = 0.05  # æ›´ä¸¥æ ¼çš„å»¶è¿Ÿè¦æ±‚
        
        self.logger.info("æ·±åŸºå‘å·¥ç¨‹æµå¤„ç†å™¨é…ç½®å®Œæˆ")

if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    async def test_stream_processor():
        """æµå¤„ç†å™¨æµ‹è¯•"""
        print("ğŸŒŠ DeepCADå®æ—¶æ•°æ®æµå¤„ç†å™¨æµ‹è¯• ğŸŒŠ")
        
        # åˆ›å»ºæ·±åŸºå‘æµå¤„ç†å™¨
        processor = DeepExcavationStreamProcessor(
            num_workers=6,
            buffer_size=5000
        )
        
        # å¯åŠ¨å¤„ç†å™¨
        await processor.start_processing()
        
        print("âš¡ å¼€å§‹æ¨¡æ‹Ÿæ•°æ®æµ...")
        
        # æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®æµ
        sensor_types = [
            'displacement_sensor_1', 'displacement_sensor_2',
            'inclinometer_1', 'strain_gauge_1', 'piezometer_1',
            'settlement_marker_1', 'temperature_sensor_1'
        ]
        
        start_time = time.time()
        processed_count = 0
        
        # æ¨¡æ‹Ÿ10000ä¸ªæ•°æ®ç‚¹
        for i in range(10000):
            # éšæœºé€‰æ‹©ä¼ æ„Ÿå™¨
            sensor_id = np.random.choice(sensor_types)
            
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆåŒ…å«ä¸€äº›å¼‚å¸¸å€¼ï¼‰
            if np.random.random() < 0.05:  # 5%å¼‚å¸¸æ•°æ®
                value = np.random.normal(0, 10)  # å¼‚å¸¸å€¼
            else:
                value = np.random.normal(0, 1)   # æ­£å¸¸å€¼
            
            # åˆ›å»ºæ•°æ®ç‚¹
            data = StreamData(
                sensor_id=sensor_id,
                timestamp=time.time(),
                value=value,
                metadata={'source': 'simulation'}
            )
            
            # å¤„ç†æ•°æ®
            result = await processor.process_stream_data(data)
            if result:
                processed_count += 1
                
                # æ¯1000ä¸ªç‚¹è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if processed_count % 1000 == 0:
                    print(f"  å·²å¤„ç†: {processed_count}/10000 "
                          f"(å¼‚å¸¸: {'æ˜¯' if result.is_anomaly else 'å¦'}, "
                          f"åˆ†æ•°: {result.anomaly_score:.3f})")
            
            # æ§åˆ¶æ•°æ®ç”Ÿæˆé€Ÿç‡
            if i % 100 == 0:
                await asyncio.sleep(0.01)  # 10mså»¶è¿Ÿ
        
        # ç­‰å¾…å¤„ç†å®Œæˆ
        await asyncio.sleep(2.0)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = processor.get_performance_statistics()
        health = processor.get_system_health()
        
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"  æ€»å¤„ç†æ•°: {stats['total_processed']}")
        print(f"  å¼‚å¸¸æ£€æµ‹: {stats['total_anomalies']}")
        print(f"  ä¸¢å¼ƒæ•°æ®: {stats['total_dropped']}")
        print(f"  å¹³å‡å»¶è¿Ÿ: {stats['avg_processing_time']:.3f}ç§’")
        print(f"  ååé‡: {stats['throughput']:.1f}ç‚¹/ç§’")
        print(f"  å¼‚å¸¸ç‡: {stats['anomaly_rate']:.1%}")
        
        print(f"\nğŸ¥ ç³»ç»Ÿå¥åº·:")
        print(f"  å¥åº·è¯„åˆ†: {health['health_score']:.1f}/100")
        print(f"  å¥åº·ç­‰çº§: {health['health_level']}")
        print(f"  é˜Ÿåˆ—çŠ¶æ€: è¾“å…¥{health['queue_sizes']['input']}, è¾“å‡º{health['queue_sizes']['output']}")
        
        if health['recommendations']:
            print(f"  æ”¹å–„å»ºè®®:")
            for rec in health['recommendations']:
                print(f"    - {rec}")
        
        # åœæ­¢å¤„ç†å™¨
        processor.stop_processing()
        
        print("\nâœ… å®æ—¶æ•°æ®æµå¤„ç†å™¨æµ‹è¯•å®Œæˆï¼")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_stream_processor())