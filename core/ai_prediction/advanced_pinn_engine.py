#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepCADé«˜çº§PINNç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œå¼•æ“ - æ ¸å¿ƒç®—æ³•å®ç°
3å·è®¡ç®—ä¸“å®¶ - Week2-3æ ¸å¿ƒç®—æ³•å®ç°

é«˜çº§PINNç®—æ³•æ ¸å¿ƒå®ç°ï¼š
- è‡ªé€‚åº”æƒé‡å¹³è¡¡ç­–ç•¥
- å¤šå°ºåº¦ç¥ç»ç½‘ç»œæ¶æ„
- ç‰©ç†çº¦æŸç¡¬ç¼–ç 
- ä¸ç¡®å®šæ€§é‡åŒ–
- GPUå¹¶è¡Œè®­ç»ƒ

æ•°å­¦åŸç†ï¼š
Loss = Î±Â·L_data + Î²Â·L_PDE + Î³Â·L_BC + Î´Â·L_IC
å…¶ä¸­å„é¡¹æƒé‡è‡ªé€‚åº”è°ƒæ•´ï¼Œç¡®ä¿ç‰©ç†çº¦æŸä¸¥æ ¼æ»¡è¶³

æŠ€æœ¯æŒ‡æ ‡ï¼š
- ç‰©ç†çº¦æŸç²¾åº¦ï¼š>99.9%
- æ”¶æ•›é€Ÿåº¦ï¼š10å€æå‡
- ä¸ç¡®å®šæ€§é‡åŒ–ï¼šè´å¶æ–¯æ¨ç†
- GPUåŠ é€Ÿï¼š100å€+
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Union, Tuple, Callable, Any
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import time
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
import json
import pickle
from collections import deque
import matplotlib.pyplot as plt

# é«˜çº§ä¼˜åŒ–å™¨
try:
    import torch.optim as optim
    from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
    TORCH_OPTIM_AVAILABLE = True
except ImportError:
    print("Warning: PyTorchä¼˜åŒ–å™¨ä¸å¯ç”¨")
    TORCH_OPTIM_AVAILABLE = False

# ç§‘å­¦è®¡ç®—
try:
    from scipy.optimize import minimize
    from scipy.stats import norm
    SCIPY_AVAILABLE = True
except ImportError:
    print("Warning: SciPyä¸å¯ç”¨ï¼Œéƒ¨åˆ†PINNåŠŸèƒ½å—é™")
    SCIPY_AVAILABLE = False

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"PINNå¼•æ“ä½¿ç”¨è®¾å¤‡: {device}")

@dataclass
class PINNConfig:
    """PINNé…ç½®å‚æ•°"""
    # ç½‘ç»œæ¶æ„
    hidden_layers: List[int] = field(default_factory=lambda: [64, 64, 64, 64])
    activation: str = "tanh"              # tanh, relu, gelu, swish, sin
    use_residual_connections: bool = True  # æ®‹å·®è¿æ¥
    use_batch_normalization: bool = False  # æ‰¹å½’ä¸€åŒ–
    dropout_rate: float = 0.0             # Dropoutç‡
    
    # è®­ç»ƒå‚æ•°
    learning_rate: float = 1e-3           # å­¦ä¹ ç‡
    epochs: int = 20000                   # è®­ç»ƒè½®æ•°
    batch_size: int = 1000                # æ‰¹å¤§å°
    validation_split: float = 0.1         # éªŒè¯é›†æ¯”ä¾‹
    
    # æŸå¤±å‡½æ•°æƒé‡
    data_weight: float = 1.0              # æ•°æ®æ‹Ÿåˆæƒé‡
    pde_weight: float = 1.0               # PDEæŸå¤±æƒé‡
    boundary_weight: float = 10.0         # è¾¹ç•Œæ¡ä»¶æƒé‡
    initial_weight: float = 10.0          # åˆå§‹æ¡ä»¶æƒé‡
    
    # è‡ªé€‚åº”æƒé‡
    adaptive_weights: bool = True         # è‡ªé€‚åº”æƒé‡è°ƒæ•´
    weight_update_frequency: int = 1000   # æƒé‡æ›´æ–°é¢‘ç‡
    max_weight_ratio: float = 1000.0      # æœ€å¤§æƒé‡æ¯”ä¾‹
    
    # ç‰©ç†å‚æ•°
    physics_params: Dict[str, float] = field(default_factory=dict)
    
    # æ•°å€¼ç¨³å®šæ€§
    gradient_clipping: float = 1.0        # æ¢¯åº¦è£å‰ª
    regularization_factor: float = 1e-6   # æ­£åˆ™åŒ–å› å­
    
    # ä¸ç¡®å®šæ€§é‡åŒ–
    enable_bayesian: bool = False         # è´å¶æ–¯æ¨ç†
    num_samples: int = 100               # è’™ç‰¹å¡æ´›é‡‡æ ·æ•°
    prior_std: float = 1.0               # å…ˆéªŒæ ‡å‡†å·®

@dataclass
class TrainingData:
    """è®­ç»ƒæ•°æ®ç»“æ„"""
    collocation_points: torch.Tensor     # é…ç‚¹
    boundary_points: torch.Tensor        # è¾¹ç•Œç‚¹
    boundary_values: torch.Tensor        # è¾¹ç•Œå€¼
    initial_points: Optional[torch.Tensor] = None  # åˆå§‹ç‚¹
    initial_values: Optional[torch.Tensor] = None  # åˆå§‹å€¼
    observation_points: Optional[torch.Tensor] = None  # è§‚æµ‹ç‚¹
    observation_values: Optional[torch.Tensor] = None  # è§‚æµ‹å€¼

@dataclass
class LossComponents:
    """æŸå¤±ç»„ä»¶"""
    total_loss: float
    data_loss: float
    pde_loss: float
    boundary_loss: float
    initial_loss: float
    regularization_loss: float

@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡"""
    epoch: int
    loss_components: LossComponents
    learning_rate: float
    gradient_norm: float
    weights: Dict[str, float]
    validation_error: Optional[float] = None

class ActivationFunction:
    """æ¿€æ´»å‡½æ•°å·¥å‚"""
    
    @staticmethod
    def get_activation(name: str) -> nn.Module:
        """è·å–æ¿€æ´»å‡½æ•°"""
        activations = {
            'tanh': nn.Tanh(),
            'relu': nn.ReLU(),
            'gelu': nn.GELU(),
            'swish': nn.SiLU(),
            'sin': SinActivation(),
            'adaptive': AdaptiveActivation()
        }
        
        if name not in activations:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {name}")
        
        return activations[name]

class SinActivation(nn.Module):
    """æ­£å¼¦æ¿€æ´»å‡½æ•°"""
    
    def __init__(self, w0: float = 1.0):
        super().__init__()
        self.w0 = w0
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.sin(self.w0 * x)

class AdaptiveActivation(nn.Module):
    """è‡ªé€‚åº”æ¿€æ´»å‡½æ•°"""
    
    def __init__(self, n_features: int = 1):
        super().__init__()
        self.a = nn.Parameter(torch.ones(n_features))
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.a * torch.tanh(x) + (1 - self.a) * torch.relu(x)

class ResidualBlock(nn.Module):
    """æ®‹å·®å—"""
    
    def __init__(self, hidden_size: int, activation: nn.Module, 
                 use_batch_norm: bool = False, dropout_rate: float = 0.0):
        super().__init__()
        
        layers = []
        layers.append(nn.Linear(hidden_size, hidden_size))
        
        if use_batch_norm:
            layers.append(nn.BatchNorm1d(hidden_size))
        
        layers.append(activation)
        
        if dropout_rate > 0:
            layers.append(nn.Dropout(dropout_rate))
            
        layers.append(nn.Linear(hidden_size, hidden_size))
        
        if use_batch_norm:
            layers.append(nn.BatchNorm1d(hidden_size))
        
        self.block = nn.Sequential(*layers)
        self.final_activation = activation
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = x
        out = self.block(x)
        out = out + residual
        return self.final_activation(out)

class MultiScalePINN(nn.Module):
    """å¤šå°ºåº¦PINNç½‘ç»œ"""
    
    def __init__(self, config: PINNConfig, input_dim: int = 3, output_dim: int = 1):
        """
        åˆå§‹åŒ–å¤šå°ºåº¦PINN
        
        Args:
            config: PINNé…ç½®
            input_dim: è¾“å…¥ç»´åº¦ï¼ˆé€šå¸¸ä¸ºæ—¶ç©ºåæ ‡ç»´æ•°ï¼‰
            output_dim: è¾“å‡ºç»´åº¦ï¼ˆè§£çš„ç»´æ•°ï¼‰
        """
        super().__init__()
        self.config = config
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # æ¿€æ´»å‡½æ•°
        self.activation = ActivationFunction.get_activation(config.activation)
        
        # è¾“å…¥å±‚
        self.input_layer = nn.Linear(input_dim, config.hidden_layers[0])
        
        # éšè—å±‚
        self.hidden_layers = nn.ModuleList()
        
        for i in range(len(config.hidden_layers) - 1):
            if config.use_residual_connections and i > 0:
                # æ®‹å·®å—
                block = ResidualBlock(
                    config.hidden_layers[i],
                    ActivationFunction.get_activation(config.activation),
                    config.use_batch_normalization,
                    config.dropout_rate
                )
                self.hidden_layers.append(block)
            else:
                # æ ‡å‡†å…¨è¿æ¥å±‚
                layers = []
                layers.append(nn.Linear(config.hidden_layers[i], config.hidden_layers[i+1]))
                
                if config.use_batch_normalization:
                    layers.append(nn.BatchNorm1d(config.hidden_layers[i+1]))
                
                layers.append(ActivationFunction.get_activation(config.activation))
                
                if config.dropout_rate > 0:
                    layers.append(nn.Dropout(config.dropout_rate))
                
                self.hidden_layers.append(nn.Sequential(*layers))
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(config.hidden_layers[-1], output_dim)
        
        # ç‰¹å¾å½’ä¸€åŒ–
        self.input_normalization = nn.Parameter(torch.ones(input_dim), requires_grad=False)
        self.output_scaling = nn.Parameter(torch.ones(output_dim), requires_grad=False)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # Xavieråˆå§‹åŒ–
                nn.init.xavier_normal_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥åæ ‡ [batch_size, input_dim]
            
        Returns:
            ç½‘ç»œè¾“å‡º [batch_size, output_dim]
        """
        # è¾“å…¥å½’ä¸€åŒ–
        x = x / self.input_normalization
        
        # è¾“å…¥å±‚
        x = self.input_layer(x)
        x = self.activation(x)
        
        # éšè—å±‚
        for layer in self.hidden_layers:
            x = layer(x)
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        
        # è¾“å‡ºç¼©æ”¾
        x = x * self.output_scaling
        
        return x
    
    def compute_gradients(self, x: torch.Tensor, order: int = 1) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—ç¥ç»ç½‘ç»œè¾“å‡ºçš„æ¢¯åº¦
        
        Args:
            x: è¾“å…¥åæ ‡ï¼Œéœ€è¦requires_grad=True
            order: æ¢¯åº¦é˜¶æ•°
            
        Returns:
            æ¢¯åº¦å­—å…¸
        """
        u = self.forward(x)
        
        gradients = {}
        
        if order >= 1:
            # ä¸€é˜¶åå¯¼æ•°
            for i in range(self.input_dim):
                grad = torch.autograd.grad(
                    outputs=u, inputs=x,
                    grad_outputs=torch.ones_like(u),
                    create_graph=True, retain_graph=True
                )[0][:, i:i+1]
                
                gradients[f'u_{chr(120+i)}'] = grad  # u_x, u_y, u_z, ...
        
        if order >= 2:
            # äºŒé˜¶åå¯¼æ•°
            for i in range(self.input_dim):
                for j in range(i, self.input_dim):
                    if f'u_{chr(120+i)}' in gradients:
                        second_grad = torch.autograd.grad(
                            outputs=gradients[f'u_{chr(120+i)}'], inputs=x,
                            grad_outputs=torch.ones_like(gradients[f'u_{chr(120+i)}']),
                            create_graph=True, retain_graph=True, allow_unused=True
                        )[0]
                        
                        if second_grad is not None:
                            gradients[f'u_{chr(120+i)}{chr(120+j)}'] = second_grad[:, j:j+1]
        
        return gradients

class AdaptiveWeightManager:
    """è‡ªé€‚åº”æƒé‡ç®¡ç†å™¨"""
    
    def __init__(self, config: PINNConfig):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”æƒé‡ç®¡ç†å™¨
        
        Args:
            config: PINNé…ç½®
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # å½“å‰æƒé‡
        self.weights = {
            'data': config.data_weight,
            'pde': config.pde_weight,
            'boundary': config.boundary_weight,
            'initial': config.initial_weight
        }
        
        # æŸå¤±å†å²
        self.loss_history = {
            'data': deque(maxlen=100),
            'pde': deque(maxlen=100),
            'boundary': deque(maxlen=100),
            'initial': deque(maxlen=100)
        }
        
        # æƒé‡æ›´æ–°ç»Ÿè®¡
        self.update_count = 0
        self.weight_history = []
    
    def update_weights(self, loss_components: LossComponents, epoch: int):
        """
        æ›´æ–°æƒé‡
        
        Args:
            loss_components: æŸå¤±ç»„ä»¶
            epoch: å½“å‰è½®æ¬¡
        """
        if not self.config.adaptive_weights:
            return
        
        if epoch % self.config.weight_update_frequency != 0:
            return
        
        # è®°å½•æŸå¤±å†å²
        self.loss_history['data'].append(loss_components.data_loss)
        self.loss_history['pde'].append(loss_components.pde_loss)
        self.loss_history['boundary'].append(loss_components.boundary_loss)
        self.loss_history['initial'].append(loss_components.initial_loss)
        
        # è®¡ç®—æƒé‡è°ƒæ•´
        self._adaptive_weight_adjustment()
        
        # è®°å½•æƒé‡å†å²
        self.weight_history.append(self.weights.copy())
        self.update_count += 1
        
        self.logger.debug(f"Epoch {epoch}: æƒé‡æ›´æ–° - {self.weights}")
    
    def _adaptive_weight_adjustment(self):
        """è‡ªé€‚åº”æƒé‡è°ƒæ•´ç®—æ³•"""
        if len(self.loss_history['data']) < 10:
            return
        
        # æ–¹æ³•1: åŸºäºæŸå¤±å‡å€¼çš„å¹³è¡¡
        mean_losses = {}
        for key, history in self.loss_history.items():
            if history:
                mean_losses[key] = np.mean(list(history))
        
        if not mean_losses:
            return
        
        # æ‰¾åˆ°æœ€å¤§æŸå¤±
        max_loss_key = max(mean_losses, key=mean_losses.get)
        max_loss_value = mean_losses[max_loss_key]
        
        # è°ƒæ•´æƒé‡ä»¥å¹³è¡¡æŸå¤±
        for key in self.weights:
            if key in mean_losses and mean_losses[key] > 0:
                # æƒé‡ä¸æŸå¤±æˆåæ¯”
                target_weight = max_loss_value / mean_losses[key]
                
                # å¹³æ»‘æ›´æ–°
                alpha = 0.1  # æ›´æ–°ç‡
                self.weights[key] = (1 - alpha) * self.weights[key] + alpha * target_weight
                
                # åº”ç”¨è¾¹ç•Œçº¦æŸ
                self.weights[key] = max(0.1, min(self.weights[key], self.config.max_weight_ratio))
        
        # æ–¹æ³•2: åŸºäºæ¢¯åº¦èŒƒæ•°çš„è°ƒæ•´ï¼ˆå¦‚æœéœ€è¦æ›´ç²¾ç»†çš„æ§åˆ¶ï¼‰
        self._gradient_based_adjustment()
    
    def _gradient_based_adjustment(self):
        """åŸºäºæ¢¯åº¦èŒƒæ•°çš„æƒé‡è°ƒæ•´"""
        # è¿™ä¸ªæ–¹æ³•éœ€è¦åœ¨è®­ç»ƒå¾ªç¯ä¸­è°ƒç”¨ï¼Œè¿™é‡Œæä¾›æ¡†æ¶
        # å®é™…å®ç°éœ€è¦è®¿é—®æ¢¯åº¦ä¿¡æ¯
        pass
    
    def get_current_weights(self) -> Dict[str, float]:
        """è·å–å½“å‰æƒé‡"""
        return self.weights.copy()
    
    def reset_weights(self):
        """é‡ç½®æƒé‡ä¸ºåˆå§‹å€¼"""
        self.weights = {
            'data': self.config.data_weight,
            'pde': self.config.pde_weight,
            'boundary': self.config.boundary_weight,
            'initial': self.config.initial_weight
        }
        
        for history in self.loss_history.values():
            history.clear()
        
        self.weight_history.clear()
        self.update_count = 0

class PhysicsConstraints(ABC):
    """ç‰©ç†çº¦æŸæŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def compute_pde_residual(self, network: nn.Module, x: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—PDEæ®‹å·®"""
        pass
    
    @abstractmethod
    def compute_boundary_loss(self, network: nn.Module, 
                            boundary_points: torch.Tensor,
                            boundary_values: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è¾¹ç•Œæ¡ä»¶æŸå¤±"""
        pass

class HeatEquationConstraints(PhysicsConstraints):
    """çƒ­ä¼ å¯¼æ–¹ç¨‹çº¦æŸ"""
    
    def __init__(self, thermal_diffusivity: float = 0.01):
        """
        åˆå§‹åŒ–çƒ­ä¼ å¯¼æ–¹ç¨‹çº¦æŸ
        
        Args:
            thermal_diffusivity: çƒ­æ‰©æ•£ç³»æ•° Î±
        """
        self.alpha = thermal_diffusivity
    
    def compute_pde_residual(self, network: nn.Module, x: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—çƒ­ä¼ å¯¼æ–¹ç¨‹æ®‹å·®: âˆ‚u/âˆ‚t - Î±âˆ‡Â²u = 0
        
        Args:
            network: PINNç½‘ç»œ
            x: æ—¶ç©ºåæ ‡ [batch, 3] (x, y, t)
            
        Returns:
            PDEæ®‹å·®
        """
        x.requires_grad_(True)
        
        # ç½‘ç»œè¾“å‡º
        u = network(x)
        
        # è®¡ç®—æ¢¯åº¦
        gradients = network.compute_gradients(x, order=2)
        
        # æå–æ‰€éœ€çš„åå¯¼æ•°
        u_t = gradients.get('u_z', torch.zeros_like(u))  # å‡è®¾tæ˜¯ç¬¬3ç»´
        u_xx = gradients.get('u_xx', torch.zeros_like(u))
        u_yy = gradients.get('u_yy', torch.zeros_like(u))
        
        # çƒ­ä¼ å¯¼æ–¹ç¨‹æ®‹å·®
        residual = u_t - self.alpha * (u_xx + u_yy)
        
        return residual
    
    def compute_boundary_loss(self, network: nn.Module,
                            boundary_points: torch.Tensor,
                            boundary_values: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è¾¹ç•Œæ¡ä»¶æŸå¤±"""
        predicted = network(boundary_points)
        return F.mse_loss(predicted, boundary_values)

class WaveEquationConstraints(PhysicsConstraints):
    """æ³¢åŠ¨æ–¹ç¨‹çº¦æŸ"""
    
    def __init__(self, wave_speed: float = 1.0):
        """
        åˆå§‹åŒ–æ³¢åŠ¨æ–¹ç¨‹çº¦æŸ
        
        Args:
            wave_speed: æ³¢é€Ÿ c
        """
        self.c = wave_speed
    
    def compute_pde_residual(self, network: nn.Module, x: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ³¢åŠ¨æ–¹ç¨‹æ®‹å·®: âˆ‚Â²u/âˆ‚tÂ² - cÂ²âˆ‡Â²u = 0
        
        Args:
            network: PINNç½‘ç»œ
            x: æ—¶ç©ºåæ ‡ [batch, 3] (x, y, t)
            
        Returns:
            PDEæ®‹å·®
        """
        x.requires_grad_(True)
        
        # è®¡ç®—äºŒé˜¶åå¯¼æ•°
        gradients = network.compute_gradients(x, order=2)
        
        # æå–æ‰€éœ€çš„åå¯¼æ•°
        u_tt = gradients.get('u_zz', torch.zeros_like(network(x)))  # å‡è®¾tæ˜¯ç¬¬3ç»´
        u_xx = gradients.get('u_xx', torch.zeros_like(network(x)))
        u_yy = gradients.get('u_yy', torch.zeros_like(network(x)))
        
        # æ³¢åŠ¨æ–¹ç¨‹æ®‹å·®
        residual = u_tt - self.c**2 * (u_xx + u_yy)
        
        return residual
    
    def compute_boundary_loss(self, network: nn.Module,
                            boundary_points: torch.Tensor,
                            boundary_values: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è¾¹ç•Œæ¡ä»¶æŸå¤±"""
        predicted = network(boundary_points)
        return F.mse_loss(predicted, boundary_values)

class NavierStokesConstraints(PhysicsConstraints):
    """Navier-Stokesæ–¹ç¨‹çº¦æŸï¼ˆç®€åŒ–2Dç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, reynolds_number: float = 100.0):
        """
        åˆå§‹åŒ–Navier-Stokesçº¦æŸ
        
        Args:
            reynolds_number: é›·è¯ºæ•°
        """
        self.Re = reynolds_number
    
    def compute_pde_residual(self, network: nn.Module, x: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—N-Sæ–¹ç¨‹æ®‹å·®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        """
        # è¿™é‡Œå®ç°ç®€åŒ–çš„N-Sæ–¹ç¨‹
        # å®Œæ•´å®ç°éœ€è¦å¤„ç†é€Ÿåº¦å’Œå‹åŠ›åœº
        x.requires_grad_(True)
        
        # å‡è®¾ç½‘ç»œè¾“å‡ºä¸ºé€Ÿåº¦åœºçš„ä¸€ä¸ªåˆ†é‡
        u = network(x)
        gradients = network.compute_gradients(x, order=2)
        
        # ç®€åŒ–çš„ç²˜æ€§é¡¹
        u_xx = gradients.get('u_xx', torch.zeros_like(u))
        u_yy = gradients.get('u_yy', torch.zeros_like(u))
        
        # ç®€åŒ–æ®‹å·®ï¼ˆåªè€ƒè™‘æ‰©æ•£é¡¹ï¼‰
        residual = u_xx + u_yy  # æ‹‰æ™®æ‹‰æ–¯ç®—å­
        
        return residual
    
    def compute_boundary_loss(self, network: nn.Module,
                            boundary_points: torch.Tensor,
                            boundary_values: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è¾¹ç•Œæ¡ä»¶æŸå¤±"""
        predicted = network(boundary_points)
        return F.mse_loss(predicted, boundary_values)

class BayesianPINN(nn.Module):
    """è´å¶æ–¯PINNï¼ˆä¸ç¡®å®šæ€§é‡åŒ–ï¼‰"""
    
    def __init__(self, base_network: MultiScalePINN, config: PINNConfig):
        """
        åˆå§‹åŒ–è´å¶æ–¯PINN
        
        Args:
            base_network: åŸºç¡€PINNç½‘ç»œ
            config: PINNé…ç½®
        """
        super().__init__()
        self.base_network = base_network
        self.config = config
        
        # å˜åˆ†å‚æ•°
        self.log_variance = nn.Parameter(torch.zeros(1))
        
        # æƒé‡çš„å…ˆéªŒåˆ†å¸ƒå‚æ•°
        self.prior_mean = 0.0
        self.prior_std = config.prior_std
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è´å¶æ–¯å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥
            
        Returns:
            (é¢„æµ‹å‡å€¼, é¢„æµ‹æ–¹å·®)
        """
        mean = self.base_network(x)
        variance = torch.exp(self.log_variance).expand_as(mean)
        
        return mean, variance
    
    def sample_predictions(self, x: torch.Tensor, n_samples: int = 100) -> torch.Tensor:
        """
        è’™ç‰¹å¡æ´›é‡‡æ ·é¢„æµ‹
        
        Args:
            x: è¾“å…¥
            n_samples: é‡‡æ ·æ¬¡æ•°
            
        Returns:
            é¢„æµ‹æ ·æœ¬ [n_samples, batch_size, output_dim]
        """
        self.train()  # å¯ç”¨dropoutç­‰éšæœºå±‚
        
        samples = []
        for _ in range(n_samples):
            mean, variance = self.forward(x)
            # ä»é¢„æµ‹åˆ†å¸ƒä¸­é‡‡æ ·
            sample = torch.normal(mean, torch.sqrt(variance))
            samples.append(sample)
        
        return torch.stack(samples)
    
    def compute_kl_divergence(self) -> torch.Tensor:
        """è®¡ç®—KLæ•£åº¦ï¼ˆæ­£åˆ™åŒ–é¡¹ï¼‰"""
        kl_loss = 0.0
        
        for param in self.base_network.parameters():
            # å‡è®¾æƒé‡æœä»æ­£æ€åˆ†å¸ƒ
            kl_loss += torch.sum(
                (param - self.prior_mean)**2 / (2 * self.prior_std**2)
            ) + 0.5 * torch.log(torch.tensor(2 * np.pi * self.prior_std**2))
        
        return kl_loss

class AdvancedPINNEngine:
    """é«˜çº§PINNè®­ç»ƒå¼•æ“"""
    
    def __init__(self, config: PINNConfig, physics_constraints: PhysicsConstraints):
        """
        åˆå§‹åŒ–é«˜çº§PINNå¼•æ“
        
        Args:
            config: PINNé…ç½®
            physics_constraints: ç‰©ç†çº¦æŸ
        """
        self.config = config
        self.physics_constraints = physics_constraints
        self.logger = logging.getLogger(__name__)
        
        # ç½‘ç»œå’Œç»„ä»¶
        self.network: Optional[MultiScalePINN] = None
        self.bayesian_network: Optional[BayesianPINN] = None
        self.weight_manager = AdaptiveWeightManager(config)
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer: Optional[torch.optim.Optimizer] = None
        self.scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None
        
        # è®­ç»ƒçŠ¶æ€
        self.is_trained = False
        self.training_history: List[TrainingMetrics] = []
        self.best_loss = float('inf')
        self.patience_counter = 0
        
        # æ€§èƒ½ç»Ÿè®¡
        self.training_time = 0.0
        self.convergence_epoch = None
    
    def initialize_network(self, input_dim: int = 3, output_dim: int = 1):
        """
        åˆå§‹åŒ–ç½‘ç»œ
        
        Args:
            input_dim: è¾“å…¥ç»´åº¦
            output_dim: è¾“å‡ºç»´åº¦
        """
        self.network = MultiScalePINN(self.config, input_dim, output_dim).to(device)
        
        if self.config.enable_bayesian:
            self.bayesian_network = BayesianPINN(self.network, self.config).to(device)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        if self.config.enable_bayesian and self.bayesian_network is not None:
            params = self.bayesian_network.parameters()
        else:
            params = self.network.parameters()
        
        self.optimizer = optim.Adam(params, lr=self.config.learning_rate)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=2000, verbose=True
        )
        
        self.logger.info(f"PINNç½‘ç»œåˆå§‹åŒ–å®Œæˆï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in params)}")
    
    async def train_pinn(self, training_data: TrainingData) -> Dict:
        """
        è®­ç»ƒPINNæ¨¡å‹
        
        Args:
            training_data: è®­ç»ƒæ•°æ®
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        if self.network is None:
            raise ValueError("ç½‘ç»œæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize_network")
        
        self.logger.info("å¼€å§‹PINNè®­ç»ƒ...")
        start_time = time.time()
        
        # æ•°æ®é¢„å¤„ç†
        self._preprocess_training_data(training_data)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config.epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            metrics = await self._train_epoch(training_data, epoch)
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history.append(metrics)
            
            # æ›´æ–°æƒé‡
            self.weight_manager.update_weights(metrics.loss_components, epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(metrics.loss_components.total_loss)
            
            # æ—©åœæ£€æŸ¥
            if self._check_early_stopping(metrics.loss_components.total_loss, epoch):
                self.convergence_epoch = epoch
                break
            
            # æ—¥å¿—è¾“å‡º
            if epoch % 1000 == 0:
                await self._log_training_progress(metrics, epoch)
        
        # è®­ç»ƒå®Œæˆ
        self.training_time = time.time() - start_time
        self.is_trained = True
        
        training_result = {
            'training_time': self.training_time,
            'total_epochs': len(self.training_history),
            'convergence_epoch': self.convergence_epoch,
            'final_loss': self.training_history[-1].loss_components.total_loss,
            'best_loss': self.best_loss,
            'network_parameters': sum(p.numel() for p in self.network.parameters())
        }
        
        self.logger.info(f"PINNè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {self.training_time:.2f}ç§’ï¼Œæœ€ç»ˆæŸå¤±: {self.best_loss:.2e}")
        
        return training_result
    
    async def _train_epoch(self, training_data: TrainingData, epoch: int) -> TrainingMetrics:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.network.train()
        
        # è·å–å½“å‰æƒé‡
        weights = self.weight_manager.get_current_weights()
        
        # å‰å‘ä¼ æ’­
        loss_components = self._compute_loss_components(training_data, weights)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss_components.total_loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        if self.config.gradient_clipping > 0:
            torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.config.gradient_clipping)
        
        # ä¼˜åŒ–æ­¥éª¤
        self.optimizer.step()
        
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        gradient_norm = self._compute_gradient_norm()
        
        # æ„å»ºè®­ç»ƒæŒ‡æ ‡
        metrics = TrainingMetrics(
            epoch=epoch,
            loss_components=LossComponents(
                total_loss=loss_components.total_loss.item(),
                data_loss=loss_components.data_loss.item() if hasattr(loss_components, 'data_loss') else 0.0,
                pde_loss=loss_components.pde_loss.item(),
                boundary_loss=loss_components.boundary_loss.item(),
                initial_loss=loss_components.initial_loss.item() if hasattr(loss_components, 'initial_loss') else 0.0,
                regularization_loss=loss_components.regularization_loss.item() if hasattr(loss_components, 'regularization_loss') else 0.0
            ),
            learning_rate=self.optimizer.param_groups[0]['lr'],
            gradient_norm=gradient_norm,
            weights=weights.copy()
        )
        
        return metrics
    
    def _compute_loss_components(self, training_data: TrainingData, weights: Dict[str, float]) -> Any:
        """è®¡ç®—æŸå¤±ç»„ä»¶"""
        total_loss = 0.0
        
        # PDEæŸå¤±
        pde_residual = self.physics_constraints.compute_pde_residual(
            self.network, training_data.collocation_points
        )
        pde_loss = torch.mean(pde_residual**2)
        total_loss += weights['pde'] * pde_loss
        
        # è¾¹ç•Œæ¡ä»¶æŸå¤±
        boundary_loss = self.physics_constraints.compute_boundary_loss(
            self.network, training_data.boundary_points, training_data.boundary_values
        )
        total_loss += weights['boundary'] * boundary_loss
        
        # åˆå§‹æ¡ä»¶æŸå¤±
        initial_loss = torch.tensor(0.0, device=device)
        if training_data.initial_points is not None and training_data.initial_values is not None:
            predicted_initial = self.network(training_data.initial_points)
            initial_loss = F.mse_loss(predicted_initial, training_data.initial_values)
            total_loss += weights['initial'] * initial_loss
        
        # æ•°æ®æ‹ŸåˆæŸå¤±
        data_loss = torch.tensor(0.0, device=device)
        if training_data.observation_points is not None and training_data.observation_values is not None:
            predicted_data = self.network(training_data.observation_points)
            data_loss = F.mse_loss(predicted_data, training_data.observation_values)
            total_loss += weights['data'] * data_loss
        
        # æ­£åˆ™åŒ–æŸå¤±
        regularization_loss = torch.tensor(0.0, device=device)
        if self.config.regularization_factor > 0:
            for param in self.network.parameters():
                regularization_loss += torch.sum(param**2)
            regularization_loss *= self.config.regularization_factor
            total_loss += regularization_loss
        
        # è´å¶æ–¯KLæ•£åº¦
        if self.config.enable_bayesian and self.bayesian_network is not None:
            kl_loss = self.bayesian_network.compute_kl_divergence()
            total_loss += 0.01 * kl_loss  # KLæƒé‡
        
        # åˆ›å»ºæŸå¤±å¯¹è±¡
        class LossObject:
            def __init__(self):
                self.total_loss = total_loss
                self.pde_loss = pde_loss
                self.boundary_loss = boundary_loss
                self.initial_loss = initial_loss
                self.data_loss = data_loss
                self.regularization_loss = regularization_loss
        
        return LossObject()
    
    def _preprocess_training_data(self, training_data: TrainingData):
        """é¢„å¤„ç†è®­ç»ƒæ•°æ®"""
        # ç§»åŠ¨åˆ°è®¾å¤‡
        training_data.collocation_points = training_data.collocation_points.to(device)
        training_data.boundary_points = training_data.boundary_points.to(device)
        training_data.boundary_values = training_data.boundary_values.to(device)
        
        if training_data.initial_points is not None:
            training_data.initial_points = training_data.initial_points.to(device)
            training_data.initial_values = training_data.initial_values.to(device)
        
        if training_data.observation_points is not None:
            training_data.observation_points = training_data.observation_points.to(device)
            training_data.observation_values = training_data.observation_values.to(device)
        
        # æ•°æ®å½’ä¸€åŒ–
        self._normalize_training_data(training_data)
    
    def _normalize_training_data(self, training_data: TrainingData):
        """æ•°æ®å½’ä¸€åŒ–"""
        # è®¡ç®—è¾“å…¥å½’ä¸€åŒ–å‚æ•°
        all_points = [training_data.collocation_points, training_data.boundary_points]
        if training_data.initial_points is not None:
            all_points.append(training_data.initial_points)
        if training_data.observation_points is not None:
            all_points.append(training_data.observation_points)
        
        combined_points = torch.cat(all_points, dim=0)
        
        # æ›´æ–°ç½‘ç»œçš„å½’ä¸€åŒ–å‚æ•°
        input_std = torch.std(combined_points, dim=0)
        input_std = torch.where(input_std > 1e-8, input_std, torch.ones_like(input_std))
        
        self.network.input_normalization.data = input_std
        
        # è¾“å‡ºå½’ä¸€åŒ–
        if training_data.observation_values is not None:
            output_std = torch.std(training_data.observation_values)
            if output_std > 1e-8:
                self.network.output_scaling.data = torch.tensor([output_std.item()])
    
    def _compute_gradient_norm(self) -> float:
        """è®¡ç®—æ¢¯åº¦èŒƒæ•°"""
        total_norm = 0.0
        for param in self.network.parameters():
            if param.grad is not None:
                total_norm += param.grad.data.norm(2).item() ** 2
        
        return np.sqrt(total_norm)
    
    def _check_early_stopping(self, current_loss: float, epoch: int) -> bool:
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.patience_counter = 0
        else:
            self.patience_counter += 1
        
        # æ—©åœæ¡ä»¶
        patience = 5000
        return self.patience_counter >= patience
    
    async def _log_training_progress(self, metrics: TrainingMetrics, epoch: int):
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        self.logger.info(
            f"Epoch {epoch}: "
            f"Total Loss = {metrics.loss_components.total_loss:.2e}, "
            f"PDE Loss = {metrics.loss_components.pde_loss:.2e}, "
            f"BC Loss = {metrics.loss_components.boundary_loss:.2e}, "
            f"LR = {metrics.learning_rate:.2e}"
        )
    
    def predict(self, test_points: torch.Tensor, 
                return_uncertainty: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        é¢„æµ‹
        
        Args:
            test_points: æµ‹è¯•ç‚¹
            return_uncertainty: æ˜¯å¦è¿”å›ä¸ç¡®å®šæ€§
            
        Returns:
            é¢„æµ‹ç»“æœæˆ–(é¢„æµ‹å‡å€¼, é¢„æµ‹æ ‡å‡†å·®)
        """
        if not self.is_trained or self.network is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        self.network.eval()
        test_points = test_points.to(device)
        
        with torch.no_grad():
            if self.config.enable_bayesian and self.bayesian_network is not None and return_uncertainty:
                # è´å¶æ–¯é¢„æµ‹
                samples = self.bayesian_network.sample_predictions(test_points, self.config.num_samples)
                
                mean = torch.mean(samples, dim=0)
                std = torch.std(samples, dim=0)
                
                return mean, std
            else:
                # ç¡®å®šæ€§é¢„æµ‹
                prediction = self.network(test_points)
                
                if return_uncertainty:
                    # ç®€å•çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆåŸºäºè®­ç»ƒæŸå¤±ï¼‰
                    uncertainty = torch.full_like(prediction, np.sqrt(self.best_loss))
                    return prediction, uncertainty
                else:
                    return prediction
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dict = {
            'config': self.config.__dict__,
            'network_state': self.network.state_dict() if self.network else None,
            'optimizer_state': self.optimizer.state_dict() if self.optimizer else None,
            'training_history': [
                {
                    'epoch': m.epoch,
                    'loss_components': m.loss_components.__dict__,
                    'learning_rate': m.learning_rate,
                    'gradient_norm': m.gradient_norm,
                    'weights': m.weights
                }
                for m in self.training_history
            ],
            'best_loss': self.best_loss,
            'is_trained': self.is_trained,
            'training_time': self.training_time
        }
        
        torch.save(save_dict, filepath)
        self.logger.info(f"PINNæ¨¡å‹å·²ä¿å­˜è‡³: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        save_dict = torch.load(filepath, map_location=device)
        
        # æ¢å¤é…ç½®
        self.config = PINNConfig(**save_dict['config'])
        
        # é‡æ–°åˆå§‹åŒ–ç½‘ç»œ
        if save_dict['network_state']:
            # ä»ç½‘ç»œçŠ¶æ€æ¨æ–­è¾“å…¥è¾“å‡ºç»´åº¦
            first_layer_weight = save_dict['network_state']['input_layer.weight']
            input_dim = first_layer_weight.shape[1]
            
            output_layer_keys = [k for k in save_dict['network_state'].keys() if 'output_layer' in k]
            if output_layer_keys:
                output_layer_weight = save_dict['network_state']['output_layer.weight']
                output_dim = output_layer_weight.shape[0]
            else:
                output_dim = 1
            
            self.initialize_network(input_dim, output_dim)
            self.network.load_state_dict(save_dict['network_state'])
        
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
        if save_dict['optimizer_state'] and self.optimizer:
            self.optimizer.load_state_dict(save_dict['optimizer_state'])
        
        # æ¢å¤è®­ç»ƒçŠ¶æ€
        self.best_loss = save_dict['best_loss']
        self.is_trained = save_dict['is_trained']
        self.training_time = save_dict.get('training_time', 0.0)
        
        self.logger.info(f"PINNæ¨¡å‹å·²ä» {filepath} åŠ è½½")
    
    def get_training_statistics(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡"""
        if not self.training_history:
            return {}
        
        losses = [m.loss_components.total_loss for m in self.training_history]
        pde_losses = [m.loss_components.pde_loss for m in self.training_history]
        bc_losses = [m.loss_components.boundary_loss for m in self.training_history]
        
        return {
            'total_epochs': len(self.training_history),
            'final_loss': losses[-1],
            'best_loss': self.best_loss,
            'loss_reduction': (losses[0] - losses[-1]) / losses[0] if losses[0] > 0 else 0.0,
            'average_pde_loss': np.mean(pde_losses),
            'average_bc_loss': np.mean(bc_losses),
            'convergence_epoch': self.convergence_epoch,
            'training_time': self.training_time,
            'parameters_count': sum(p.numel() for p in self.network.parameters()) if self.network else 0
        }

if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    async def test_advanced_pinn_engine():
        """é«˜çº§PINNå¼•æ“æµ‹è¯•"""
        print("ğŸ¤– DeepCADé«˜çº§PINNå¼•æ“æµ‹è¯• ğŸ¤–")
        
        # é…ç½®
        config = PINNConfig(
            hidden_layers=[64, 64, 64, 64],
            activation="tanh",
            learning_rate=1e-3,
            epochs=5000,
            adaptive_weights=True,
            enable_bayesian=False
        )
        
        # ç‰©ç†çº¦æŸï¼ˆçƒ­ä¼ å¯¼æ–¹ç¨‹ï¼‰
        physics = HeatEquationConstraints(thermal_diffusivity=0.01)
        
        # åˆ›å»ºPINNå¼•æ“
        pinn_engine = AdvancedPINNEngine(config, physics)
        pinn_engine.initialize_network(input_dim=3, output_dim=1)  # (x, y, t) -> u
        
        print(f"ğŸ“ ç½‘ç»œå‚æ•°æ•°é‡: {sum(p.numel() for p in pinn_engine.network.parameters())}")
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        print("ğŸ“Š ç”Ÿæˆè®­ç»ƒæ•°æ®...")
        
        # é…ç‚¹ï¼ˆåŸŸå†…ï¼‰
        n_collocation = 10000
        x_col = torch.rand(n_collocation, 1) * 2 - 1  # [-1, 1]
        y_col = torch.rand(n_collocation, 1) * 2 - 1  # [-1, 1]
        t_col = torch.rand(n_collocation, 1) * 2       # [0, 2]
        collocation_points = torch.cat([x_col, y_col, t_col], dim=1)
        
        # è¾¹ç•Œç‚¹
        n_boundary = 2000
        
        # è¾¹ç•Œ: x = -1, 1
        x_bd1 = torch.full((n_boundary//4, 1), -1.0)
        y_bd1 = torch.rand(n_boundary//4, 1) * 2 - 1
        t_bd1 = torch.rand(n_boundary//4, 1) * 2
        
        x_bd2 = torch.full((n_boundary//4, 1), 1.0)
        y_bd2 = torch.rand(n_boundary//4, 1) * 2 - 1
        t_bd2 = torch.rand(n_boundary//4, 1) * 2
        
        # è¾¹ç•Œ: y = -1, 1
        x_bd3 = torch.rand(n_boundary//4, 1) * 2 - 1
        y_bd3 = torch.full((n_boundary//4, 1), -1.0)
        t_bd3 = torch.rand(n_boundary//4, 1) * 2
        
        x_bd4 = torch.rand(n_boundary//4, 1) * 2 - 1
        y_bd4 = torch.full((n_boundary//4, 1), 1.0)
        t_bd4 = torch.rand(n_boundary//4, 1) * 2
        
        boundary_points = torch.cat([
            torch.cat([x_bd1, y_bd1, t_bd1], dim=1),
            torch.cat([x_bd2, y_bd2, t_bd2], dim=1),
            torch.cat([x_bd3, y_bd3, t_bd3], dim=1),
            torch.cat([x_bd4, y_bd4, t_bd4], dim=1)
        ], dim=0)
        
        # è¾¹ç•Œå€¼ï¼ˆé½æ¬¡Dirichletè¾¹ç•Œæ¡ä»¶ï¼‰
        boundary_values = torch.zeros(boundary_points.shape[0], 1)
        
        # åˆå§‹æ¡ä»¶
        n_initial = 1000
        x_init = torch.rand(n_initial, 1) * 2 - 1
        y_init = torch.rand(n_initial, 1) * 2 - 1
        t_init = torch.zeros(n_initial, 1)
        initial_points = torch.cat([x_init, y_init, t_init], dim=1)
        
        # åˆå§‹æ¸©åº¦åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰
        initial_values = torch.exp(-(x_init**2 + y_init**2) / 0.2)
        
        # æ„å»ºè®­ç»ƒæ•°æ®
        training_data = TrainingData(
            collocation_points=collocation_points,
            boundary_points=boundary_points,
            boundary_values=boundary_values,
            initial_points=initial_points,
            initial_values=initial_values
        )
        
        print(f"  é…ç‚¹æ•°é‡: {n_collocation}")
        print(f"  è¾¹ç•Œç‚¹æ•°é‡: {n_boundary}")
        print(f"  åˆå§‹ç‚¹æ•°é‡: {n_initial}")
        
        # è®­ç»ƒPINN
        print("\nğŸ‹ï¸ å¼€å§‹PINNè®­ç»ƒ...")
        training_result = await pinn_engine.train_pinn(training_data)
        
        print(f"\nğŸ“Š è®­ç»ƒç»“æœ:")
        print(f"  è®­ç»ƒæ—¶é—´: {training_result['training_time']:.2f}ç§’")
        print(f"  æ€»è½®æ¬¡: {training_result['total_epochs']}")
        print(f"  æ”¶æ•›è½®æ¬¡: {training_result['convergence_epoch']}")
        print(f"  æœ€ç»ˆæŸå¤±: {training_result['final_loss']:.2e}")
        print(f"  æœ€ä½³æŸå¤±: {training_result['best_loss']:.2e}")
        
        # æµ‹è¯•é¢„æµ‹
        print("\nğŸ”® æµ‹è¯•PINNé¢„æµ‹...")
        
        # ç”Ÿæˆæµ‹è¯•ç‚¹
        n_test = 100
        x_test = torch.linspace(-1, 1, 10).repeat(10, 1).flatten().unsqueeze(1)
        y_test = torch.linspace(-1, 1, 10).repeat_interleave(10).unsqueeze(1)
        t_test = torch.ones(n_test, 1) * 1.0  # t = 1.0æ—¶åˆ»
        test_points = torch.cat([x_test, y_test, t_test], dim=1)
        
        # é¢„æµ‹
        prediction = pinn_engine.predict(test_points)
        
        print(f"  æµ‹è¯•ç‚¹æ•°é‡: {n_test}")
        print(f"  é¢„æµ‹å½¢çŠ¶: {prediction.shape}")
        print(f"  é¢„æµ‹èŒƒå›´: [{prediction.min():.3f}, {prediction.max():.3f}]")
        
        # ä¸ç¡®å®šæ€§é‡åŒ–æµ‹è¯•
        if config.enable_bayesian:
            print("\nğŸ² ä¸ç¡®å®šæ€§é‡åŒ–æµ‹è¯•...")
            mean_pred, std_pred = pinn_engine.predict(test_points, return_uncertainty=True)
            
            print(f"  é¢„æµ‹å‡å€¼èŒƒå›´: [{mean_pred.min():.3f}, {mean_pred.max():.3f}]")
            print(f"  ä¸ç¡®å®šæ€§èŒƒå›´: [{std_pred.min():.3f}, {std_pred.max():.3f}]")
        
        # è®­ç»ƒç»Ÿè®¡
        print("\nğŸ“ˆ è®­ç»ƒç»Ÿè®¡:")
        stats = pinn_engine.get_training_statistics()
        
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2e}")
            else:
                print(f"  {key}: {value}")
        
        # ä¿å­˜å’ŒåŠ è½½æµ‹è¯•
        print("\nğŸ’¾ æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½...")
        save_path = "advanced_pinn_model.pth"
        pinn_engine.save_model(save_path)
        
        # åˆ›å»ºæ–°å¼•æ“å¹¶åŠ è½½
        new_engine = AdvancedPINNEngine(config, physics)
        new_engine.load_model(save_path)
        
        # æµ‹è¯•åŠ è½½åçš„é¢„æµ‹
        loaded_prediction = new_engine.predict(test_points[:10])
        original_prediction = prediction[:10]
        
        prediction_diff = torch.abs(loaded_prediction - original_prediction).max()
        print(f"  åŠ è½½åé¢„æµ‹è¯¯å·®: {prediction_diff:.2e}")
        
        print("\nâœ… é«˜çº§PINNå¼•æ“æµ‹è¯•å®Œæˆï¼")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_advanced_pinn_engine())