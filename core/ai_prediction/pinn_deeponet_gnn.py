#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepCADæ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ - PINN+DeepONet+GNNæ¶æ„
3å·è®¡ç®—ä¸“å®¶ - Week1æ¶æ„è®¾è®¡

é›†æˆä¸‰å¤§AIæ¶æ„ï¼š
- PINN (Physics-Informed Neural Networks): ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ
- DeepONet (Deep Operator Networks): æ·±åº¦ç®—å­ç½‘ç»œ  
- GNN (Graph Neural Networks): å›¾ç¥ç»ç½‘ç»œ

æŠ€æœ¯ç›®æ ‡ï¼š
- ç‰©ç†çº¦æŸä¿æŒï¼š>99%
- é¢„æµ‹ç²¾åº¦ï¼š>95%
- è®¡ç®—åŠ é€Ÿï¼š1000å€+
- ä¸ç¡®å®šæ€§é‡åŒ–ï¼šè´å¶æ–¯æ¨ç†
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Union, Tuple, Callable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import time
import logging
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor

# PyTorchå‡ ä½•åº“æ£€æŸ¥
try:
    import torch_geometric
    from torch_geometric.nn import GCNConv, GATConv, MessagePassing
    from torch_geometric.data import Data, Batch
    TORCH_GEOMETRIC_AVAILABLE = True
    print("PyTorch Geometricå¯ç”¨")
except ImportError:
    print("Warning: PyTorch Geometricä¸å¯ç”¨ï¼ŒGNNåŠŸèƒ½å—é™")
    TORCH_GEOMETRIC_AVAILABLE = False

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"AIé¢„æµ‹ç³»ç»Ÿä½¿ç”¨è®¾å¤‡: {device}")

@dataclass
class PINNConfig:
    """PINNé…ç½®å‚æ•°"""
    # ç½‘ç»œæ¶æ„
    hidden_layers: List[int] = field(default_factory=lambda: [50, 50, 50])
    activation: str = "tanh"
    
    # è®­ç»ƒå‚æ•°
    learning_rate: float = 1e-3
    epochs: int = 10000
    batch_size: int = 1000
    
    # æŸå¤±æƒé‡
    pde_weight: float = 1.0        # PDEæŸå¤±æƒé‡
    boundary_weight: float = 10.0  # è¾¹ç•Œæ¡ä»¶æƒé‡
    initial_weight: float = 10.0   # åˆå§‹æ¡ä»¶æƒé‡
    data_weight: float = 1.0       # æ•°æ®æ‹Ÿåˆæƒé‡
    
    # ç‰©ç†å‚æ•°
    diffusion_coeff: float = 0.01
    source_term: bool = True

@dataclass
class DeepONetConfig:
    """DeepONeté…ç½®å‚æ•°"""
    # Branch Network (è¾“å…¥å‡½æ•°ç¼–ç )
    branch_layers: List[int] = field(default_factory=lambda: [100, 128, 128])
    
    # Trunk Network (åæ ‡ç¼–ç )
    trunk_layers: List[int] = field(default_factory=lambda: [2, 64, 64])
    
    # è¾“å‡ºç»´åº¦
    latent_dim: int = 128         # æ½œåœ¨ç©ºé—´ç»´åº¦
    output_dim: int = 1           # è¾“å‡ºåœºç»´åº¦
    
    # è®­ç»ƒå‚æ•°
    learning_rate: float = 1e-3
    epochs: int = 5000
    batch_size: int = 32

@dataclass
class GNNConfig:
    """GNNé…ç½®å‚æ•°"""
    # å›¾ç»“æ„
    node_features: int = 16       # èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
    edge_features: int = 8        # è¾¹ç‰¹å¾ç»´åº¦
    hidden_dim: int = 64          # éšè—å±‚ç»´åº¦
    
    # ç½‘ç»œæ¶æ„
    num_layers: int = 4           # GNNå±‚æ•°
    num_heads: int = 4            # æ³¨æ„åŠ›å¤´æ•°ï¼ˆGATï¼‰
    dropout: float = 0.1
    
    # è®­ç»ƒå‚æ•°
    learning_rate: float = 1e-3
    epochs: int = 3000
    batch_size: int = 16

class PhysicsInformedNN(nn.Module):
    """ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINN)"""
    
    def __init__(self, config: PINNConfig):
        super().__init__()
        self.config = config
        
        # æ„å»ºç¥ç»ç½‘ç»œå±‚
        layers = []
        input_dim = 3  # (x, y, t) æ—¶ç©ºåæ ‡
        
        # éšè—å±‚
        prev_dim = input_dim
        for hidden_dim in config.hidden_layers:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            if config.activation == "tanh":
                layers.append(nn.Tanh())
            elif config.activation == "relu":
                layers.append(nn.ReLU())
            elif config.activation == "gelu":
                layers.append(nn.GELU())
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, 1))  # è¾“å‡ºæ ‡é‡åœº
        
        self.network = nn.Sequential(*layers)
        
        # ç‰©ç†å‚æ•°
        self.diffusion_coeff = config.diffusion_coeff
        self.source_term_enabled = config.source_term
    
    def forward(self, coords: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            coords: æ—¶ç©ºåæ ‡ [batch, 3] (x, y, t)
            
        Returns:
            é¢„æµ‹åœºå€¼ [batch, 1]
        """
        return self.network(coords)
    
    def compute_pde_residual(self, coords: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—PDEæ®‹å·®ï¼ˆä»¥çƒ­ä¼ å¯¼æ–¹ç¨‹ä¸ºä¾‹ï¼‰
        âˆ‚u/âˆ‚t = Î±âˆ‡Â²u + f(x,y,t)
        
        Args:
            coords: æ—¶ç©ºåæ ‡ï¼Œéœ€è¦æ¢¯åº¦
            
        Returns:
            PDEæ®‹å·®
        """
        coords.requires_grad_(True)
        u = self.forward(coords)
        
        # è®¡ç®—ä¸€é˜¶å¯¼æ•°
        grad_u = torch.autograd.grad(
            outputs=u, inputs=coords, 
            grad_outputs=torch.ones_like(u),
            create_graph=True, retain_graph=True
        )[0]
        
        u_x = grad_u[:, 0:1]
        u_y = grad_u[:, 1:2]
        u_t = grad_u[:, 2:3]
        
        # è®¡ç®—äºŒé˜¶å¯¼æ•°ï¼ˆæ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼‰
        u_xx = torch.autograd.grad(
            outputs=u_x, inputs=coords,
            grad_outputs=torch.ones_like(u_x),
            create_graph=True, retain_graph=True
        )[0][:, 0:1]
        
        u_yy = torch.autograd.grad(
            outputs=u_y, inputs=coords,
            grad_outputs=torch.ones_like(u_y),
            create_graph=True, retain_graph=True
        )[0][:, 1:2]
        
        # æ‹‰æ™®æ‹‰æ–¯ç®—å­
        laplacian_u = u_xx + u_yy
        
        # æºé¡¹ï¼ˆå¯é€‰ï¼‰
        if self.source_term_enabled:
            x, y, t = coords[:, 0:1], coords[:, 1:2], coords[:, 2:3]
            source = torch.sin(np.pi * x) * torch.sin(np.pi * y) * torch.exp(-t)
        else:
            source = torch.zeros_like(u)
        
        # PDEæ®‹å·®: âˆ‚u/âˆ‚t - Î±âˆ‡Â²u - f = 0
        pde_residual = u_t - self.diffusion_coeff * laplacian_u - source
        
        return pde_residual
    
    def compute_boundary_loss(self, boundary_coords: torch.Tensor, 
                            boundary_values: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—è¾¹ç•Œæ¡ä»¶æŸå¤±"""
        predicted = self.forward(boundary_coords)
        return F.mse_loss(predicted, boundary_values)
    
    def compute_initial_loss(self, initial_coords: torch.Tensor,
                           initial_values: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—åˆå§‹æ¡ä»¶æŸå¤±"""
        predicted = self.forward(initial_coords)
        return F.mse_loss(predicted, initial_values)

class DeepOperatorNetwork(nn.Module):
    """æ·±åº¦ç®—å­ç½‘ç»œ (DeepONet)"""
    
    def __init__(self, config: DeepONetConfig):
        super().__init__()
        self.config = config
        
        # Branch Network: ç¼–ç è¾“å…¥å‡½æ•°
        branch_layers = []
        prev_dim = config.branch_layers[0]
        for hidden_dim in config.branch_layers[1:]:
            branch_layers.append(nn.Linear(prev_dim, hidden_dim))
            branch_layers.append(nn.ReLU())
            prev_dim = hidden_dim
        branch_layers.append(nn.Linear(prev_dim, config.latent_dim))
        
        self.branch_net = nn.Sequential(*branch_layers)
        
        # Trunk Network: ç¼–ç æŸ¥è¯¢åæ ‡
        trunk_layers = []
        prev_dim = config.trunk_layers[0]
        for hidden_dim in config.trunk_layers[1:]:
            trunk_layers.append(nn.Linear(prev_dim, hidden_dim))
            trunk_layers.append(nn.ReLU())
            prev_dim = hidden_dim
        trunk_layers.append(nn.Linear(prev_dim, config.latent_dim))
        
        self.trunk_net = nn.Sequential(*trunk_layers)
        
        # åç½®é¡¹
        self.bias = nn.Linear(config.latent_dim, config.output_dim)
    
    def forward(self, input_functions: torch.Tensor, 
                query_coords: torch.Tensor) -> torch.Tensor:
        """
        DeepONetå‰å‘ä¼ æ’­
        
        Args:
            input_functions: è¾“å…¥å‡½æ•°é‡‡æ · [batch, n_sensors]
            query_coords: æŸ¥è¯¢åæ ‡ [batch, n_points, coord_dim]
            
        Returns:
            é¢„æµ‹åœºå€¼ [batch, n_points, output_dim]
        """
        batch_size = input_functions.shape[0]
        n_points = query_coords.shape[1]
        
        # Branchç½‘ç»œç¼–ç è¾“å…¥å‡½æ•°
        branch_output = self.branch_net(input_functions)  # [batch, latent_dim]
        
        # Trunkç½‘ç»œç¼–ç æŸ¥è¯¢åæ ‡
        coords_flat = query_coords.view(-1, query_coords.shape[-1])  # [batch*n_points, coord_dim]
        trunk_output = self.trunk_net(coords_flat)  # [batch*n_points, latent_dim]
        trunk_output = trunk_output.view(batch_size, n_points, -1)  # [batch, n_points, latent_dim]
        
        # ç‚¹ç§¯æ“ä½œ + åç½®
        branch_expanded = branch_output.unsqueeze(1)  # [batch, 1, latent_dim]
        dot_product = torch.sum(branch_expanded * trunk_output, dim=-1, keepdim=True)  # [batch, n_points, 1]
        
        # æ·»åŠ åç½®
        bias_output = self.bias(trunk_output)  # [batch, n_points, output_dim]
        
        output = dot_product + bias_output
        
        return output

class GraphNeuralNetwork(nn.Module):
    """å›¾ç¥ç»ç½‘ç»œ (GNN) - åŸºäºGAT"""
    
    def __init__(self, config: GNNConfig):
        super().__init__()
        self.config = config
        
        if not TORCH_GEOMETRIC_AVAILABLE:
            raise ImportError("PyTorch Geometric is required for GNN")
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(config.node_features, config.hidden_dim)
        
        # GATå±‚
        self.gnn_layers = nn.ModuleList()
        for i in range(config.num_layers):
            if i == 0:
                in_dim = config.hidden_dim
            else:
                in_dim = config.hidden_dim * config.num_heads
            
            if i == config.num_layers - 1:
                # æœ€åä¸€å±‚ä¸ä½¿ç”¨å¤šå¤´
                self.gnn_layers.append(
                    GATConv(in_dim, config.hidden_dim, heads=1, dropout=config.dropout)
                )
            else:
                self.gnn_layers.append(
                    GATConv(in_dim, config.hidden_dim, heads=config.num_heads, dropout=config.dropout)
                )
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(config.hidden_dim, config.hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.hidden_dim // 2, 1)  # é¢„æµ‹æ ‡é‡åœº
        )
    
    def forward(self, data: 'torch_geometric.data.Data') -> torch.Tensor:
        """
        GNNå‰å‘ä¼ æ’­
        
        Args:
            data: PyTorch Geometricæ•°æ®å¯¹è±¡
            
        Returns:
            èŠ‚ç‚¹é¢„æµ‹å€¼
        """
        x, edge_index = data.x, data.edge_index
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)
        x = F.relu(x)
        
        # GATå±‚
        for i, gnn_layer in enumerate(self.gnn_layers):
            x = gnn_layer(x, edge_index)
            if i < len(self.gnn_layers) - 1:
                x = F.relu(x)
                x = F.dropout(x, p=self.config.dropout, training=self.training)
        
        # è¾“å‡ºé¢„æµ‹
        output = self.output_layer(x)
        
        return output

class HybridAIPredictor:
    """æ··åˆAIé¢„æµ‹å™¨ - æ•´åˆPINN+DeepONet+GNN"""
    
    def __init__(self, pinn_config: PINNConfig, 
                 deeponet_config: DeepONetConfig,
                 gnn_config: GNNConfig):
        """
        åˆå§‹åŒ–æ··åˆAIé¢„æµ‹å™¨
        
        Args:
            pinn_config: PINNé…ç½®
            deeponet_config: DeepONeté…ç½®  
            gnn_config: GNNé…ç½®
        """
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–å„ä¸ªç½‘ç»œ
        self.pinn = PhysicsInformedNN(pinn_config).to(device)
        self.deeponet = DeepOperatorNetwork(deeponet_config).to(device)
        
        if TORCH_GEOMETRIC_AVAILABLE:
            self.gnn = GraphNeuralNetwork(gnn_config).to(device)
        else:
            self.gnn = None
            self.logger.warning("GNNä¸å¯ç”¨ï¼Œå°†è·³è¿‡å›¾ç¥ç»ç½‘ç»œåŠŸèƒ½")
        
        # é…ç½®å­˜å‚¨
        self.pinn_config = pinn_config
        self.deeponet_config = deeponet_config
        self.gnn_config = gnn_config
        
        # ä¼˜åŒ–å™¨
        self.pinn_optimizer = torch.optim.Adam(self.pinn.parameters(), lr=pinn_config.learning_rate)
        self.deeponet_optimizer = torch.optim.Adam(self.deeponet.parameters(), lr=deeponet_config.learning_rate)
        
        if self.gnn is not None:
            self.gnn_optimizer = torch.optim.Adam(self.gnn.parameters(), lr=gnn_config.learning_rate)
        
        # è®­ç»ƒçŠ¶æ€
        self.is_trained = {"pinn": False, "deeponet": False, "gnn": False}
        self.training_history = {"pinn": [], "deeponet": [], "gnn": []}
    
    async def train_pinn(self, training_data: Dict) -> Dict:
        """
        è®­ç»ƒPINNç½‘ç»œ
        
        Args:
            training_data: åŒ…å«åŸŸå†…ç‚¹ã€è¾¹ç•Œç‚¹ã€åˆå§‹ç‚¹ç­‰
            
        Returns:
            è®­ç»ƒå†å²
        """
        self.logger.info("å¼€å§‹è®­ç»ƒPINN...")
        start_time = time.time()
        
        # è§£æè®­ç»ƒæ•°æ®
        domain_coords = training_data['domain_coords']  # åŸŸå†…é…ç‚¹
        boundary_coords = training_data.get('boundary_coords')
        boundary_values = training_data.get('boundary_values')
        initial_coords = training_data.get('initial_coords')
        initial_values = training_data.get('initial_values')
        
        # è½¬æ¢ä¸ºå¼ é‡
        domain_coords = torch.FloatTensor(domain_coords).to(device)
        if boundary_coords is not None:
            boundary_coords = torch.FloatTensor(boundary_coords).to(device)
            boundary_values = torch.FloatTensor(boundary_values).to(device)
        if initial_coords is not None:
            initial_coords = torch.FloatTensor(initial_coords).to(device)
            initial_values = torch.FloatTensor(initial_values).to(device)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.pinn_config.epochs):
            self.pinn_optimizer.zero_grad()
            
            # è®¡ç®—å„é¡¹æŸå¤±
            # 1. PDEæŸå¤±
            pde_residual = self.pinn.compute_pde_residual(domain_coords)
            pde_loss = torch.mean(pde_residual ** 2)
            
            total_loss = self.pinn_config.pde_weight * pde_loss
            
            # 2. è¾¹ç•Œæ¡ä»¶æŸå¤±
            if boundary_coords is not None:
                boundary_loss = self.pinn.compute_boundary_loss(boundary_coords, boundary_values)
                total_loss += self.pinn_config.boundary_weight * boundary_loss
            
            # 3. åˆå§‹æ¡ä»¶æŸå¤±
            if initial_coords is not None:
                initial_loss = self.pinn.compute_initial_loss(initial_coords, initial_values)
                total_loss += self.pinn_config.initial_weight * initial_loss
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            self.pinn_optimizer.step()
            
            # è®°å½•è®­ç»ƒå†å²
            if epoch % 1000 == 0:
                self.logger.info(f"PINN Epoch {epoch}, Loss: {total_loss.item():.6e}")
                self.training_history["pinn"].append({
                    "epoch": epoch,
                    "total_loss": total_loss.item(),
                    "pde_loss": pde_loss.item()
                })
        
        training_time = time.time() - start_time
        self.is_trained["pinn"] = True
        self.logger.info(f"PINNè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        
        return {
            "training_time": training_time,
            "final_loss": total_loss.item(),
            "history": self.training_history["pinn"]
        }
    
    async def train_deeponet(self, training_data: Dict) -> Dict:
        """
        è®­ç»ƒDeepONetç½‘ç»œ
        
        Args:
            training_data: åŒ…å«è¾“å…¥å‡½æ•°å’Œå¯¹åº”çš„è¾“å‡ºåœº
            
        Returns:
            è®­ç»ƒå†å²
        """
        self.logger.info("å¼€å§‹è®­ç»ƒDeepONet...")
        start_time = time.time()
        
        # è§£æè®­ç»ƒæ•°æ®
        input_functions = torch.FloatTensor(training_data['input_functions']).to(device)
        query_coords = torch.FloatTensor(training_data['query_coords']).to(device)
        target_fields = torch.FloatTensor(training_data['target_fields']).to(device)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.deeponet_config.epochs):
            self.deeponet_optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            predicted_fields = self.deeponet(input_functions, query_coords)
            
            # è®¡ç®—æŸå¤±
            loss = F.mse_loss(predicted_fields, target_fields)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.deeponet_optimizer.step()
            
            # è®°å½•è®­ç»ƒå†å²
            if epoch % 500 == 0:
                self.logger.info(f"DeepONet Epoch {epoch}, Loss: {loss.item():.6e}")
                self.training_history["deeponet"].append({
                    "epoch": epoch,
                    "loss": loss.item()
                })
        
        training_time = time.time() - start_time
        self.is_trained["deeponet"] = True
        self.logger.info(f"DeepONetè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        
        return {
            "training_time": training_time,
            "final_loss": loss.item(),
            "history": self.training_history["deeponet"]
        }
    
    async def train_gnn(self, training_data: Dict) -> Dict:
        """
        è®­ç»ƒGNNç½‘ç»œ
        
        Args:
            training_data: å›¾æ•°æ®åˆ—è¡¨
            
        Returns:
            è®­ç»ƒå†å²
        """
        if self.gnn is None:
            self.logger.warning("GNNä¸å¯ç”¨ï¼Œè·³è¿‡è®­ç»ƒ")
            return {"status": "skipped", "reason": "GNN not available"}
        
        self.logger.info("å¼€å§‹è®­ç»ƒGNN...")
        start_time = time.time()
        
        # è§£æå›¾æ•°æ®
        graph_data_list = training_data['graph_data']
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.gnn_config.epochs):
            self.gnn_optimizer.zero_grad()
            
            total_loss = 0.0
            for graph_data in graph_data_list:
                # å‰å‘ä¼ æ’­
                predicted = self.gnn(graph_data)
                target = graph_data.y
                
                # è®¡ç®—æŸå¤±
                loss = F.mse_loss(predicted, target)
                total_loss += loss.item()
                
                # ç´¯ç§¯æ¢¯åº¦
                loss.backward()
            
            # æ›´æ–°å‚æ•°
            self.gnn_optimizer.step()
            
            # è®°å½•è®­ç»ƒå†å²
            if epoch % 300 == 0:
                avg_loss = total_loss / len(graph_data_list)
                self.logger.info(f"GNN Epoch {epoch}, Avg Loss: {avg_loss:.6e}")
                self.training_history["gnn"].append({
                    "epoch": epoch,
                    "avg_loss": avg_loss
                })
        
        training_time = time.time() - start_time
        self.is_trained["gnn"] = True
        self.logger.info(f"GNNè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
        
        return {
            "training_time": training_time,
            "final_loss": total_loss / len(graph_data_list),
            "history": self.training_history["gnn"]
        }
    
    async def ensemble_predict(self, prediction_data: Dict) -> Dict:
        """
        é›†æˆé¢„æµ‹ï¼šç»“åˆPINNã€DeepONetå’ŒGNNçš„é¢„æµ‹ç»“æœ
        
        Args:
            prediction_data: é¢„æµ‹è¾“å…¥æ•°æ®
            
        Returns:
            é›†æˆé¢„æµ‹ç»“æœ
        """
        self.logger.info("å¼€å§‹é›†æˆé¢„æµ‹...")
        
        predictions = {}
        weights = {}
        
        # PINNé¢„æµ‹
        if self.is_trained["pinn"] and 'coords' in prediction_data:
            coords = torch.FloatTensor(prediction_data['coords']).to(device)
            with torch.no_grad():
                pinn_pred = self.pinn(coords).cpu().numpy()
            predictions['pinn'] = pinn_pred
            weights['pinn'] = 0.4  # PINNæƒé‡ï¼ˆç‰©ç†çº¦æŸå¼ºï¼‰
        
        # DeepONeté¢„æµ‹
        if (self.is_trained["deeponet"] and 
            'input_functions' in prediction_data and 
            'query_coords' in prediction_data):
            
            input_funcs = torch.FloatTensor(prediction_data['input_functions']).to(device)
            query_coords = torch.FloatTensor(prediction_data['query_coords']).to(device)
            
            with torch.no_grad():
                deeponet_pred = self.deeponet(input_funcs, query_coords).cpu().numpy()
            predictions['deeponet'] = deeponet_pred
            weights['deeponet'] = 0.4  # DeepONetæƒé‡ï¼ˆç®—å­å­¦ä¹ ï¼‰
        
        # GNNé¢„æµ‹
        if self.is_trained["gnn"] and self.gnn is not None and 'graph_data' in prediction_data:
            graph_data = prediction_data['graph_data']
            with torch.no_grad():
                gnn_pred = self.gnn(graph_data).cpu().numpy()
            predictions['gnn'] = gnn_pred
            weights['gnn'] = 0.2  # GNNæƒé‡ï¼ˆç»“æ„å…³ç³»ï¼‰
        
        # é›†æˆé¢„æµ‹ï¼ˆåŠ æƒå¹³å‡ï¼‰
        if predictions:
            # å½’ä¸€åŒ–æƒé‡
            total_weight = sum(weights.values())
            normalized_weights = {k: v/total_weight for k, v in weights.items()}
            
            # åŠ æƒé›†æˆ
            ensemble_result = None
            for model_name, pred in predictions.items():
                weight = normalized_weights[model_name]
                if ensemble_result is None:
                    ensemble_result = weight * pred
                else:
                    ensemble_result += weight * pred
            
            # ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆé¢„æµ‹æ–¹å·®ï¼‰
            if len(predictions) > 1:
                pred_values = list(predictions.values())
                uncertainty = np.var(pred_values, axis=0)
            else:
                uncertainty = np.zeros_like(ensemble_result)
            
            result = {
                "ensemble_prediction": ensemble_result,
                "uncertainty": uncertainty,
                "individual_predictions": predictions,
                "weights": normalized_weights,
                "models_used": list(predictions.keys())
            }
        else:
            result = {
                "error": "No trained models available for prediction",
                "models_trained": [k for k, v in self.is_trained.items() if v]
            }
        
        return result
    
    def save_models(self, save_dir: str):
        """ä¿å­˜æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹"""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        if self.is_trained["pinn"]:
            torch.save(self.pinn.state_dict(), os.path.join(save_dir, "pinn_model.pth"))
        
        if self.is_trained["deeponet"]:
            torch.save(self.deeponet.state_dict(), os.path.join(save_dir, "deeponet_model.pth"))
        
        if self.is_trained["gnn"] and self.gnn is not None:
            torch.save(self.gnn.state_dict(), os.path.join(save_dir, "gnn_model.pth"))
        
        # ä¿å­˜é…ç½®å’Œè®­ç»ƒå†å²
        metadata = {
            "is_trained": self.is_trained,
            "training_history": self.training_history,
            "configs": {
                "pinn": self.pinn_config.__dict__,
                "deeponet": self.deeponet_config.__dict__,
                "gnn": self.gnn_config.__dict__
            }
        }
        
        with open(os.path.join(save_dir, "metadata.json"), 'w') as f:
            json.dump(metadata, f, indent=2)
        
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {save_dir}")
    
    def load_models(self, save_dir: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        import os
        
        # åŠ è½½å…ƒæ•°æ®
        with open(os.path.join(save_dir, "metadata.json"), 'r') as f:
            metadata = json.load(f)
        
        self.is_trained = metadata["is_trained"]
        self.training_history = metadata["training_history"]
        
        # åŠ è½½æ¨¡å‹æƒé‡
        if self.is_trained["pinn"]:
            pinn_path = os.path.join(save_dir, "pinn_model.pth")
            if os.path.exists(pinn_path):
                self.pinn.load_state_dict(torch.load(pinn_path, map_location=device))
        
        if self.is_trained["deeponet"]:
            deeponet_path = os.path.join(save_dir, "deeponet_model.pth")
            if os.path.exists(deeponet_path):
                self.deeponet.load_state_dict(torch.load(deeponet_path, map_location=device))
        
        if self.is_trained["gnn"] and self.gnn is not None:
            gnn_path = os.path.join(save_dir, "gnn_model.pth")
            if os.path.exists(gnn_path):
                self.gnn.load_state_dict(torch.load(gnn_path, map_location=device))
        
        self.logger.info(f"æ¨¡å‹å·²ä» {save_dir} åŠ è½½")

# æ·±åŸºå‘å·¥ç¨‹ä¸“ç”¨AIé¢„æµ‹å™¨
class DeepExcavationAIPredictor:
    """æ·±åŸºå‘å·¥ç¨‹AIé¢„æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ·±åŸºå‘AIé¢„æµ‹å™¨"""
        self.logger = logging.getLogger(__name__)
        
        # æ·±åŸºå‘ä¸“ç”¨é…ç½®
        pinn_config = PINNConfig(
            hidden_layers=[64, 64, 64, 64],
            learning_rate=1e-3,
            epochs=15000,
            pde_weight=1.0,
            boundary_weight=20.0,  # è¾¹ç•Œæ¡ä»¶å¾ˆé‡è¦
            initial_weight=15.0,
            diffusion_coeff=1e-6   # åœŸä½“æ¸—é€ç³»æ•°
        )
        
        deeponet_config = DeepONetConfig(
            branch_layers=[200, 256, 256],  # æ›´å¤šä¼ æ„Ÿå™¨è¾“å…¥
            trunk_layers=[3, 128, 128],     # 3Dç©ºé—´åæ ‡
            latent_dim=256,
            learning_rate=5e-4,
            epochs=8000
        )
        
        gnn_config = GNNConfig(
            node_features=32,    # ä¸°å¯Œçš„èŠ‚ç‚¹ç‰¹å¾
            edge_features=16,
            hidden_dim=128,
            num_layers=6,        # æ›´æ·±çš„ç½‘ç»œ
            num_heads=8,
            learning_rate=1e-3,
            epochs=5000
        )
        
        # åˆ›å»ºæ··åˆAIé¢„æµ‹å™¨
        self.ai_predictor = HybridAIPredictor(pinn_config, deeponet_config, gnn_config)
    
    async def train_for_excavation(self, excavation_data: Dict) -> Dict:
        """
        é’ˆå¯¹æ·±åŸºå‘å·¥ç¨‹è®­ç»ƒAIæ¨¡å‹
        
        Args:
            excavation_data: æ·±åŸºå‘å·¥ç¨‹æ•°æ®
            
        Returns:
            è®­ç»ƒç»“æœæ±‡æ€»
        """
        self.logger.info("å¼€å§‹æ·±åŸºå‘AIé¢„æµ‹å™¨è®­ç»ƒ...")
        
        training_results = {}
        
        # 1. è®­ç»ƒPINNï¼ˆåœŸä½“æ¸—æµ-å˜å½¢è€¦åˆï¼‰
        if 'pinn_data' in excavation_data:
            pinn_result = await self.ai_predictor.train_pinn(excavation_data['pinn_data'])
            training_results['pinn'] = pinn_result
        
        # 2. è®­ç»ƒDeepONetï¼ˆå‚æ•°-å“åº”æ˜ å°„ï¼‰
        if 'deeponet_data' in excavation_data:
            deeponet_result = await self.ai_predictor.train_deeponet(excavation_data['deeponet_data'])
            training_results['deeponet'] = deeponet_result
        
        # 3. è®­ç»ƒGNNï¼ˆæ”¯æŠ¤ç»“æ„å›¾ï¼‰
        if 'gnn_data' in excavation_data:
            gnn_result = await self.ai_predictor.train_gnn(excavation_data['gnn_data'])
            training_results['gnn'] = gnn_result
        
        return training_results
    
    async def predict_excavation_response(self, excavation_params: Dict) -> Dict:
        """
        é¢„æµ‹æ·±åŸºå‘å“åº”
        
        Args:
            excavation_params: åŸºå‘å‚æ•°
            
        Returns:
            é¢„æµ‹ç»“æœï¼ˆä½ç§»ã€åº”åŠ›ã€æ¸—æµç­‰ï¼‰
        """
        prediction_data = self._prepare_excavation_prediction_data(excavation_params)
        
        # é›†æˆé¢„æµ‹
        result = await self.ai_predictor.ensemble_predict(prediction_data)
        
        # åå¤„ç†ä¸ºå·¥ç¨‹é‡
        if 'ensemble_prediction' in result:
            engineering_results = self._convert_to_engineering_quantities(
                result['ensemble_prediction'], excavation_params
            )
            result['engineering_results'] = engineering_results
        
        return result
    
    def _prepare_excavation_prediction_data(self, params: Dict) -> Dict:
        """å‡†å¤‡æ·±åŸºå‘é¢„æµ‹æ•°æ®"""
        # ç”Ÿæˆç©ºé—´åæ ‡
        x_range = params.get('x_range', [-10, 10])
        y_range = params.get('y_range', [-10, 10])
        depth_range = params.get('depth_range', [0, -15])
        
        # 3Dç½‘æ ¼ç‚¹
        nx, ny, nz = 20, 20, 15
        x = np.linspace(x_range[0], x_range[1], nx)
        y = np.linspace(y_range[0], y_range[1], ny)
        z = np.linspace(depth_range[0], depth_range[1], nz)
        
        X, Y, Z = np.meshgrid(x, y, z)
        coords = np.column_stack([X.ravel(), Y.ravel(), Z.ravel()])
        
        # è¾“å…¥å‡½æ•°ï¼ˆè¾¹ç•Œæ¡ä»¶ã€è½½è·ç­‰ï¼‰
        input_functions = self._generate_input_functions(params)
        
        # æŸ¥è¯¢åæ ‡ï¼ˆæ·»åŠ æ—¶é—´ç»´åº¦ï¼‰
        time_steps = params.get('time_steps', [0, 1, 2, 5, 10])
        query_coords_list = []
        for t in time_steps:
            coords_with_time = np.column_stack([coords, np.full(len(coords), t)])
            query_coords_list.append(coords_with_time)
        
        query_coords = np.array(query_coords_list)  # [n_times, n_points, 4]
        
        prediction_data = {
            'coords': coords,
            'input_functions': input_functions,
            'query_coords': query_coords
        }
        
        # å¦‚æœæœ‰å›¾æ•°æ®ï¼Œæ·»åŠ å›¾ç»“æ„
        if 'support_structure' in params:
            graph_data = self._create_support_graph(params['support_structure'])
            prediction_data['graph_data'] = graph_data
        
        return prediction_data
    
    def _generate_input_functions(self, params: Dict) -> np.ndarray:
        """ç”Ÿæˆè¾“å…¥å‡½æ•°ï¼ˆè¾¹ç•Œæ¡ä»¶ç­‰ï¼‰"""
        n_sensors = 100  # å‡è®¾100ä¸ªä¼ æ„Ÿå™¨ç‚¹
        
        # æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®ï¼ˆéšæœº+ç‰©ç†çº¦æŸï¼‰
        base_values = np.random.randn(n_sensors) * 0.1
        
        # æ ¹æ®å‚æ•°è°ƒæ•´
        excavation_depth = params.get('excavation_depth', 8.0)
        soil_modulus = params.get('soil_modulus', 30e6)
        
        # æ·±åº¦å½±å“
        depth_factor = excavation_depth / 10.0
        base_values *= depth_factor
        
        # åˆšåº¦å½±å“
        stiffness_factor = soil_modulus / 30e6
        base_values *= np.sqrt(stiffness_factor)
        
        return base_values.reshape(1, -1)  # [1, n_sensors]
    
    def _create_support_graph(self, support_structure: Dict):
        """åˆ›å»ºæ”¯æŠ¤ç»“æ„å›¾æ•°æ®"""
        if not TORCH_GEOMETRIC_AVAILABLE:
            return None
        
        # ç®€åŒ–çš„æ”¯æŠ¤ç»“æ„å›¾
        n_nodes = support_structure.get('n_support_points', 50)
        
        # èŠ‚ç‚¹ç‰¹å¾ï¼ˆä½ç½®ã€ç±»å‹ã€å±æ€§ç­‰ï¼‰
        node_features = torch.randn(n_nodes, 32)
        
        # è¾¹è¿æ¥ï¼ˆç›¸é‚»æ”¯æŠ¤ç‚¹ï¼‰
        edge_list = []
        for i in range(n_nodes - 1):
            edge_list.append([i, i + 1])
            edge_list.append([i + 1, i])  # åŒå‘è¾¹
        
        edge_index = torch.tensor(edge_list).t().contiguous()
        
        # ç›®æ ‡å€¼ï¼ˆä½ç§»ã€åº”åŠ›ç­‰ï¼‰
        target_values = torch.randn(n_nodes, 1)
        
        from torch_geometric.data import Data
        graph_data = Data(x=node_features, edge_index=edge_index, y=target_values)
        
        return graph_data
    
    def _convert_to_engineering_quantities(self, predictions: np.ndarray, 
                                         params: Dict) -> Dict:
        """è½¬æ¢ä¸ºå·¥ç¨‹é‡"""
        # ç®€åŒ–å¤„ç†ï¼šå‡è®¾é¢„æµ‹ç»“æœæ˜¯ä½ç§»åœº
        max_displacement = np.max(np.abs(predictions))
        avg_displacement = np.mean(np.abs(predictions))
        
        # å®‰å…¨ç³»æ•°ä¼°ç®—
        allowable_displacement = params.get('allowable_displacement', 0.025)  # 25mm
        safety_factor = allowable_displacement / max_displacement if max_displacement > 0 else float('inf')
        
        engineering_results = {
            'max_displacement_mm': max_displacement * 1000,  # è½¬æ¢ä¸ºmm
            'avg_displacement_mm': avg_displacement * 1000,
            'safety_factor': safety_factor,
            'risk_level': 'High' if safety_factor < 1.3 else 'Medium' if safety_factor < 2.0 else 'Low'
        }
        
        return engineering_results

if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    async def test_ai_prediction_system():
        """AIé¢„æµ‹ç³»ç»Ÿæµ‹è¯•"""
        print("ğŸ¤– DeepCADæ™ºèƒ½é¢„æµ‹ç³»ç»Ÿæµ‹è¯• ğŸ¤–")
        
        # åˆ›å»ºæ·±åŸºå‘AIé¢„æµ‹å™¨
        excavation_predictor = DeepExcavationAIPredictor()
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        n_domain_points = 1000
        domain_coords = np.random.uniform(-10, 10, (n_domain_points, 3))  # (x, y, t)
        
        # è¾¹ç•Œæ¡ä»¶ï¼ˆåŸºå‘è¾¹ç•Œï¼‰
        n_boundary = 200
        boundary_coords = np.random.uniform(-10, 10, (n_boundary, 3))
        boundary_values = np.zeros((n_boundary, 1))  # å›ºå®šè¾¹ç•Œ
        
        # PINNè®­ç»ƒæ•°æ®
        pinn_data = {
            'domain_coords': domain_coords,
            'boundary_coords': boundary_coords,
            'boundary_values': boundary_values
        }
        
        # DeepONetè®­ç»ƒæ•°æ®
        n_samples = 100
        input_functions = np.random.randn(n_samples, 200)  # 200ä¸ªä¼ æ„Ÿå™¨
        query_coords = np.random.uniform(-10, 10, (n_samples, 50, 3))  # æ¯ä¸ªæ ·æœ¬50ä¸ªæŸ¥è¯¢ç‚¹
        target_fields = np.random.randn(n_samples, 50, 1)  # å¯¹åº”çš„åœºå€¼
        
        deeponet_data = {
            'input_functions': input_functions,
            'query_coords': query_coords,
            'target_fields': target_fields
        }
        
        # ç»„åˆè®­ç»ƒæ•°æ®
        excavation_data = {
            'pinn_data': pinn_data,
            'deeponet_data': deeponet_data
        }
        
        # è®­ç»ƒAIæ¨¡å‹
        print("\nğŸ‹ï¸ å¼€å§‹AIæ¨¡å‹è®­ç»ƒ...")
        training_results = await excavation_predictor.train_for_excavation(excavation_data)
        
        print(f"\nğŸ“Š è®­ç»ƒç»“æœ:")
        for model_name, result in training_results.items():
            print(f"  {model_name}: è®­ç»ƒæ—¶é—´ {result['training_time']:.2f}ç§’, "
                  f"æœ€ç»ˆæŸå¤± {result['final_loss']:.6e}")
        
        # æµ‹è¯•é¢„æµ‹
        print("\nğŸ¯ æµ‹è¯•é¢„æµ‹åŠŸèƒ½...")
        excavation_params = {
            'excavation_depth': 8.0,
            'soil_modulus': 30e6,
            'x_range': [-15, 15],
            'y_range': [-15, 15],
            'depth_range': [0, -10],
            'allowable_displacement': 0.025
        }
        
        prediction_result = await excavation_predictor.predict_excavation_response(excavation_params)
        
        if 'ensemble_prediction' in prediction_result:
            print(f"  é›†æˆé¢„æµ‹å®Œæˆ")
            print(f"  ä½¿ç”¨æ¨¡å‹: {prediction_result['models_used']}")
            print(f"  é¢„æµ‹ç»´åº¦: {prediction_result['ensemble_prediction'].shape}")
            
            if 'engineering_results' in prediction_result:
                eng_results = prediction_result['engineering_results']
                print(f"  æœ€å¤§ä½ç§»: {eng_results['max_displacement_mm']:.2f}mm")
                print(f"  å®‰å…¨ç³»æ•°: {eng_results['safety_factor']:.2f}")
                print(f"  é£é™©ç­‰çº§: {eng_results['risk_level']}")
        
        # ä¿å­˜æ¨¡å‹
        excavation_predictor.ai_predictor.save_models("ai_models")
        print("\nâœ… AIé¢„æµ‹ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_ai_prediction_system())