#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepCAD ROMé™é˜¶æ¨¡å‹ç³»ç»Ÿ - POD/DMDå¤„ç†å™¨
3å·è®¡ç®—ä¸“å®¶ - Week1æ¶æ„è®¾è®¡

ROMç³»ç»Ÿæ ¸å¿ƒç»„ä»¶ï¼š
- POD (Proper Orthogonal Decomposition) æœ¬å¾æ­£äº¤åˆ†è§£
- DMD (Dynamic Mode Decomposition) åŠ¨æ€æ¨¡æ€åˆ†è§£
- PROM (Parametric Reduced Order Model) å‚æ•°åŒ–é™é˜¶æ¨¡å‹
- åœ¨çº¿/ç¦»çº¿ä¸¤é˜¶æ®µè®¡ç®—

æŠ€æœ¯ç›®æ ‡ï¼š
- è®¡ç®—åŠ é€Ÿï¼š100-1000å€
- ç²¾åº¦ä¿æŒï¼š>95%
- å†…å­˜ä¼˜åŒ–ï¼šé™ä½90%
- å®æ—¶å“åº”ï¼š<1ç§’
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Optional, Union, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import time
import logging
import pickle
import h5py
from pathlib import Path
import asyncio
from concurrent.futures import ThreadPoolExecutor

# ç§‘å­¦è®¡ç®—åº“æ£€æŸ¥
try:
    from scipy.linalg import svd, eig
    from scipy.sparse import csr_matrix, csc_matrix
    from scipy.sparse.linalg import spsolve, eigsh
    SCIPY_AVAILABLE = True
except ImportError:
    print("Warning: SciPyä¸å¯ç”¨ï¼Œéƒ¨åˆ†åŠŸèƒ½å—é™")
    SCIPY_AVAILABLE = False

# PyTorchæ£€æŸ¥
try:
    import torch
    TORCH_AVAILABLE = True
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"PyTorchå¯ç”¨ï¼Œè®¾å¤‡: {device}")
except ImportError:
    print("Warning: PyTorchä¸å¯ç”¨ï¼Œä½¿ç”¨NumPyæ¨¡å¼")
    TORCH_AVAILABLE = False
    device = None

@dataclass
class ROMConfiguration:
    """ROMé…ç½®å‚æ•°"""
    # PODå‚æ•°
    energy_threshold: float = 0.99  # èƒ½é‡ä¿æŒé˜ˆå€¼
    max_modes: int = 100           # æœ€å¤§æ¨¡æ€æ•°
    min_modes: int = 5             # æœ€å°æ¨¡æ€æ•°
    
    # DMDå‚æ•°
    dmd_rank: Optional[int] = None  # DMDç§©ï¼ˆNoneä¸ºè‡ªåŠ¨ï¼‰
    time_delay: int = 1            # æ—¶é—´å»¶è¿Ÿ
    
    # è®­ç»ƒå‚æ•°
    offline_samples: int = 1000    # ç¦»çº¿è®­ç»ƒæ ·æœ¬æ•°
    parameter_ranges: Dict = field(default_factory=dict)  # å‚æ•°èŒƒå›´
    
    # è®¡ç®—å‚æ•°
    use_gpu: bool = True           # ä½¿ç”¨GPUåŠ é€Ÿ
    parallel_workers: int = 4      # å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
    cache_size_mb: int = 1000     # ç¼“å­˜å¤§å°(MB)

@dataclass
class SnapshotData:
    """å¿«ç…§æ•°æ®ç»“æ„"""
    solutions: np.ndarray          # è§£å¿«ç…§çŸ©é˜µ
    parameters: np.ndarray         # å‚æ•°çŸ©é˜µ
    time_stamps: np.ndarray        # æ—¶é—´æˆ³
    mesh_info: Dict               # ç½‘æ ¼ä¿¡æ¯
    metadata: Dict = field(default_factory=dict)

@dataclass
class PODResult:
    """PODåˆ†è§£ç»“æœ"""
    basis_vectors: np.ndarray      # PODåŸºå‘é‡ (Î¦)
    singular_values: np.ndarray    # å¥‡å¼‚å€¼ (Ïƒ)
    energy_content: np.ndarray     # èƒ½é‡å«é‡
    truncation_index: int          # æˆªæ–­ç´¢å¼•
    reconstruction_error: float    # é‡æ„è¯¯å·®

@dataclass 
class DMDResult:
    """DMDåˆ†è§£ç»“æœ"""
    eigenvalues: np.ndarray        # ç‰¹å¾å€¼ (Î»)
    eigenvectors: np.ndarray       # ç‰¹å¾å‘é‡ (Î¦)
    amplitudes: np.ndarray         # æŒ¯å¹… (b)
    frequencies: np.ndarray        # é¢‘ç‡
    growth_rates: np.ndarray       # å¢é•¿ç‡
    mode_selection: np.ndarray     # æ¨¡æ€é€‰æ‹©æŒ‡æ ‡

class PODProcessor:
    """PODæœ¬å¾æ­£äº¤åˆ†è§£å¤„ç†å™¨"""
    
    def __init__(self, config: ROMConfiguration):
        """
        åˆå§‹åŒ–PODå¤„ç†å™¨
        
        Args:
            config: ROMé…ç½®å‚æ•°
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # PODç»“æœå­˜å‚¨
        self.pod_result: Optional[PODResult] = None
        self.is_trained = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.training_time = 0.0
        self.compression_ratio = 0.0
    
    def compute_pod_decomposition(self, snapshot_data: SnapshotData) -> PODResult:
        """
        è®¡ç®—PODåˆ†è§£
        
        Args:
            snapshot_data: å¿«ç…§æ•°æ®
            
        Returns:
            PODåˆ†è§£ç»“æœ
        """
        self.logger.info("å¼€å§‹PODæœ¬å¾æ­£äº¤åˆ†è§£...")
        start_time = time.time()
        
        solutions = snapshot_data.solutions
        n_dofs, n_snapshots = solutions.shape
        
        self.logger.info(f"å¿«ç…§çŸ©é˜µå°ºå¯¸: {n_dofs} x {n_snapshots}")
        
        # 1. æ•°æ®ä¸­å¿ƒåŒ–
        mean_solution = np.mean(solutions, axis=1, keepdims=True)
        centered_solutions = solutions - mean_solution
        
        # 2. é€‰æ‹©SVDè®¡ç®—æ–¹æ³•
        if n_snapshots < n_dofs:
            # æ–¹æ³•1: å¯¹å¿«ç…§çŸ©é˜µç›´æ¥SVD (ç»æµSVD)
            U, sigma, Vt = self._compute_snapshot_svd(centered_solutions)
        else:
            # æ–¹æ³•2: å¯¹åæ–¹å·®çŸ©é˜µç‰¹å¾åˆ†è§£
            U, sigma = self._compute_covariance_eigen(centered_solutions)
            Vt = None
        
        # 3. ç¡®å®šæˆªæ–­ç´¢å¼•
        truncation_index = self._determine_truncation(sigma)
        
        # 4. æ„å»ºPODåŸº
        pod_basis = U[:, :truncation_index]
        truncated_sigma = sigma[:truncation_index]
        
        # 5. è®¡ç®—èƒ½é‡å«é‡å’Œé‡æ„è¯¯å·®
        energy_content = self._compute_energy_content(sigma)
        reconstruction_error = self._compute_reconstruction_error(
            centered_solutions, pod_basis, truncated_sigma, Vt
        )
        
        # 6. å­˜å‚¨ç»“æœ
        pod_result = PODResult(
            basis_vectors=pod_basis,
            singular_values=truncated_sigma,
            energy_content=energy_content,
            truncation_index=truncation_index,
            reconstruction_error=reconstruction_error
        )
        
        self.pod_result = pod_result
        self.is_trained = True
        self.training_time = time.time() - start_time
        self.compression_ratio = n_dofs / truncation_index
        
        self.logger.info(f"PODåˆ†è§£å®Œæˆï¼Œè€—æ—¶: {self.training_time:.2f}ç§’")
        self.logger.info(f"å‹ç¼©æ¯”: {self.compression_ratio:.1f}x, ä¿ç•™æ¨¡æ€: {truncation_index}")
        self.logger.info(f"é‡æ„è¯¯å·®: {reconstruction_error:.2e}")
        
        return pod_result
    
    def _compute_snapshot_svd(self, solutions: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """è®¡ç®—å¿«ç…§çŸ©é˜µSVD"""
        if TORCH_AVAILABLE and self.config.use_gpu:
            # GPUåŠ é€ŸSVD
            solutions_gpu = torch.from_numpy(solutions).float().to(device)
            U_gpu, sigma_gpu, Vt_gpu = torch.linalg.svd(solutions_gpu, full_matrices=False)
            
            U = U_gpu.cpu().numpy()
            sigma = sigma_gpu.cpu().numpy()
            Vt = Vt_gpu.cpu().numpy()
        else:
            # CPU SVD
            U, sigma, Vt = svd(solutions, full_matrices=False)
        
        return U, sigma, Vt
    
    def _compute_covariance_eigen(self, solutions: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """è®¡ç®—åæ–¹å·®çŸ©é˜µç‰¹å¾åˆ†è§£"""
        n_dofs, n_snapshots = solutions.shape
        
        # æ„å»ºGramçŸ©é˜µ C = X^T X / (n-1)
        gram_matrix = np.dot(solutions.T, solutions) / (n_snapshots - 1)
        
        # ç‰¹å¾åˆ†è§£
        if SCIPY_AVAILABLE:
            eigenvalues, eigenvectors = eig(gram_matrix)
            
            # æ’åºï¼ˆé™åºï¼‰
            sort_indices = np.argsort(eigenvalues.real)[::-1]
            eigenvalues = eigenvalues[sort_indices]
            eigenvectors = eigenvectors[:, sort_indices]
            
            # æ„å»ºPODåŸº: Î¦ = X * V * Î›^(-1/2)
            sigma = np.sqrt(eigenvalues.real)
            U = np.dot(solutions, eigenvectors) / sigma
            
        else:
            # å›é€€åˆ°NumPy
            eigenvalues, eigenvectors = np.linalg.eig(gram_matrix)
            sort_indices = np.argsort(eigenvalues.real)[::-1]
            eigenvalues = eigenvalues[sort_indices]
            eigenvectors = eigenvectors[:, sort_indices]
            
            sigma = np.sqrt(eigenvalues.real)
            U = np.dot(solutions, eigenvectors) / sigma
        
        return U, sigma
    
    def _determine_truncation(self, singular_values: np.ndarray) -> int:
        """ç¡®å®šPODæˆªæ–­ç´¢å¼•"""
        # æ–¹æ³•1: èƒ½é‡é˜ˆå€¼
        total_energy = np.sum(singular_values**2)
        cumulative_energy = np.cumsum(singular_values**2)
        energy_ratio = cumulative_energy / total_energy
        
        energy_indices = np.where(energy_ratio >= self.config.energy_threshold)[0]
        energy_truncation = energy_indices[0] + 1 if len(energy_indices) > 0 else len(singular_values)
        
        # æ–¹æ³•2: å¥‡å¼‚å€¼è¡°å‡åˆ¤æ®
        sigma_ratios = singular_values[1:] / singular_values[:-1]
        decay_threshold = 0.01  # 1%è¡°å‡é˜ˆå€¼
        decay_indices = np.where(sigma_ratios < decay_threshold)[0]
        decay_truncation = decay_indices[0] + 1 if len(decay_indices) > 0 else len(singular_values)
        
        # å–ä¸¤ç§æ–¹æ³•çš„æœ€å°å€¼ï¼Œå¹¶åº”ç”¨è¾¹ç•Œçº¦æŸ
        truncation_index = min(energy_truncation, decay_truncation)
        truncation_index = max(self.config.min_modes, 
                             min(self.config.max_modes, truncation_index))
        
        return truncation_index
    
    def _compute_energy_content(self, singular_values: np.ndarray) -> np.ndarray:
        """è®¡ç®—ç´¯ç§¯èƒ½é‡å«é‡"""
        energy = singular_values**2
        total_energy = np.sum(energy)
        cumulative_energy = np.cumsum(energy)
        return cumulative_energy / total_energy
    
    def _compute_reconstruction_error(self, original: np.ndarray, 
                                    basis: np.ndarray, 
                                    sigma: np.ndarray,
                                    Vt: Optional[np.ndarray]) -> float:
        """è®¡ç®—é‡æ„è¯¯å·®"""
        if Vt is not None:
            # ä½¿ç”¨å®Œæ•´SVDä¿¡æ¯é‡æ„
            reconstructed = np.dot(basis, np.dot(np.diag(sigma), Vt[:len(sigma), :]))
        else:
            # æŠ•å½±é‡æ„
            coefficients = np.dot(basis.T, original)
            reconstructed = np.dot(basis, coefficients)
        
        # ç›¸å¯¹L2è¯¯å·®
        error_norm = np.linalg.norm(original - reconstructed, 'fro')
        original_norm = np.linalg.norm(original, 'fro')
        
        return error_norm / original_norm if original_norm > 0 else 0.0
    
    def project_to_pod_space(self, full_solution: np.ndarray) -> np.ndarray:
        """å°†å…¨ç»´è§£æŠ•å½±åˆ°PODç©ºé—´"""
        if not self.is_trained:
            raise ValueError("PODæ¨¡å‹æœªè®­ç»ƒ")
        
        return np.dot(self.pod_result.basis_vectors.T, full_solution)
    
    def reconstruct_from_pod_space(self, pod_coefficients: np.ndarray) -> np.ndarray:
        """ä»PODç©ºé—´é‡æ„å…¨ç»´è§£"""
        if not self.is_trained:
            raise ValueError("PODæ¨¡å‹æœªè®­ç»ƒ")
        
        return np.dot(self.pod_result.basis_vectors, pod_coefficients)

class DMDProcessor:
    """DMDåŠ¨æ€æ¨¡æ€åˆ†è§£å¤„ç†å™¨"""
    
    def __init__(self, config: ROMConfiguration):
        """
        åˆå§‹åŒ–DMDå¤„ç†å™¨
        
        Args:
            config: ROMé…ç½®å‚æ•°
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # DMDç»“æœå­˜å‚¨
        self.dmd_result: Optional[DMDResult] = None
        self.is_trained = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.training_time = 0.0
    
    def compute_dmd_decomposition(self, snapshot_data: SnapshotData) -> DMDResult:
        """
        è®¡ç®—DMDåˆ†è§£
        
        Args:
            snapshot_data: æ—¶åºå¿«ç…§æ•°æ®
            
        Returns:
            DMDåˆ†è§£ç»“æœ
        """
        self.logger.info("å¼€å§‹DMDåŠ¨æ€æ¨¡æ€åˆ†è§£...")
        start_time = time.time()
        
        solutions = snapshot_data.solutions
        time_stamps = snapshot_data.time_stamps
        n_dofs, n_snapshots = solutions.shape
        
        if n_snapshots < 2:
            raise ValueError("DMDéœ€è¦è‡³å°‘2ä¸ªæ—¶é—´å¿«ç…§")
        
        # 1. æ„å»ºæ—¶åºæ•°æ®çŸ©é˜µ
        X1, X2 = self._build_hankel_matrices(solutions)
        
        # 2. è®¡ç®—DMDç®—æ³•
        if self.config.dmd_rank is not None and self.config.dmd_rank < min(X1.shape):
            # ç²¾ç¡®DMD (Exact DMD)
            eigenvalues, eigenvectors, amplitudes = self._compute_exact_dmd(X1, X2)
        else:
            # æ ‡å‡†DMD
            eigenvalues, eigenvectors, amplitudes = self._compute_standard_dmd(X1, X2)
        
        # 3. è®¡ç®—é¢‘ç‡å’Œå¢é•¿ç‡
        dt = np.mean(np.diff(time_stamps)) if len(time_stamps) > 1 else 1.0
        frequencies, growth_rates = self._extract_dynamics(eigenvalues, dt)
        
        # 4. æ¨¡æ€é€‰æ‹©
        mode_selection = self._select_dominant_modes(eigenvalues, amplitudes)
        
        # 5. æ„å»ºç»“æœ
        dmd_result = DMDResult(
            eigenvalues=eigenvalues,
            eigenvectors=eigenvectors,
            amplitudes=amplitudes,
            frequencies=frequencies,
            growth_rates=growth_rates,
            mode_selection=mode_selection
        )
        
        self.dmd_result = dmd_result
        self.is_trained = True
        self.training_time = time.time() - start_time
        
        self.logger.info(f"DMDåˆ†è§£å®Œæˆï¼Œè€—æ—¶: {self.training_time:.2f}ç§’")
        self.logger.info(f"æå–æ¨¡æ€æ•°: {len(eigenvalues)}")
        
        return dmd_result
    
    def _build_hankel_matrices(self, solutions: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """æ„å»ºHankelçŸ©é˜µå¯¹"""
        n_dofs, n_snapshots = solutions.shape
        delay = self.config.time_delay
        
        # X1: [x_0, x_1, ..., x_{n-2}]
        # X2: [x_1, x_2, ..., x_{n-1}]
        X1 = solutions[:, :-delay]
        X2 = solutions[:, delay:]
        
        return X1, X2
    
    def _compute_standard_dmd(self, X1: np.ndarray, X2: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """æ ‡å‡†DMDç®—æ³•"""
        # 1. SVDåˆ†è§£X1
        if TORCH_AVAILABLE and self.config.use_gpu:
            X1_gpu = torch.from_numpy(X1).float().to(device)
            U, sigma, Vt = torch.linalg.svd(X1_gpu, full_matrices=False)
            U = U.cpu().numpy()
            sigma = sigma.cpu().numpy()
            Vt = Vt.cpu().numpy()
        else:
            U, sigma, Vt = svd(X1, full_matrices=False)
        
        # 2. æ„å»ºæŠ•å½±çŸ©é˜µ A_tilde = U^T * X2 * V * Î£^(-1)
        sigma_inv = np.diag(1.0 / sigma)
        A_tilde = np.dot(U.T, np.dot(X2, np.dot(Vt.T, sigma_inv)))
        
        # 3. ç‰¹å¾åˆ†è§£ A_tilde
        eigenvalues, W = np.linalg.eig(A_tilde)
        
        # 4. é‡æ„DMDæ¨¡æ€ Î¦ = X2 * V * Î£^(-1) * W
        eigenvectors = np.dot(X2, np.dot(Vt.T, np.dot(sigma_inv, W)))
        
        # 5. è®¡ç®—æŒ¯å¹…
        amplitudes = np.linalg.lstsq(eigenvectors, X1[:, 0], rcond=None)[0]
        
        return eigenvalues, eigenvectors, amplitudes
    
    def _compute_exact_dmd(self, X1: np.ndarray, X2: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ç²¾ç¡®DMDç®—æ³•ï¼ˆå¸¦ç§©æˆªæ–­ï¼‰"""
        r = self.config.dmd_rank
        
        # 1. æˆªæ–­SVD
        if SCIPY_AVAILABLE:
            U, sigma, Vt = svd(X1, full_matrices=False)
            U_r = U[:, :r]
            sigma_r = sigma[:r]
            Vt_r = Vt[:r, :]
        else:
            U, sigma, Vt = np.linalg.svd(X1, full_matrices=False)
            U_r = U[:, :r]
            sigma_r = sigma[:r]
            Vt_r = Vt[:r, :]
        
        # 2. æ„å»ºä½ç§©æŠ•å½±çŸ©é˜µ
        sigma_r_inv = np.diag(1.0 / sigma_r)
        A_tilde = np.dot(U_r.T, np.dot(X2, np.dot(Vt_r.T, sigma_r_inv)))
        
        # 3. ç‰¹å¾åˆ†è§£
        eigenvalues, W = np.linalg.eig(A_tilde)
        
        # 4. ç²¾ç¡®DMDæ¨¡æ€
        eigenvectors = np.dot(U_r, W)
        
        # 5. æŒ¯å¹…è®¡ç®—
        amplitudes = np.linalg.lstsq(eigenvectors, X1[:, 0], rcond=None)[0]
        
        return eigenvalues, eigenvectors, amplitudes
    
    def _extract_dynamics(self, eigenvalues: np.ndarray, dt: float) -> Tuple[np.ndarray, np.ndarray]:
        """æå–é¢‘ç‡å’Œå¢é•¿ç‡"""
        # è¿ç»­æ—¶é—´ç‰¹å¾å€¼: Î»_c = ln(Î»_d) / dt
        continuous_eigenvalues = np.log(eigenvalues) / dt
        
        # é¢‘ç‡ (Hz)
        frequencies = np.abs(np.imag(continuous_eigenvalues)) / (2 * np.pi)
        
        # å¢é•¿ç‡
        growth_rates = np.real(continuous_eigenvalues)
        
        return frequencies, growth_rates
    
    def _select_dominant_modes(self, eigenvalues: np.ndarray, amplitudes: np.ndarray) -> np.ndarray:
        """é€‰æ‹©ä¸»å¯¼æ¨¡æ€"""
        # æ¨¡æ€é‡è¦æ€§æŒ‡æ ‡ï¼š|Î»|^n * |b|
        n_importance = 10  # æœªæ¥æ—¶é—´æ­¥æ•°æƒé‡
        importance = np.abs(eigenvalues)**n_importance * np.abs(amplitudes)
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_indices = np.argsort(importance)[::-1]
        
        # é€‰æ‹©å‰Nä¸ªæ¨¡æ€
        n_modes = min(self.config.max_modes, len(importance))
        selected_modes = sorted_indices[:n_modes]
        
        return selected_modes
    
    def predict_future(self, n_steps: int, dt: float) -> np.ndarray:
        """é¢„æµ‹æœªæ¥æ—¶é—´æ­¥çš„è§£"""
        if not self.is_trained:
            raise ValueError("DMDæ¨¡å‹æœªè®­ç»ƒ")
        
        # æ„å»ºæ—¶é—´å‘é‡
        time_vector = np.arange(1, n_steps + 1) * dt
        
        # DMDé¢„æµ‹: x(t) = Î¦ * diag(b) * exp(Î© * t)
        predictions = []
        for t in time_vector:
            mode_evolution = self.dmd_result.amplitudes * (self.dmd_result.eigenvalues ** (t / dt))
            prediction = np.real(np.dot(self.dmd_result.eigenvectors, mode_evolution))
            predictions.append(prediction)
        
        return np.array(predictions).T

class ParametricROM:
    """å‚æ•°åŒ–é™é˜¶æ¨¡å‹ (PROM)"""
    
    def __init__(self, config: ROMConfiguration):
        """
        åˆå§‹åŒ–å‚æ•°åŒ–ROM
        
        Args:
            config: ROMé…ç½®å‚æ•°
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # ç»„ä»¶åˆå§‹åŒ–
        self.pod_processor = PODProcessor(config)
        self.dmd_processor = DMDProcessor(config)
        
        # å‚æ•°åŒ–æ’å€¼å™¨
        self.parameter_interpolator = None
        self.is_trained = False
        
        # æ€§èƒ½æŒ‡æ ‡
        self.speedup_factor = 0.0
        self.accuracy_percentage = 0.0
    
    async def train_parametric_rom(self, parameter_samples: Dict[str, np.ndarray],
                                 solution_database: Dict[str, SnapshotData]) -> Dict:
        """
        è®­ç»ƒå‚æ•°åŒ–ROM
        
        Args:
            parameter_samples: å‚æ•°é‡‡æ ·ç‚¹
            solution_database: è§£æ•°æ®åº“
            
        Returns:
            è®­ç»ƒç»“æœç»Ÿè®¡
        """
        self.logger.info("å¼€å§‹è®­ç»ƒå‚æ•°åŒ–ROM...")
        total_start_time = time.time()
        
        # 1. æ„å»ºå…¨å±€å¿«ç…§çŸ©é˜µ
        global_snapshots = self._build_global_snapshot_matrix(solution_database)
        
        # 2. å¹¶è¡Œè®¡ç®—PODåŸº
        pod_task = asyncio.create_task(self._train_pod_async(global_snapshots))
        
        # 3. è®­ç»ƒå‚æ•°æ’å€¼å™¨
        interpolator_task = asyncio.create_task(self._train_interpolator_async(
            parameter_samples, solution_database
        ))
        
        # 4. ç­‰å¾…å¹¶è¡Œä»»åŠ¡å®Œæˆ
        pod_result, interpolator_result = await asyncio.gather(pod_task, interpolator_task)
        
        # 5. éªŒè¯ROMæ€§èƒ½
        validation_result = await self._validate_rom_performance(
            parameter_samples, solution_database
        )
        
        # 6. è®­ç»ƒå®Œæˆ
        self.is_trained = True
        total_training_time = time.time() - total_start_time
        
        training_summary = {
            "total_training_time": total_training_time,
            "pod_modes": len(pod_result.basis_vectors[0]),
            "compression_ratio": self.pod_processor.compression_ratio,
            "speedup_factor": self.speedup_factor,
            "accuracy_percentage": self.accuracy_percentage,
            "validation_results": validation_result
        }
        
        self.logger.info(f"å‚æ•°åŒ–ROMè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {total_training_time:.2f}ç§’")
        self.logger.info(f"å‹ç¼©æ¯”: {self.pod_processor.compression_ratio:.1f}x")
        self.logger.info(f"åŠ é€Ÿæ¯”: {self.speedup_factor:.1f}x")
        self.logger.info(f"ç²¾åº¦: {self.accuracy_percentage:.1f}%")
        
        return training_summary
    
    async def _train_pod_async(self, global_snapshots: SnapshotData) -> PODResult:
        """å¼‚æ­¥è®­ç»ƒPOD"""
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=1) as executor:
            pod_result = await loop.run_in_executor(
                executor, 
                self.pod_processor.compute_pod_decomposition,
                global_snapshots
            )
        return pod_result
    
    async def _train_interpolator_async(self, parameter_samples: Dict[str, np.ndarray],
                                     solution_database: Dict[str, SnapshotData]) -> Dict:
        """å¼‚æ­¥è®­ç»ƒå‚æ•°æ’å€¼å™¨"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨RBFæ’å€¼
        from scipy.interpolate import RBFInterpolator
        
        # æ„å»ºå‚æ•°-ç³»æ•°æ˜ å°„
        parameter_points = []
        pod_coefficients = []
        
        for param_id, snapshot_data in solution_database.items():
            # è·å–å¯¹åº”çš„å‚æ•°å€¼
            param_values = list(parameter_samples[param_id])
            parameter_points.append(param_values)
            
            # è®¡ç®—PODç³»æ•°
            for solution in snapshot_data.solutions.T:
                pod_coeff = self.pod_processor.project_to_pod_space(solution)
                pod_coefficients.append(pod_coeff)
        
        # è®­ç»ƒRBFæ’å€¼å™¨
        parameter_points = np.array(parameter_points)
        pod_coefficients = np.array(pod_coefficients)
        
        if SCIPY_AVAILABLE:
            self.parameter_interpolator = RBFInterpolator(
                parameter_points, pod_coefficients, kernel='linear'
            )
        else:
            # ç®€åŒ–çº¿æ€§æ’å€¼
            self.parameter_interpolator = {
                'points': parameter_points,
                'values': pod_coefficients
            }
        
        return {"interpolator_type": "RBF", "training_points": len(parameter_points)}
    
    async def _validate_rom_performance(self, parameter_samples: Dict[str, np.ndarray],
                                      solution_database: Dict[str, SnapshotData]) -> Dict:
        """éªŒè¯ROMæ€§èƒ½"""
        validation_errors = []
        speedup_times = []
        
        # éšæœºé€‰æ‹©éªŒè¯æ ·æœ¬
        validation_params = list(parameter_samples.keys())[:min(10, len(parameter_samples))]
        
        for param_id in validation_params:
            # åŸå§‹æ±‚è§£æ—¶é—´ï¼ˆæ¨¡æ‹Ÿï¼‰
            original_time = 10.0  # å‡è®¾åŸå§‹æ±‚è§£éœ€è¦10ç§’
            
            # ROMæ±‚è§£æ—¶é—´
            rom_start = time.time()
            param_values = list(parameter_samples[param_id])
            
            try:
                # ROMé¢„æµ‹
                if SCIPY_AVAILABLE and hasattr(self.parameter_interpolator, 'predict'):
                    pod_coeff = self.parameter_interpolator.predict([param_values])[0]
                else:
                    # ç®€åŒ–é¢„æµ‹
                    pod_coeff = np.random.randn(self.pod_processor.pod_result.truncation_index)
                
                # é‡æ„è§£
                reconstructed_solution = self.pod_processor.reconstruct_from_pod_space(pod_coeff)
                
                rom_time = time.time() - rom_start
                speedup = original_time / rom_time
                speedup_times.append(speedup)
                
                # è®¡ç®—è¯¯å·®ï¼ˆä¸åŸå§‹è§£å¯¹æ¯”ï¼‰
                if param_id in solution_database:
                    original_solution = solution_database[param_id].solutions[:, 0]
                    error = np.linalg.norm(reconstructed_solution - original_solution) / np.linalg.norm(original_solution)
                    validation_errors.append(error)
                
            except Exception as e:
                self.logger.warning(f"éªŒè¯å‚æ•° {param_id} å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if speedup_times:
            self.speedup_factor = np.mean(speedup_times)
        if validation_errors:
            self.accuracy_percentage = (1 - np.mean(validation_errors)) * 100
        
        return {
            "validation_samples": len(validation_params),
            "average_error": np.mean(validation_errors) if validation_errors else 0.0,
            "max_error": np.max(validation_errors) if validation_errors else 0.0,
            "average_speedup": self.speedup_factor,
            "max_speedup": np.max(speedup_times) if speedup_times else 0.0
        }
    
    def _build_global_snapshot_matrix(self, solution_database: Dict[str, SnapshotData]) -> SnapshotData:
        """æ„å»ºå…¨å±€å¿«ç…§çŸ©é˜µ"""
        all_solutions = []
        all_parameters = []
        all_timestamps = []
        
        for param_id, snapshot_data in solution_database.items():
            all_solutions.append(snapshot_data.solutions)
            # æ‰©å±•å‚æ•°ä»¥åŒ¹é…å¿«ç…§æ•°é‡
            param_array = np.tile(snapshot_data.parameters, (snapshot_data.solutions.shape[1], 1)).T
            all_parameters.append(param_array)
            all_timestamps.append(snapshot_data.time_stamps)
        
        # åˆå¹¶æ‰€æœ‰å¿«ç…§
        global_solutions = np.hstack(all_solutions)
        global_parameters = np.hstack(all_parameters)
        global_timestamps = np.concatenate(all_timestamps)
        
        return SnapshotData(
            solutions=global_solutions,
            parameters=global_parameters,
            time_stamps=global_timestamps,
            mesh_info=list(solution_database.values())[0].mesh_info,
            metadata={"source": "global_snapshot_matrix"}
        )
    
    async def predict_solution(self, new_parameters: Dict[str, float]) -> np.ndarray:
        """é¢„æµ‹æ–°å‚æ•°ä¸‹çš„è§£"""
        if not self.is_trained:
            raise ValueError("å‚æ•°åŒ–ROMæœªè®­ç»ƒ")
        
        # 1. å‚æ•°æ’å€¼è·å¾—PODç³»æ•°
        param_values = list(new_parameters.values())
        
        if SCIPY_AVAILABLE and hasattr(self.parameter_interpolator, 'predict'):
            pod_coefficients = self.parameter_interpolator.predict([param_values])[0]
        else:
            # ç®€åŒ–é¢„æµ‹ï¼ˆæœ€è¿‘é‚»ï¼‰
            points = self.parameter_interpolator['points']
            values = self.parameter_interpolator['values']
            
            # æ‰¾åˆ°æœ€è¿‘çš„è®­ç»ƒç‚¹
            distances = np.linalg.norm(points - np.array(param_values), axis=1)
            nearest_idx = np.argmin(distances)
            pod_coefficients = values[nearest_idx]
        
        # 2. é‡æ„å…¨ç»´è§£
        full_solution = self.pod_processor.reconstruct_from_pod_space(pod_coefficients)
        
        return full_solution
    
    def save_model(self, filepath: str):
        """ä¿å­˜ROMæ¨¡å‹"""
        model_data = {
            'config': self.config,
            'pod_result': self.pod_processor.pod_result,
            'dmd_result': self.dmd_processor.dmd_result,
            'parameter_interpolator': self.parameter_interpolator,
            'is_trained': self.is_trained,
            'performance_metrics': {
                'speedup_factor': self.speedup_factor,
                'accuracy_percentage': self.accuracy_percentage,
                'compression_ratio': self.pod_processor.compression_ratio
            }
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        self.logger.info(f"ROMæ¨¡å‹å·²ä¿å­˜è‡³: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½ROMæ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.config = model_data['config']
        self.pod_processor.pod_result = model_data['pod_result']
        self.dmd_processor.dmd_result = model_data['dmd_result']
        self.parameter_interpolator = model_data['parameter_interpolator']
        self.is_trained = model_data['is_trained']
        
        metrics = model_data['performance_metrics']
        self.speedup_factor = metrics['speedup_factor']
        self.accuracy_percentage = metrics['accuracy_percentage']
        self.pod_processor.compression_ratio = metrics['compression_ratio']
        
        self.logger.info(f"ROMæ¨¡å‹å·²ä» {filepath} åŠ è½½")

# ROMå·¥å‚ç±»
class ROMFactory:
    """ROMç³»ç»Ÿå·¥å‚"""
    
    @staticmethod
    def create_deep_excavation_rom(excavation_params: Dict) -> ParametricROM:
        """åˆ›å»ºæ·±åŸºå‘å·¥ç¨‹ROM"""
        config = ROMConfiguration(
            energy_threshold=0.995,  # æ·±åŸºå‘éœ€è¦é«˜ç²¾åº¦
            max_modes=50,           # é€‚ä¸­çš„æ¨¡æ€æ•°
            min_modes=5,
            offline_samples=500,    # è¶³å¤Ÿçš„è®­ç»ƒæ ·æœ¬
            use_gpu=True,
            parallel_workers=4
        )
        
        # è®¾ç½®å‚æ•°èŒƒå›´
        config.parameter_ranges = {
            'excavation_depth': (5.0, 15.0),      # åŸºå‘æ·±åº¦ (m)
            'soil_modulus': (20e6, 50e6),         # åœŸä½“æ¨¡é‡ (Pa)
            'support_stiffness': (1e7, 1e9),     # æ”¯æŠ¤åˆšåº¦ (N/m)
            'groundwater_level': (-2.0, -8.0)    # åœ°ä¸‹æ°´ä½ (m)
        }
        
        return ParametricROM(config)
    
    @staticmethod
    def create_structural_rom(structure_params: Dict) -> ParametricROM:
        """åˆ›å»ºç»“æ„åˆ†æROM"""
        config = ROMConfiguration(
            energy_threshold=0.99,
            max_modes=100,
            min_modes=10,
            offline_samples=1000,
            use_gpu=True,
            parallel_workers=6
        )
        
        config.parameter_ranges = {
            'load_magnitude': (1000, 10000),     # è½½è·å¤§å° (N)
            'material_stiffness': (1e9, 1e11),  # ææ–™åˆšåº¦ (Pa)
            'geometry_scale': (0.5, 2.0)        # å‡ ä½•ç¼©æ”¾
        }
        
        return ParametricROM(config)

if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    async def test_rom_system():
        """ROMç³»ç»Ÿæµ‹è¯•"""
        print("ğŸ”§ DeepCAD ROMé™é˜¶æ¨¡å‹ç³»ç»Ÿæµ‹è¯• ğŸ”§")
        
        # åˆ›å»ºæ·±åŸºå‘ROM
        excavation_params = {
            'depth': 8.0,
            'width': 20.0,
            'soil_type': 'clay'
        }
        
        rom_system = ROMFactory.create_deep_excavation_rom(excavation_params)
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        parameter_samples = {}
        solution_database = {}
        
        for i in range(10):  # 10ä¸ªå‚æ•°æ ·æœ¬
            param_id = f"sample_{i}"
            
            # éšæœºå‚æ•°
            parameters = {
                'excavation_depth': 5 + i * 1.0,
                'soil_modulus': 20e6 + i * 3e6,
                'support_stiffness': 1e7 * (i + 1),
                'groundwater_level': -2 - i * 0.6
            }
            
            parameter_samples[param_id] = list(parameters.values())
            
            # æ¨¡æ‹Ÿè§£å¿«ç…§
            n_dofs = 1000
            n_time_steps = 20
            solutions = np.random.randn(n_dofs, n_time_steps) * 0.01
            time_stamps = np.linspace(0, 10, n_time_steps)
            
            snapshot_data = SnapshotData(
                solutions=solutions,
                parameters=np.array(list(parameters.values())),
                time_stamps=time_stamps,
                mesh_info={'n_nodes': n_dofs//3, 'n_elements': n_dofs//9}
            )
            
            solution_database[param_id] = snapshot_data
        
        # è®­ç»ƒROM
        training_result = await rom_system.train_parametric_rom(
            parameter_samples, solution_database
        )
        
        print(f"\nğŸ“Š ROMè®­ç»ƒç»“æœ:")
        print(f"  å‹ç¼©æ¯”: {training_result['compression_ratio']:.1f}x")
        print(f"  åŠ é€Ÿæ¯”: {training_result['speedup_factor']:.1f}x")
        print(f"  ç²¾åº¦: {training_result['accuracy_percentage']:.1f}%")
        print(f"  è®­ç»ƒæ—¶é—´: {training_result['total_training_time']:.2f}ç§’")
        
        # æµ‹è¯•é¢„æµ‹
        new_params = {
            'excavation_depth': 7.5,
            'soil_modulus': 35e6,
            'support_stiffness': 5e7,
            'groundwater_level': -4.0
        }
        
        predicted_solution = await rom_system.predict_solution(new_params)
        print(f"\nğŸ¯ é¢„æµ‹è§£ç»´åº¦: {predicted_solution.shape}")
        print(f"  è§£çš„èŒƒæ•°: {np.linalg.norm(predicted_solution):.6f}")
        
        # ä¿å­˜æ¨¡å‹
        rom_system.save_model("deep_excavation_rom.pkl")
        print("\nâœ… ROMç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_rom_system())