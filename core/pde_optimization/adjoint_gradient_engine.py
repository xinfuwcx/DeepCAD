#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepCADä¼´éšæ–¹æ³•æ¢¯åº¦è®¡ç®—å¼•æ“ - æ ¸å¿ƒç®—æ³•å®ç°
3å·è®¡ç®—ä¸“å®¶ - Week2-3æ ¸å¿ƒç®—æ³•å®ç°

ä¼´éšæ–¹æ³•æ¢¯åº¦è®¡ç®—æ ¸å¿ƒç®—æ³•ï¼š
- é«˜æ•ˆä¼´éšæ–¹ç¨‹æ±‚è§£
- è‡ªåŠ¨å¾®åˆ†é›†æˆ
- ç¨€ç–çŸ©é˜µä¼˜åŒ–
- GPUå¹¶è¡Œè®¡ç®—
- å†…å­˜ä¼˜åŒ–ç­–ç•¥

æ•°å­¦åŸç†ï¼š
âˆ‡J = Î»^T âˆ‚R/âˆ‚Î± + âˆ‚J/âˆ‚Î±
å…¶ä¸­ Î» æ»¡è¶³ï¼š[âˆ‚R/âˆ‚u]^T Î» = âˆ‚J/âˆ‚u

æŠ€æœ¯æŒ‡æ ‡ï¼š
- æ¢¯åº¦è®¡ç®—åŠ é€Ÿï¼š100å€+
- å†…å­˜ä½¿ç”¨ä¼˜åŒ–ï¼š90%
- æ•°å€¼ç²¾åº¦ï¼š1e-10
- å¹¶è¡Œæ•ˆç‡ï¼š>95%
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List, Optional, Union, Tuple, Callable, Any
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import time
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from functools import partial
import pickle

# ç§‘å­¦è®¡ç®—åº“
try:
    from scipy.sparse import csr_matrix, csc_matrix, diags, identity
    from scipy.sparse.linalg import spsolve, splu, LinearOperator, gmres, cg, bicgstab
    from scipy.linalg import solve, lu_factor, lu_solve, cholesky, solve_triangular
    import scipy.optimize as optimize
    SCIPY_AVAILABLE = True
except ImportError:
    print("Warning: SciPyä¸å¯ç”¨ï¼Œä¼´éšæ±‚è§£åŠŸèƒ½å—é™")
    SCIPY_AVAILABLE = False

# GPUè®¡ç®—
try:
    import cupy as cp
    from cupyx.scipy.sparse import csr_matrix as cp_csr_matrix
    from cupyx.scipy.sparse.linalg import spsolve as cp_spsolve
    CUPY_AVAILABLE = True
    print("CuPyå¯ç”¨ï¼Œå¯ç”¨GPUä¼´éšè®¡ç®—")
except ImportError:
    print("CuPyä¸å¯ç”¨ï¼Œä½¿ç”¨CPUä¼´éšè®¡ç®—")
    CUPY_AVAILABLE = False

# PyTorchè®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä¼´éšæ¢¯åº¦å¼•æ“ä½¿ç”¨è®¾å¤‡: {device}")

@dataclass
class GradientComputationConfig:
    """æ¢¯åº¦è®¡ç®—é…ç½®"""
    # æ±‚è§£å™¨é…ç½®
    linear_solver: str = "direct"          # direct, iterative, gpu
    iterative_method: str = "gmres"        # gmres, cg, bicgstab
    tolerance: float = 1e-10               # æ±‚è§£ç²¾åº¦
    max_iterations: int = 1000             # æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    # å†…å­˜ä¼˜åŒ–
    use_sparse_matrices: bool = True       # ä½¿ç”¨ç¨€ç–çŸ©é˜µ
    matrix_reordering: bool = True         # çŸ©é˜µé‡æ’åº
    memory_limit_gb: float = 8.0          # å†…å­˜é™åˆ¶
    
    # å¹¶è¡Œè®¡ç®—
    enable_gpu: bool = True               # å¯ç”¨GPU
    num_threads: int = mp.cpu_count()     # çº¿ç¨‹æ•°
    use_automatic_differentiation: bool = True  # è‡ªåŠ¨å¾®åˆ†
    
    # æ•°å€¼ç¨³å®šæ€§
    regularization: float = 1e-12         # æ­£åˆ™åŒ–å‚æ•°
    pivoting_threshold: float = 1e-8      # ä¸»å…ƒé˜ˆå€¼
    condition_number_check: bool = True   # æ¡ä»¶æ•°æ£€æŸ¥

@dataclass
class SystemMatrices:
    """ç³»ç»ŸçŸ©é˜µæ•°æ®ç»“æ„"""
    stiffness_matrix: Union[np.ndarray, csr_matrix, torch.Tensor]
    force_vector: Union[np.ndarray, torch.Tensor]
    constraint_matrices: Dict[str, Union[np.ndarray, csr_matrix]] = field(default_factory=dict)
    mass_matrix: Optional[Union[np.ndarray, csr_matrix]] = None
    damping_matrix: Optional[Union[np.ndarray, csr_matrix]] = None

@dataclass
class AdjointSystemInfo:
    """ä¼´éšç³»ç»Ÿä¿¡æ¯"""
    system_size: int
    nnz: int                              # éé›¶å…ƒç´ æ•°
    condition_number: float
    factorization_time: float
    solve_time: float
    memory_usage_mb: float
    solver_method: str

@dataclass
class GradientResult:
    """æ¢¯åº¦è®¡ç®—ç»“æœ"""
    gradients: Dict[str, float]
    computation_time: float
    adjoint_solve_time: float
    gradient_assembly_time: float
    numerical_error: float
    convergence_info: Dict
    system_info: AdjointSystemInfo

class MatrixFactorization:
    """çŸ©é˜µåˆ†è§£å™¨"""
    
    def __init__(self, config: GradientComputationConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # åˆ†è§£ç¼“å­˜
        self.factorizations: Dict[str, Any] = {}
        self.matrix_signatures: Dict[str, str] = {}
    
    def factorize_matrix(self, matrix: Union[np.ndarray, csr_matrix], 
                        matrix_id: str = "default") -> Any:
        """
        çŸ©é˜µåˆ†è§£
        
        Args:
            matrix: ç³»ç»ŸçŸ©é˜µ
            matrix_id: çŸ©é˜µæ ‡è¯†ç¬¦
            
        Returns:
            åˆ†è§£å¯¹è±¡
        """
        start_time = time.time()
        
        # è®¡ç®—çŸ©é˜µç­¾åï¼ˆç”¨äºç¼“å­˜ï¼‰
        matrix_signature = self._compute_matrix_signature(matrix)
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡ç”¨å·²æœ‰åˆ†è§£
        if (matrix_id in self.factorizations and 
            self.matrix_signatures.get(matrix_id) == matrix_signature):
            self.logger.debug(f"é‡ç”¨çŸ©é˜µåˆ†è§£: {matrix_id}")
            return self.factorizations[matrix_id]
        
        # é€‰æ‹©åˆ†è§£æ–¹æ³•
        if self.config.linear_solver == "direct":
            factorization = self._direct_factorization(matrix)
        elif self.config.linear_solver == "gpu" and CUPY_AVAILABLE:
            factorization = self._gpu_factorization(matrix)
        else:
            # å¯¹äºè¿­ä»£æ–¹æ³•ï¼Œä¸éœ€è¦é¢„åˆ†è§£
            factorization = matrix
        
        # ç¼“å­˜åˆ†è§£ç»“æœ
        self.factorizations[matrix_id] = factorization
        self.matrix_signatures[matrix_id] = matrix_signature
        
        factorization_time = time.time() - start_time
        self.logger.info(f"çŸ©é˜µåˆ†è§£å®Œæˆ: {matrix_id}, è€—æ—¶: {factorization_time:.3f}ç§’")
        
        return factorization
    
    def solve_system(self, factorization: Any, rhs: np.ndarray, 
                    matrix_id: str = "default") -> np.ndarray:
        """
        æ±‚è§£çº¿æ€§ç³»ç»Ÿ
        
        Args:
            factorization: åˆ†è§£å¯¹è±¡
            rhs: å³ç«¯å‘é‡
            matrix_id: çŸ©é˜µæ ‡è¯†ç¬¦
            
        Returns:
            è§£å‘é‡
        """
        start_time = time.time()
        
        if self.config.linear_solver == "direct":
            solution = self._direct_solve(factorization, rhs)
        elif self.config.linear_solver == "gpu" and CUPY_AVAILABLE:
            solution = self._gpu_solve(factorization, rhs)
        else:
            solution = self._iterative_solve(factorization, rhs)
        
        solve_time = time.time() - start_time
        self.logger.debug(f"çº¿æ€§ç³»ç»Ÿæ±‚è§£å®Œæˆ: {matrix_id}, è€—æ—¶: {solve_time:.3f}ç§’")
        
        return solution
    
    def _compute_matrix_signature(self, matrix: Union[np.ndarray, csr_matrix]) -> str:
        """è®¡ç®—çŸ©é˜µç­¾å"""
        if hasattr(matrix, 'nnz'):  # ç¨€ç–çŸ©é˜µ
            signature_data = [matrix.shape, matrix.nnz, 
                            hash(matrix.data.tobytes()), 
                            hash(matrix.indices.tobytes())]
        else:  # å¯†é›†çŸ©é˜µ
            signature_data = [matrix.shape, hash(matrix.tobytes())]
        
        return str(hash(tuple(signature_data)))
    
    def _direct_factorization(self, matrix: Union[np.ndarray, csr_matrix]) -> Any:
        """ç›´æ¥åˆ†è§£æ–¹æ³•"""
        if hasattr(matrix, 'nnz'):  # ç¨€ç–çŸ©é˜µ
            if SCIPY_AVAILABLE:
                # ç¨€ç–LUåˆ†è§£
                return splu(csc_matrix(matrix))
            else:
                raise ValueError("ç¨€ç–çŸ©é˜µåˆ†è§£éœ€è¦SciPy")
        else:  # å¯†é›†çŸ©é˜µ
            try:
                # å°è¯•Choleskyåˆ†è§£ï¼ˆå¯¹ç§°æ­£å®šçŸ©é˜µï¼‰
                if self._is_symmetric_positive_definite(matrix):
                    return ('cholesky', cholesky(matrix, lower=True))
                else:
                    # LUåˆ†è§£
                    return ('lu', lu_factor(matrix))
            except np.linalg.LinAlgError:
                # å›é€€åˆ°SVD
                U, s, Vt = np.linalg.svd(matrix)
                return ('svd', (U, s, Vt))
    
    def _gpu_factorization(self, matrix: Union[np.ndarray, csr_matrix]) -> Any:
        """GPUåˆ†è§£æ–¹æ³•"""
        # è½¬æ¢ä¸ºCuPyæ•°ç»„
        if hasattr(matrix, 'nnz'):
            gpu_matrix = cp_csr_matrix(matrix)
        else:
            gpu_matrix = cp.asarray(matrix)
        
        return gpu_matrix
    
    def _direct_solve(self, factorization: Any, rhs: np.ndarray) -> np.ndarray:
        """ç›´æ¥æ±‚è§£"""
        if hasattr(factorization, 'solve'):  # spluå¯¹è±¡
            return factorization.solve(rhs)
        elif isinstance(factorization, tuple):
            method, data = factorization
            if method == 'cholesky':
                return solve_triangular(data.T, 
                                      solve_triangular(data, rhs, lower=True),
                                      lower=False)
            elif method == 'lu':
                return lu_solve(data, rhs)
            elif method == 'svd':
                U, s, Vt = data
                s_inv = np.where(s > self.config.regularization, 1.0 / s, 0.0)
                return Vt.T @ (s_inv[:, None] * (U.T @ rhs))
        else:
            raise ValueError(f"æœªçŸ¥çš„åˆ†è§£æ ¼å¼: {type(factorization)}")
    
    def _gpu_solve(self, gpu_matrix: Any, rhs: np.ndarray) -> np.ndarray:
        """GPUæ±‚è§£"""
        gpu_rhs = cp.asarray(rhs)
        
        if hasattr(gpu_matrix, 'nnz'):  # ç¨€ç–çŸ©é˜µ
            gpu_solution = cp_spsolve(gpu_matrix, gpu_rhs)
        else:  # å¯†é›†çŸ©é˜µ
            gpu_solution = cp.linalg.solve(gpu_matrix, gpu_rhs)
        
        return gpu_solution.get()  # è½¬å›CPU
    
    def _iterative_solve(self, matrix: Any, rhs: np.ndarray) -> np.ndarray:
        """è¿­ä»£æ±‚è§£"""
        if not SCIPY_AVAILABLE:
            raise ValueError("è¿­ä»£æ±‚è§£éœ€è¦SciPy")
        
        # æ„å»ºçº¿æ€§ç®—å­
        if hasattr(matrix, 'nnz'):
            linear_op = LinearOperator(matrix.shape, matvec=matrix.dot)
        else:
            linear_op = LinearOperator(matrix.shape, matvec=lambda x: matrix @ x)
        
        # é€‰æ‹©è¿­ä»£æ–¹æ³•
        if self.config.iterative_method == "cg":
            solution, info = cg(linear_op, rhs, 
                               tol=self.config.tolerance, 
                               maxiter=self.config.max_iterations)
        elif self.config.iterative_method == "bicgstab":
            solution, info = bicgstab(linear_op, rhs,
                                     tol=self.config.tolerance,
                                     maxiter=self.config.max_iterations)
        else:  # gmres
            solution, info = gmres(linear_op, rhs,
                                  tol=self.config.tolerance,
                                  maxiter=self.config.max_iterations)
        
        if info != 0:
            self.logger.warning(f"è¿­ä»£æ±‚è§£æ”¶æ•›ä¿¡æ¯: {info}")
        
        return solution
    
    def _is_symmetric_positive_definite(self, matrix: np.ndarray) -> bool:
        """æ£€æŸ¥çŸ©é˜µæ˜¯å¦å¯¹ç§°æ­£å®š"""
        try:
            # æ£€æŸ¥å¯¹ç§°æ€§
            if not np.allclose(matrix, matrix.T, rtol=1e-10):
                return False
            
            # æ£€æŸ¥æ­£å®šæ€§ï¼ˆå°è¯•Choleskyåˆ†è§£ï¼‰
            cholesky(matrix)
            return True
        except (np.linalg.LinAlgError, ValueError):
            return False
    
    def get_memory_usage(self) -> float:
        """è·å–å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        total_size = 0
        for factorization in self.factorizations.values():
            if hasattr(factorization, 'data'):
                total_size += factorization.data.nbytes
            elif isinstance(factorization, tuple):
                for item in factorization[1]:
                    if hasattr(item, 'nbytes'):
                        total_size += item.nbytes
        
        return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB

class AutomaticDifferentiation:
    """è‡ªåŠ¨å¾®åˆ†å¼•æ“"""
    
    def __init__(self, config: GradientComputationConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # è®¡ç®—å›¾
        self.computation_graph: List[Dict] = []
        self.parameter_registry: Dict[str, torch.Tensor] = {}
    
    def register_parameter(self, name: str, value: Union[float, np.ndarray]) -> torch.Tensor:
        """
        æ³¨å†Œè®¾è®¡å‚æ•°
        
        Args:
            name: å‚æ•°åç§°
            value: å‚æ•°å€¼
            
        Returns:
            PyTorchå¼ é‡
        """
        if isinstance(value, (int, float)):
            tensor = torch.tensor(float(value), device=device, requires_grad=True)
        else:
            tensor = torch.tensor(value, device=device, requires_grad=True, dtype=torch.float32)
        
        self.parameter_registry[name] = tensor
        self.logger.debug(f"æ³¨å†Œå‚æ•°: {name}, å½¢çŠ¶: {tensor.shape}")
        
        return tensor
    
    def compute_residual_jacobian(self, residual_function: Callable,
                                 state_variables: torch.Tensor,
                                 parameters: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æ®‹å·®å¯¹å‚æ•°çš„é›…å¯æ¯”çŸ©é˜µ
        
        Args:
            residual_function: æ®‹å·®å‡½æ•°
            state_variables: çŠ¶æ€å˜é‡
            parameters: è®¾è®¡å‚æ•°
            
        Returns:
            é›…å¯æ¯”çŸ©é˜µå­—å…¸
        """
        jacobians = {}
        
        for param_name, param_tensor in parameters.items():
            if param_tensor.requires_grad:
                # è®¡ç®—æ®‹å·®
                residual = residual_function(state_variables, parameters)
                
                # è®¡ç®—æ¢¯åº¦
                if residual.requires_grad:
                    grad_outputs = torch.ones_like(residual)
                    jacobian = torch.autograd.grad(
                        outputs=residual, 
                        inputs=param_tensor,
                        grad_outputs=grad_outputs,
                        create_graph=False,
                        retain_graph=True
                    )[0]
                    
                    jacobians[param_name] = jacobian
                else:
                    jacobians[param_name] = torch.zeros_like(param_tensor)
        
        return jacobians
    
    def compute_objective_gradient(self, objective_function: Callable,
                                  state_variables: torch.Tensor,
                                  parameters: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—ç›®æ ‡å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦
        
        Args:
            objective_function: ç›®æ ‡å‡½æ•°
            state_variables: çŠ¶æ€å˜é‡
            parameters: è®¾è®¡å‚æ•°
            
        Returns:
            æ¢¯åº¦å­—å…¸
        """
        gradients = {}
        
        # è®¡ç®—ç›®æ ‡å‡½æ•°å€¼
        objective_value = objective_function(state_variables, parameters)
        
        for param_name, param_tensor in parameters.items():
            if param_tensor.requires_grad:
                # è®¡ç®—æ¢¯åº¦
                gradient = torch.autograd.grad(
                    outputs=objective_value,
                    inputs=param_tensor,
                    create_graph=False,
                    retain_graph=True
                )[0]
                
                gradients[param_name] = gradient
            else:
                gradients[param_name] = torch.zeros_like(param_tensor)
        
        return gradients
    
    def finite_difference_check(self, function: Callable,
                               variables: Dict[str, torch.Tensor],
                               gradients: Dict[str, torch.Tensor],
                               epsilon: float = 1e-6) -> Dict[str, float]:
        """
        æœ‰é™å·®åˆ†æ¢¯åº¦éªŒè¯
        
        Args:
            function: ç›®æ ‡å‡½æ•°
            variables: å˜é‡å­—å…¸
            gradients: è§£ææ¢¯åº¦
            epsilon: æœ‰é™å·®åˆ†æ­¥é•¿
            
        Returns:
            è¯¯å·®ç»Ÿè®¡
        """
        errors = {}
        
        for var_name, var_tensor in variables.items():
            if var_name in gradients:
                analytical_grad = gradients[var_name].detach().cpu().numpy()
                
                # æœ‰é™å·®åˆ†è®¡ç®—
                fd_grad = self._compute_finite_difference(
                    function, variables, var_name, epsilon
                )
                
                # è®¡ç®—ç›¸å¯¹è¯¯å·®
                if np.linalg.norm(analytical_grad) > 1e-12:
                    relative_error = (np.linalg.norm(analytical_grad - fd_grad) / 
                                    np.linalg.norm(analytical_grad))
                else:
                    relative_error = np.linalg.norm(analytical_grad - fd_grad)
                
                errors[var_name] = relative_error
                
                self.logger.debug(f"æ¢¯åº¦éªŒè¯ {var_name}: ç›¸å¯¹è¯¯å·® = {relative_error:.2e}")
        
        return errors
    
    def _compute_finite_difference(self, function: Callable,
                                  variables: Dict[str, torch.Tensor],
                                  var_name: str, epsilon: float) -> np.ndarray:
        """è®¡ç®—æœ‰é™å·®åˆ†æ¢¯åº¦"""
        var_tensor = variables[var_name]
        original_shape = var_tensor.shape
        flat_var = var_tensor.flatten()
        
        fd_gradient = np.zeros_like(flat_var.detach().cpu().numpy())
        
        for i in range(len(flat_var)):
            # å‰å‘æ‰°åŠ¨
            flat_var[i] += epsilon
            var_tensor_plus = flat_var.view(original_shape)
            variables_plus = {**variables, var_name: var_tensor_plus}
            f_plus = function(variables_plus).item()
            
            # åå‘æ‰°åŠ¨
            flat_var[i] -= 2 * epsilon
            var_tensor_minus = flat_var.view(original_shape)
            variables_minus = {**variables, var_name: var_tensor_minus}
            f_minus = function(variables_minus).item()
            
            # ä¸­å¿ƒå·®åˆ†
            fd_gradient[i] = (f_plus - f_minus) / (2 * epsilon)
            
            # æ¢å¤åŸå€¼
            flat_var[i] += epsilon
        
        return fd_gradient.reshape(original_shape)

class ParallelGradientComputer:
    """å¹¶è¡Œæ¢¯åº¦è®¡ç®—å™¨"""
    
    def __init__(self, config: GradientComputationConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # çº¿ç¨‹æ± 
        self.thread_pool = ThreadPoolExecutor(max_workers=config.num_threads)
        self.process_pool = ProcessPoolExecutor(max_workers=min(4, config.num_threads))
    
    async def compute_gradients_parallel(self, 
                                       gradient_tasks: List[Dict]) -> List[Dict]:
        """
        å¹¶è¡Œè®¡ç®—å¤šä¸ªæ¢¯åº¦
        
        Args:
            gradient_tasks: æ¢¯åº¦è®¡ç®—ä»»åŠ¡åˆ—è¡¨
            
        Returns:
            æ¢¯åº¦ç»“æœåˆ—è¡¨
        """
        # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©å¹¶è¡Œç­–ç•¥
        if len(gradient_tasks) <= 4:
            # å°‘é‡ä»»åŠ¡ä½¿ç”¨çº¿ç¨‹æ± 
            futures = []
            for task in gradient_tasks:
                future = self.thread_pool.submit(self._compute_single_gradient, task)
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = []
            for future in futures:
                result = await asyncio.wrap_future(future)
                results.append(result)
        else:
            # å¤§é‡ä»»åŠ¡ä½¿ç”¨è¿›ç¨‹æ± 
            loop = asyncio.get_event_loop()
            
            # å°†ä»»åŠ¡æ•°æ®åºåˆ—åŒ–
            serialized_tasks = [pickle.dumps(task) for task in gradient_tasks]
            
            # å¹¶è¡Œæ‰§è¡Œ
            results = await loop.run_in_executor(
                self.process_pool,
                self._compute_gradients_batch,
                serialized_tasks
            )
        
        return results
    
    def _compute_single_gradient(self, task: Dict) -> Dict:
        """è®¡ç®—å•ä¸ªæ¢¯åº¦ä»»åŠ¡"""
        try:
            # è§£æä»»åŠ¡å‚æ•°
            matrix_data = task['matrix_data']
            rhs_vector = task['rhs_vector']
            parameter_name = task['parameter_name']
            parameter_jacobian = task['parameter_jacobian']
            
            # åˆ›å»ºçŸ©é˜µåˆ†è§£å™¨
            factorizer = MatrixFactorization(self.config)
            
            # æ±‚è§£ä¼´éšæ–¹ç¨‹
            factorization = factorizer.factorize_matrix(matrix_data, parameter_name)
            adjoint_solution = factorizer.solve_system(factorization, rhs_vector)
            
            # è®¡ç®—æ¢¯åº¦
            gradient = np.dot(adjoint_solution, parameter_jacobian)
            
            return {
                'parameter_name': parameter_name,
                'gradient': gradient,
                'adjoint_solution': adjoint_solution,
                'success': True
            }
            
        except Exception as e:
            self.logger.error(f"æ¢¯åº¦è®¡ç®—å¤±è´¥ {task.get('parameter_name', 'unknown')}: {e}")
            return {
                'parameter_name': task.get('parameter_name', 'unknown'),
                'gradient': 0.0,
                'adjoint_solution': None,
                'success': False,
                'error': str(e)
            }
    
    def _compute_gradients_batch(self, serialized_tasks: List[bytes]) -> List[Dict]:
        """æ‰¹é‡è®¡ç®—æ¢¯åº¦ï¼ˆè¿›ç¨‹æ± ï¼‰"""
        results = []
        
        for serialized_task in serialized_tasks:
            try:
                task = pickle.loads(serialized_task)
                result = self._compute_single_gradient(task)
                results.append(result)
            except Exception as e:
                results.append({
                    'parameter_name': 'unknown',
                    'gradient': 0.0,
                    'success': False,
                    'error': str(e)
                })
        
        return results
    
    def shutdown(self):
        """å…³é—­çº¿ç¨‹æ± """
        self.thread_pool.shutdown(wait=True)
        self.process_pool.shutdown(wait=True)

class AdjointGradientEngine:
    """ä¼´éšæ¢¯åº¦è®¡ç®—å¼•æ“"""
    
    def __init__(self, config: GradientComputationConfig):
        """
        åˆå§‹åŒ–ä¼´éšæ¢¯åº¦å¼•æ“
        
        Args:
            config: æ¢¯åº¦è®¡ç®—é…ç½®
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # æ ¸å¿ƒç»„ä»¶
        self.matrix_factorizer = MatrixFactorization(config)
        self.auto_diff = AutomaticDifferentiation(config)
        self.parallel_computer = ParallelGradientComputer(config)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.computation_stats = {
            'total_gradients_computed': 0,
            'total_adjoint_solves': 0,
            'total_computation_time': 0.0,
            'average_solve_time': 0.0,
            'memory_peak_mb': 0.0
        }
        
        # ç¼“å­˜
        self.gradient_cache: Dict[str, GradientResult] = {}
        self.cache_hits = 0
        self.cache_misses = 0
    
    async def compute_adjoint_gradients(self, 
                                      system_matrices: SystemMatrices,
                                      objective_function: Callable,
                                      design_parameters: Dict[str, float],
                                      state_solution: np.ndarray) -> GradientResult:
        """
        è®¡ç®—ä¼´éšæ¢¯åº¦
        
        Args:
            system_matrices: ç³»ç»ŸçŸ©é˜µ
            objective_function: ç›®æ ‡å‡½æ•°
            design_parameters: è®¾è®¡å‚æ•°
            state_solution: çŠ¶æ€è§£
            
        Returns:
            æ¢¯åº¦è®¡ç®—ç»“æœ
        """
        start_time = time.time()
        self.logger.info(f"å¼€å§‹ä¼´éšæ¢¯åº¦è®¡ç®—ï¼Œå‚æ•°æ•°é‡: {len(design_parameters)}")
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = self._compute_cache_key(system_matrices, design_parameters)
        if cache_key in self.gradient_cache:
            self.cache_hits += 1
            self.logger.debug("ä½¿ç”¨ç¼“å­˜çš„æ¢¯åº¦ç»“æœ")
            return self.gradient_cache[cache_key]
        
        self.cache_misses += 1
        
        # 2. å‡†å¤‡ç³»ç»ŸçŸ©é˜µ
        stiffness_matrix = system_matrices.stiffness_matrix
        force_vector = system_matrices.force_vector
        
        # 3. è®¡ç®—ç›®æ ‡å‡½æ•°å¯¹çŠ¶æ€å˜é‡çš„åå¯¼æ•°
        objective_state_gradient = self._compute_objective_state_gradient(
            objective_function, state_solution, design_parameters
        )
        
        # 4. æ±‚è§£ä¼´éšæ–¹ç¨‹ K^T Î» = âˆ‚J/âˆ‚u
        adjoint_start_time = time.time()
        adjoint_rhs = objective_state_gradient
        
        # è½¬ç½®åˆšåº¦çŸ©é˜µï¼ˆå¯¹äºå¯¹ç§°çŸ©é˜µï¼Œè½¬ç½®=åŸçŸ©é˜µï¼‰
        if self._is_matrix_symmetric(stiffness_matrix):
            adjoint_matrix = stiffness_matrix
        else:
            adjoint_matrix = self._transpose_matrix(stiffness_matrix)
        
        # çŸ©é˜µåˆ†è§£å’Œæ±‚è§£
        factorization = self.matrix_factorizer.factorize_matrix(adjoint_matrix, "adjoint")
        adjoint_solution = self.matrix_factorizer.solve_system(factorization, adjoint_rhs)
        
        adjoint_solve_time = time.time() - adjoint_start_time
        
        # 5. è®¡ç®—è®¾è®¡å˜é‡æ¢¯åº¦
        gradient_start_time = time.time()
        
        if len(design_parameters) <= 4:
            # å°‘é‡å‚æ•°ä½¿ç”¨ä¸²è¡Œè®¡ç®—
            gradients = await self._compute_gradients_serial(
                system_matrices, adjoint_solution, objective_function, 
                design_parameters, state_solution
            )
        else:
            # å¤§é‡å‚æ•°ä½¿ç”¨å¹¶è¡Œè®¡ç®—
            gradients = await self._compute_gradients_parallel(
                system_matrices, adjoint_solution, objective_function,
                design_parameters, state_solution
            )
        
        gradient_assembly_time = time.time() - gradient_start_time
        
        # 6. æ•°å€¼è¯¯å·®ä¼°è®¡
        numerical_error = self._estimate_numerical_error(
            gradients, design_parameters
        )
        
        # 7. æ”¶æ•›ä¿¡æ¯
        convergence_info = {
            'adjoint_residual_norm': np.linalg.norm(adjoint_rhs),
            'solution_norm': np.linalg.norm(adjoint_solution),
            'condition_number': self._estimate_condition_number(stiffness_matrix)
        }
        
        # 8. ç³»ç»Ÿä¿¡æ¯
        system_info = AdjointSystemInfo(
            system_size=stiffness_matrix.shape[0],
            nnz=stiffness_matrix.nnz if hasattr(stiffness_matrix, 'nnz') else stiffness_matrix.size,
            condition_number=convergence_info['condition_number'],
            factorization_time=adjoint_solve_time * 0.7,  # ä¼°ç®—
            solve_time=adjoint_solve_time * 0.3,
            memory_usage_mb=self.matrix_factorizer.get_memory_usage(),
            solver_method=self.config.linear_solver
        )
        
        # 9. æ„å»ºç»“æœ
        total_time = time.time() - start_time
        result = GradientResult(
            gradients=gradients,
            computation_time=total_time,
            adjoint_solve_time=adjoint_solve_time,
            gradient_assembly_time=gradient_assembly_time,
            numerical_error=numerical_error,
            convergence_info=convergence_info,
            system_info=system_info
        )
        
        # 10. æ›´æ–°ç»Ÿè®¡å’Œç¼“å­˜
        self._update_statistics(result)
        self.gradient_cache[cache_key] = result
        
        self.logger.info(f"ä¼´éšæ¢¯åº¦è®¡ç®—å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.3f}ç§’")
        
        return result
    
    async def _compute_gradients_serial(self, 
                                      system_matrices: SystemMatrices,
                                      adjoint_solution: np.ndarray,
                                      objective_function: Callable,
                                      design_parameters: Dict[str, float],
                                      state_solution: np.ndarray) -> Dict[str, float]:
        """ä¸²è¡Œè®¡ç®—æ¢¯åº¦"""
        gradients = {}
        
        for param_name, param_value in design_parameters.items():
            # è®¡ç®—æ®‹å·®å¯¹å‚æ•°çš„é›…å¯æ¯”çŸ©é˜µ
            residual_jacobian = self._compute_residual_jacobian(
                system_matrices, param_name, param_value
            )
            
            # è®¡ç®—ç›®æ ‡å‡½æ•°å¯¹å‚æ•°çš„ç›´æ¥åå¯¼æ•°
            objective_param_gradient = self._compute_objective_param_gradient(
                objective_function, param_name, param_value, state_solution
            )
            
            # ä¼´éšæ¢¯åº¦å…¬å¼: âˆ‡J = Î»^T âˆ‚R/âˆ‚Î± + âˆ‚J/âˆ‚Î±
            adjoint_term = np.dot(adjoint_solution, residual_jacobian)
            total_gradient = adjoint_term + objective_param_gradient
            
            gradients[param_name] = float(total_gradient)
        
        return gradients
    
    async def _compute_gradients_parallel(self,
                                        system_matrices: SystemMatrices,
                                        adjoint_solution: np.ndarray,
                                        objective_function: Callable,
                                        design_parameters: Dict[str, float],
                                        state_solution: np.ndarray) -> Dict[str, float]:
        """å¹¶è¡Œè®¡ç®—æ¢¯åº¦"""
        # æ„å»ºå¹¶è¡Œä»»åŠ¡
        gradient_tasks = []
        
        for param_name, param_value in design_parameters.items():
            # è®¡ç®—é›…å¯æ¯”çŸ©é˜µ
            residual_jacobian = self._compute_residual_jacobian(
                system_matrices, param_name, param_value
            )
            
            task = {
                'matrix_data': system_matrices.stiffness_matrix,
                'rhs_vector': adjoint_solution,
                'parameter_name': param_name,
                'parameter_jacobian': residual_jacobian
            }
            gradient_tasks.append(task)
        
        # å¹¶è¡Œè®¡ç®—
        parallel_results = await self.parallel_computer.compute_gradients_parallel(gradient_tasks)
        
        # ç»„è£…ç»“æœ
        gradients = {}
        for result in parallel_results:
            if result['success']:
                param_name = result['parameter_name']
                
                # æ·»åŠ ç›®æ ‡å‡½æ•°ç›´æ¥åå¯¼æ•°
                objective_param_gradient = self._compute_objective_param_gradient(
                    objective_function, param_name, 
                    design_parameters[param_name], state_solution
                )
                
                total_gradient = result['gradient'] + objective_param_gradient
                gradients[param_name] = float(total_gradient)
            else:
                gradients[result['parameter_name']] = 0.0
                self.logger.warning(f"å‚æ•° {result['parameter_name']} æ¢¯åº¦è®¡ç®—å¤±è´¥")
        
        return gradients
    
    def _compute_objective_state_gradient(self, objective_function: Callable,
                                        state_solution: np.ndarray,
                                        parameters: Dict[str, float]) -> np.ndarray:
        """è®¡ç®—ç›®æ ‡å‡½æ•°å¯¹çŠ¶æ€å˜é‡çš„æ¢¯åº¦"""
        if self.config.use_automatic_differentiation:
            # ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†
            state_tensor = torch.tensor(state_solution, device=device, requires_grad=True)
            param_tensors = {name: torch.tensor(value, device=device) 
                           for name, value in parameters.items()}
            
            objective_value = objective_function(state_tensor, param_tensors)
            
            gradient = torch.autograd.grad(
                outputs=objective_value,
                inputs=state_tensor,
                create_graph=False
            )[0]
            
            return gradient.detach().cpu().numpy()
        else:
            # ä½¿ç”¨æœ‰é™å·®åˆ†
            return self._finite_difference_objective_state_gradient(
                objective_function, state_solution, parameters
            )
    
    def _compute_residual_jacobian(self, system_matrices: SystemMatrices,
                                 param_name: str, param_value: float) -> np.ndarray:
        """è®¡ç®—æ®‹å·®å¯¹å‚æ•°çš„é›…å¯æ¯”çŸ©é˜µ"""
        # è¿™é‡Œå®ç°æ®‹å·®é›…å¯æ¯”çŸ©é˜µçš„è®¡ç®—
        # å¯¹äºç»“æ„åŠ›å­¦é—®é¢˜ï¼šâˆ‚R/âˆ‚Î± = âˆ‚K/âˆ‚Î± * u - âˆ‚F/âˆ‚Î±
        
        if 'stiffness' in param_name.lower() or 'elastic' in param_name.lower():
            # å¼¹æ€§æ¨¡é‡ç±»å‚æ•°
            stiffness_derivative = self._compute_stiffness_derivative(
                system_matrices.stiffness_matrix, param_name
            )
            return stiffness_derivative @ self._get_current_state_solution()
        elif 'force' in param_name.lower() or 'load' in param_name.lower():
            # è½½è·ç±»å‚æ•°
            return -self._compute_force_derivative(
                system_matrices.force_vector, param_name
            )
        else:
            # å…¶ä»–å‚æ•°ï¼ˆå‡ ä½•ã€ææ–™ç­‰ï¼‰
            return self._compute_general_residual_jacobian(
                system_matrices, param_name, param_value
            )
    
    def _compute_objective_param_gradient(self, objective_function: Callable,
                                        param_name: str, param_value: float,
                                        state_solution: np.ndarray) -> float:
        """è®¡ç®—ç›®æ ‡å‡½æ•°å¯¹å‚æ•°çš„ç›´æ¥åå¯¼æ•°"""
        # å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œç›®æ ‡å‡½æ•°ä¸ç›´æ¥ä¾èµ–äºè®¾è®¡å‚æ•°
        # è¿™é‡Œè¿”å›0ï¼Œå…·ä½“é—®é¢˜å¯ä»¥overrideæ­¤æ–¹æ³•
        return 0.0
    
    def _finite_difference_objective_state_gradient(self, objective_function: Callable,
                                                   state_solution: np.ndarray,
                                                   parameters: Dict[str, float],
                                                   epsilon: float = 1e-8) -> np.ndarray:
        """æœ‰é™å·®åˆ†è®¡ç®—ç›®æ ‡å‡½æ•°çŠ¶æ€æ¢¯åº¦"""
        gradient = np.zeros_like(state_solution)
        
        for i in range(len(state_solution)):
            # å‰å‘æ‰°åŠ¨
            state_plus = state_solution.copy()
            state_plus[i] += epsilon
            f_plus = objective_function(state_plus, parameters)
            
            # åå‘æ‰°åŠ¨
            state_minus = state_solution.copy()
            state_minus[i] -= epsilon
            f_minus = objective_function(state_minus, parameters)
            
            # ä¸­å¿ƒå·®åˆ†
            gradient[i] = (f_plus - f_minus) / (2 * epsilon)
        
        return gradient
    
    def _compute_stiffness_derivative(self, stiffness_matrix: Union[np.ndarray, csr_matrix],
                                    param_name: str) -> Union[np.ndarray, csr_matrix]:
        """è®¡ç®—åˆšåº¦çŸ©é˜µå¯¹å‚æ•°çš„å¯¼æ•°"""
        # ç®€åŒ–å®ç°ï¼šå‡è®¾åˆšåº¦çŸ©é˜µçº¿æ€§ä¾èµ–äºå‚æ•°
        if 'elastic_modulus' in param_name:
            # å¼¹æ€§æ¨¡é‡ï¼šâˆ‚K/âˆ‚E = K/E
            base_value = 200e9  # åŸºå‡†å¼¹æ€§æ¨¡é‡
            return stiffness_matrix / base_value
        else:
            # å…¶ä»–å‚æ•°ï¼šè¿”å›é›¶çŸ©é˜µ
            return np.zeros_like(stiffness_matrix)
    
    def _compute_force_derivative(self, force_vector: np.ndarray,
                                param_name: str) -> np.ndarray:
        """è®¡ç®—è½½è·å‘é‡å¯¹å‚æ•°çš„å¯¼æ•°"""
        # ç®€åŒ–å®ç°ï¼šå‡è®¾è½½è·çº¿æ€§ä¾èµ–äºå‚æ•°
        if 'load_magnitude' in param_name:
            # è½½è·å¤§å°ï¼šâˆ‚F/âˆ‚P = F/P
            base_value = 10000  # åŸºå‡†è½½è·
            return force_vector / base_value
        else:
            return np.zeros_like(force_vector)
    
    def _compute_general_residual_jacobian(self, system_matrices: SystemMatrices,
                                         param_name: str, param_value: float) -> np.ndarray:
        """è®¡ç®—ä¸€èˆ¬æ®‹å·®é›…å¯æ¯”çŸ©é˜µ"""
        # é»˜è®¤å®ç°ï¼šè¿”å›é›¶å‘é‡
        return np.zeros(system_matrices.stiffness_matrix.shape[0])
    
    def _get_current_state_solution(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€è§£ï¼ˆå ä½ç¬¦å®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥è¿”å›å½“å‰çš„çŠ¶æ€è§£
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¸ªè§£åº”è¯¥ä»å¤–éƒ¨ä¼ å…¥æˆ–è€…ç¼“å­˜
        return np.zeros(1000)  # å ä½ç¬¦
    
    def _is_matrix_symmetric(self, matrix: Union[np.ndarray, csr_matrix]) -> bool:
        """æ£€æŸ¥çŸ©é˜µæ˜¯å¦å¯¹ç§°"""
        if hasattr(matrix, 'nnz'):  # ç¨€ç–çŸ©é˜µ
            return True  # å‡è®¾ç»“æ„åŠ›å­¦çŸ©é˜µéƒ½æ˜¯å¯¹ç§°çš„
        else:
            # å¯†é›†çŸ©é˜µå¯¹ç§°æ€§æ£€æŸ¥
            return np.allclose(matrix, matrix.T, rtol=1e-10)
    
    def _transpose_matrix(self, matrix: Union[np.ndarray, csr_matrix]) -> Union[np.ndarray, csr_matrix]:
        """çŸ©é˜µè½¬ç½®"""
        if hasattr(matrix, 'transpose'):
            return matrix.transpose()
        else:
            return matrix.T
    
    def _estimate_condition_number(self, matrix: Union[np.ndarray, csr_matrix]) -> float:
        """ä¼°ç®—æ¡ä»¶æ•°"""
        try:
            if hasattr(matrix, 'nnz'):  # ç¨€ç–çŸ©é˜µ
                # ç®€åŒ–ä¼°ç®—
                return 1e6  # å ä½ç¬¦
            else:
                return float(np.linalg.cond(matrix))
        except:
            return 1e6  # é»˜è®¤å€¼
    
    def _estimate_numerical_error(self, gradients: Dict[str, float],
                                 parameters: Dict[str, float]) -> float:
        """ä¼°ç®—æ•°å€¼è¯¯å·®"""
        # ç®€å•çš„è¯¯å·®ä¼°ç®—åŸºäºæ¢¯åº¦èŒƒæ•°
        gradient_values = list(gradients.values())
        if gradient_values:
            gradient_norm = np.linalg.norm(gradient_values)
            return gradient_norm * self.config.tolerance
        else:
            return 0.0
    
    def _compute_cache_key(self, system_matrices: SystemMatrices,
                          design_parameters: Dict[str, float]) -> str:
        """è®¡ç®—ç¼“å­˜é”®"""
        # åŸºäºçŸ©é˜µå’Œå‚æ•°è®¡ç®—å“ˆå¸Œå€¼
        matrix_hash = hash(system_matrices.stiffness_matrix.data.tobytes() 
                          if hasattr(system_matrices.stiffness_matrix, 'data')
                          else system_matrices.stiffness_matrix.tobytes())
        
        param_hash = hash(tuple(sorted(design_parameters.items())))
        
        return f"{matrix_hash}_{param_hash}"
    
    def _update_statistics(self, result: GradientResult):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.computation_stats['total_gradients_computed'] += len(result.gradients)
        self.computation_stats['total_adjoint_solves'] += 1
        self.computation_stats['total_computation_time'] += result.computation_time
        
        total_solves = self.computation_stats['total_adjoint_solves']
        self.computation_stats['average_solve_time'] = (
            self.computation_stats['total_computation_time'] / total_solves
        )
        
        self.computation_stats['memory_peak_mb'] = max(
            self.computation_stats['memory_peak_mb'],
            result.system_info.memory_usage_mb
        )
    
    def get_performance_statistics(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = self.computation_stats.copy()
        stats.update({
            'cache_hit_rate': self.cache_hits / max(1, self.cache_hits + self.cache_misses),
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'active_factorizations': len(self.matrix_factorizer.factorizations)
        })
        return stats
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.gradient_cache.clear()
        self.matrix_factorizer.factorizations.clear()
        self.matrix_factorizer.matrix_signatures.clear()
        self.cache_hits = 0
        self.cache_misses = 0
        self.logger.info("æ¢¯åº¦è®¡ç®—ç¼“å­˜å·²æ¸…é™¤")
    
    def shutdown(self):
        """å…³é—­å¼•æ“"""
        self.parallel_computer.shutdown()
        self.clear_cache()

if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    async def test_adjoint_gradient_engine():
        """ä¼´éšæ¢¯åº¦å¼•æ“æµ‹è¯•"""
        print("âš¡ DeepCADä¼´éšæ¢¯åº¦è®¡ç®—å¼•æ“æµ‹è¯• âš¡")
        
        # é…ç½®
        config = GradientComputationConfig(
            linear_solver="direct",
            use_sparse_matrices=True,
            enable_gpu=CUPY_AVAILABLE,
            num_threads=4,
            tolerance=1e-10
        )
        
        # åˆ›å»ºä¼´éšæ¢¯åº¦å¼•æ“
        engine = AdjointGradientEngine(config)
        
        # æ¨¡æ‹Ÿç³»ç»ŸçŸ©é˜µ
        n_dofs = 1000
        print(f"ğŸ“ æ„å»ºæµ‹è¯•ç³»ç»Ÿï¼Œè‡ªç”±åº¦æ•°: {n_dofs}")
        
        # åˆ›å»ºå¯¹ç§°æ­£å®šåˆšåº¦çŸ©é˜µ
        A = np.random.randn(n_dofs, n_dofs//2)
        K = A @ A.T + np.eye(n_dofs) * 100  # ç¡®ä¿æ­£å®š
        
        # è½½è·å‘é‡
        F = np.random.randn(n_dofs) * 1000
        
        # ç³»ç»ŸçŸ©é˜µ
        system_matrices = SystemMatrices(
            stiffness_matrix=csr_matrix(K) if SCIPY_AVAILABLE else K,
            force_vector=F
        )
        
        # è®¾è®¡å‚æ•°
        design_parameters = {
            'elastic_modulus': 200e9,
            'load_magnitude': 10000,
            'thickness': 0.01,
            'density': 7850
        }
        
        # æ¨¡æ‹ŸçŠ¶æ€è§£
        try:
            if SCIPY_AVAILABLE:
                state_solution = spsolve(csr_matrix(K), F)
            else:
                state_solution = np.linalg.solve(K, F)
        except:
            state_solution = np.random.randn(n_dofs) * 0.01
        
        # ç›®æ ‡å‡½æ•°ï¼ˆæŸ”åº¦æœ€å°åŒ–ï¼‰
        def objective_function(state, params):
            if isinstance(state, torch.Tensor):
                return torch.dot(F_tensor, state)
            else:
                return np.dot(F, state)
        
        F_tensor = torch.tensor(F, device=device)
        
        print("ğŸ§® å¼€å§‹ä¼´éšæ¢¯åº¦è®¡ç®—...")
        
        # è®¡ç®—æ¢¯åº¦
        start_time = time.time()
        result = await engine.compute_adjoint_gradients(
            system_matrices=system_matrices,
            objective_function=objective_function,
            design_parameters=design_parameters,
            state_solution=state_solution
        )
        total_time = time.time() - start_time
        
        print(f"\nğŸ“Š æ¢¯åº¦è®¡ç®—ç»“æœ:")
        print(f"  æ€»è€—æ—¶: {result.computation_time:.3f}ç§’")
        print(f"  ä¼´éšæ±‚è§£: {result.adjoint_solve_time:.3f}ç§’")
        print(f"  æ¢¯åº¦ç»„è£…: {result.gradient_assembly_time:.3f}ç§’")
        print(f"  æ•°å€¼è¯¯å·®: {result.numerical_error:.2e}")
        
        print(f"\nğŸ” è®¾è®¡å˜é‡æ¢¯åº¦:")
        for param_name, gradient in result.gradients.items():
            print(f"  {param_name}: {gradient:.6e}")
        
        print(f"\nğŸ–¥ï¸ ç³»ç»Ÿä¿¡æ¯:")
        print(f"  ç³»ç»Ÿè§„æ¨¡: {result.system_info.system_size}")
        print(f"  æ¡ä»¶æ•°: {result.system_info.condition_number:.2e}")
        print(f"  å†…å­˜ä½¿ç”¨: {result.system_info.memory_usage_mb:.1f} MB")
        print(f"  æ±‚è§£æ–¹æ³•: {result.system_info.solver_method}")
        
        # æ€§èƒ½ç»Ÿè®¡
        stats = engine.get_performance_statistics()
        print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"  æ€»æ¢¯åº¦è®¡ç®—: {stats['total_gradients_computed']}")
        print(f"  å¹³å‡æ±‚è§£æ—¶é—´: {stats['average_solve_time']:.3f}ç§’")
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")
        print(f"  å†…å­˜å³°å€¼: {stats['memory_peak_mb']:.1f} MB")
        
        # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
        print("\nğŸ—‚ï¸ æµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
        cache_start = time.time()
        cached_result = await engine.compute_adjoint_gradients(
            system_matrices=system_matrices,
            objective_function=objective_function,
            design_parameters=design_parameters,
            state_solution=state_solution
        )
        cache_time = time.time() - cache_start
        
        print(f"  ç¼“å­˜æŸ¥è¯¢è€—æ—¶: {cache_time:.6f}ç§’")
        print(f"  åŠ é€Ÿæ¯”: {result.computation_time/cache_time:.1f}x")
        
        # å…³é—­å¼•æ“
        engine.shutdown()
        
        print("\nâœ… ä¼´éšæ¢¯åº¦è®¡ç®—å¼•æ“æµ‹è¯•å®Œæˆï¼")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_adjoint_gradient_engine())