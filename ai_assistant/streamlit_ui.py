"""
DeepCAD AI Assistant Streamlit UI
@description ç‚«é…·çš„AIåŠ©æ‰‹å‰ç«¯ç•Œé¢
@author 1å·é¦–å¸­æ¶æ„å¸ˆ & AIç³»ç»Ÿæ¶æ„å¸ˆ
@version 1.0.0
"""

import streamlit as st
import asyncio
import json
import time
from datetime import datetime
from deepcad_ai_assistant import DeepCADAIAssistant, AIResponse
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ğŸ¤– DeepCAD AI Assistant",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        border-left: 4px solid #667eea;
    }
    
    .user-message {
        background-color: #f0f2f6;
        border-left-color: #ff6b6b;
    }
    
    .assistant-message {
        background-color: #e8f4f8;
        border-left-color: #4ecdc4;
    }
    
    .stats-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
    
    .intent-badge {
        background-color: #667eea;
        color: white;
        padding: 0.2em 0.6em;
        border-radius: 20px;
        font-size: 0.8em;
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–AIåŠ©æ‰‹
@st.cache_resource
def initialize_ai_assistant():
    """åˆå§‹åŒ–AIåŠ©æ‰‹ï¼ˆç¼“å­˜èµ„æºï¼‰"""
    return DeepCADAIAssistant()

def main():
    """ä¸»ç•Œé¢"""
    
    # ä¸»æ ‡é¢˜
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ¤– DeepCAD AI Assistant</h1>
        <p>Your Intelligent CAE Companion - Powered by Ollama</p>
    </div>
    """, unsafe_allow_html=True)
    
    # åˆå§‹åŒ–AIåŠ©æ‰‹
    try:
        ai_assistant = initialize_ai_assistant()
    except Exception as e:
        st.error(f"âŒ AIåŠ©æ‰‹åˆå§‹åŒ–å¤±è´¥: {e}")
        st.stop()
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("ğŸ› ï¸ æ§åˆ¶é¢æ¿")
        
        # æ¨¡å‹é€‰æ‹©
        available_models = ai_assistant.get_available_models()
        if available_models:
            selected_model = st.selectbox(
                "é€‰æ‹©AIæ¨¡å‹",
                available_models,
                index=0 if available_models else None
            )
            ai_assistant.set_model(selected_model)
        else:
            st.warning("âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°å¯ç”¨æ¨¡å‹")
        
        st.divider()
        
        # å¯¹è¯ç»Ÿè®¡
        st.header("ğŸ“Š å¯¹è¯ç»Ÿè®¡")
        stats = ai_assistant.get_conversation_stats()
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("æ€»å¯¹è¯æ•°", stats.get("total_queries", 0))
        with col2:
            st.metric("å½“å‰æ¨¡å‹", stats.get("latest_model", "æœªçŸ¥")[:10] + "...")
        
        # æ„å›¾åˆ†å¸ƒå›¾
        if stats.get("intent_distribution"):
            intent_df = pd.DataFrame(
                list(stats["intent_distribution"].items()),
                columns=["æ„å›¾", "æ¬¡æ•°"]
            )
            fig = px.pie(
                intent_df, 
                values="æ¬¡æ•°", 
                names="æ„å›¾",
                title="å¯¹è¯æ„å›¾åˆ†å¸ƒ"
            )
            fig.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig, use_container_width=True)
        
        st.divider()
        
        # CAEå·¥å…·å¿«é€Ÿè®¿é—®
        st.header("ğŸ”§ CAEå·¥å…·")
        if st.button("ğŸ—ï¸ Kratosè„šæœ¬ç”Ÿæˆ"):
            st.session_state.quick_prompt = "å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªåŸºæœ¬çš„Kratosæœ‰é™å…ƒæ±‚è§£è„šæœ¬"
        
        if st.button("ğŸ•¸ï¸ ç½‘æ ¼è´¨é‡æ£€æŸ¥"):
            st.session_state.quick_prompt = "å¦‚ä½•æ£€æŸ¥å’Œä¼˜åŒ–æœ‰é™å…ƒç½‘æ ¼è´¨é‡ï¼Ÿ"
        
        if st.button("ğŸ“Š ç»“æœå¯è§†åŒ–"):
            st.session_state.quick_prompt = "å¸®æˆ‘ç”ŸæˆPyVistaç»“æœå¯è§†åŒ–ä»£ç "
        
        if st.button("ğŸ” æ”¶æ•›æ€§è¯Šæ–­"):
            st.session_state.quick_prompt = "æˆ‘çš„CAEè®¡ç®—ä¸æ”¶æ•›ï¼Œå¦‚ä½•è¯Šæ–­é—®é¢˜ï¼Ÿ"
    
    # ä¸»èŠå¤©ç•Œé¢
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.header("ğŸ’¬ AIå¯¹è¯")
        
        # åˆå§‹åŒ–å¯¹è¯å†å²
        if "messages" not in st.session_state:
            st.session_state.messages = [
                {
                    "role": "assistant",
                    "content": "ä½ å¥½ï¼æˆ‘æ˜¯DeepCAD AIåŠ©æ‰‹ ğŸ¤–\n\næˆ‘å¯ä»¥å¸®åŠ©ä½ ï¼š\n- ğŸ”§ ç”ŸæˆCAEä»£ç \n- ğŸ“š è§£ç­”æœ‰é™å…ƒç†è®º\n- ğŸ•¸ï¸ ç½‘æ ¼åˆ’åˆ†å»ºè®®\n- ğŸ” é—®é¢˜è¯Šæ–­è§£å†³\n- ğŸ“Š æ•°æ®å¯è§†åŒ–\n\næœ‰ä»€ä¹ˆCAEé—®é¢˜å°½ç®¡é—®æˆ‘ï¼",
                    "intent": "greeting",
                    "timestamp": datetime.now(),
                    "confidence": 1.0,
                    "processing_time": 0.0
                }
            ]
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        for i, message in enumerate(st.session_state.messages):
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                
                # æ˜¾ç¤ºAIå“åº”çš„å…ƒä¿¡æ¯
                if message["role"] == "assistant" and i > 0:
                    col_a, col_b, col_c = st.columns(3)
                    with col_a:
                        st.caption(f"ğŸ¯ æ„å›¾: {message.get('intent', 'unknown')}")
                    with col_b:
                        st.caption(f"â±ï¸ è€—æ—¶: {message.get('processing_time', 0):.2f}s")
                    with col_c:
                        confidence = message.get('confidence', 0)
                        st.caption(f"ğŸª ç½®ä¿¡åº¦: {confidence:.1%}")
        
        # ç”¨æˆ·è¾“å…¥
        user_input = st.chat_input("è¾“å…¥ä½ çš„CAEé—®é¢˜...")
        
        # å¤„ç†å¿«é€Ÿæç¤º
        if "quick_prompt" in st.session_state:
            user_input = st.session_state.quick_prompt
            del st.session_state.quick_prompt
        
        # å¤„ç†ç”¨æˆ·è¾“å…¥
        if user_input:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            st.session_state.messages.append({
                "role": "user",
                "content": user_input,
                "timestamp": datetime.now()
            })
            
            # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            with st.chat_message("user"):
                st.markdown(user_input)
            
            # ç”ŸæˆAIå“åº”
            with st.chat_message("assistant"):
                with st.spinner("ğŸ¤– AIæ­£åœ¨æ€è€ƒä¸­..."):
                    try:
                        # è¿è¡Œå¼‚æ­¥å‡½æ•°
                        response = asyncio.run(ai_assistant.process_query(user_input))
                        
                        # æ˜¾ç¤ºå“åº”å†…å®¹
                        st.markdown(response.content)
                        
                        # æ˜¾ç¤ºå“åº”å…ƒä¿¡æ¯
                        col_a, col_b, col_c = st.columns(3)
                        with col_a:
                            st.caption(f"ğŸ¯ æ„å›¾: {response.intent}")
                        with col_b:
                            st.caption(f"â±ï¸ è€—æ—¶: {response.processing_time:.2f}s")
                        with col_c:
                            st.caption(f"ğŸª ç½®ä¿¡åº¦: {response.confidence:.1%}")
                        
                        # æ˜¾ç¤ºå»ºè®®
                        if response.suggestions:
                            st.info("ğŸ’¡ ç›¸å…³å»ºè®®ï¼š\n" + "\n".join([f"â€¢ {s}" for s in response.suggestions]))
                        
                        # æ·»åŠ AIå“åº”åˆ°å†å²
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": response.content,
                            "intent": response.intent,
                            "timestamp": datetime.now(),
                            "confidence": response.confidence,
                            "processing_time": response.processing_time,
                            "suggestions": response.suggestions
                        })
                        
                    except Exception as e:
                        st.error(f"âŒ AIå“åº”ç”Ÿæˆå¤±è´¥: {e}")
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": f"æŠ±æ­‰ï¼Œå¤„ç†ä½ çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}",
                            "intent": "error",
                            "timestamp": datetime.now(),
                            "confidence": 0.0,
                            "processing_time": 0.0
                        })
            
            # åˆ·æ–°é¡µé¢ä»¥æ˜¾ç¤ºæ–°æ¶ˆæ¯
            st.rerun()
    
    with col2:
        st.header("ğŸ¯ CAEä¸“ä¸šé¢†åŸŸ")
        
        # CAEä¸“ä¸šé¢†åŸŸæŒ‡å—
        with st.expander("ğŸ”§ ä»£ç ç”Ÿæˆ", expanded=False):
            st.write("""
            **æ”¯æŒçš„ä»£ç ç±»å‹ï¼š**
            - Kratos Pythonè„šæœ¬
            - GMSHç½‘æ ¼ç”Ÿæˆ
            - PyVistaå¯è§†åŒ–
            - NumPyæ•°å€¼è®¡ç®—
            - FEMç®—æ³•å®ç°
            """)
        
        with st.expander("ğŸ“š ç†è®ºå’¨è¯¢", expanded=False):
            st.write("""
            **æ¶µç›–çš„ç†è®ºé¢†åŸŸï¼š**
            - æœ‰é™å…ƒåŸºç¡€ç†è®º
            - å•å…ƒç±»å‹å’Œå½¢å‡½æ•°
            - æ•°å€¼ç§¯åˆ†æ–¹æ³•
            - éçº¿æ€§åˆ†æ
            - å¤šç‰©ç†åœºè€¦åˆ
            """)
        
        with st.expander("ğŸ•¸ï¸ ç½‘æ ¼å»ºè®®", expanded=False):
            st.write("""
            **ç½‘æ ¼ç›¸å…³æœåŠ¡ï¼š**
            - ç½‘æ ¼åˆ’åˆ†ç­–ç•¥
            - è´¨é‡è¯„ä¼°æ–¹æ³•
            - æ”¶æ•›æ€§åˆ†æ
            - è‡ªé€‚åº”ç»†åŒ–
            - ä¼˜åŒ–ç®—æ³•
            """)
        
        with st.expander("ğŸ” é—®é¢˜è¯Šæ–­", expanded=False):
            st.write("""
            **å¸¸è§é—®é¢˜ç±»å‹ï¼š**
            - æ”¶æ•›æ€§é—®é¢˜
            - æ•°å€¼ä¸ç¨³å®š
            - è®¡ç®—é”™è¯¯è¯Šæ–­
            - æ€§èƒ½ä¼˜åŒ–
            - å‚æ•°è°ƒä¼˜
            """)
        
        # å®æ—¶ç³»ç»ŸçŠ¶æ€
        st.header("ğŸ–¥ï¸ ç³»ç»ŸçŠ¶æ€")
        
        # Ollamaè¿æ¥çŠ¶æ€
        if ai_assistant.ollama_engine.check_connection():
            st.success("ğŸŸ¢ OllamaæœåŠ¡æ­£å¸¸")
        else:
            st.error("ğŸ”´ OllamaæœåŠ¡å¼‚å¸¸")
        
        # å½“å‰æ¨¡å‹ä¿¡æ¯
        current_model = ai_assistant.ollama_engine.current_model
        st.info(f"ğŸ¤– å½“å‰æ¨¡å‹: {current_model}")
        
        # ç³»ç»Ÿèµ„æºï¼ˆæ¨¡æ‹Ÿï¼‰
        import psutil
        cpu_usage = psutil.cpu_percent()
        memory_usage = psutil.virtual_memory().percent
        
        # åˆ›å»ºä»ªè¡¨ç›˜
        fig = go.Figure()
        
        fig.add_trace(go.Indicator(
            mode="gauge+number",
            value=cpu_usage,
            domain={'x': [0, 0.5], 'y': [0.5, 1]},
            title={'text': "CPU"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "darkblue"},
                   'steps': [{'range': [0, 50], 'color': "lightgray"},
                            {'range': [50, 100], 'color': "gray"}],
                   'threshold': {'line': {'color': "red", 'width': 4},
                               'thickness': 0.75, 'value': 90}}))
        
        fig.add_trace(go.Indicator(
            mode="gauge+number",
            value=memory_usage,
            domain={'x': [0.5, 1], 'y': [0.5, 1]},
            title={'text': "Memory"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "darkgreen"},
                   'steps': [{'range': [0, 50], 'color': "lightgray"},
                            {'range': [50, 100], 'color': "gray"}],
                   'threshold': {'line': {'color': "red", 'width': 4},
                               'thickness': 0.75, 'value': 90}}))
        
        fig.update_layout(height=300, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)

# è¿è¡Œåº”ç”¨
if __name__ == "__main__":
    main()