"""
DeepCAD AI Assistant - åŸºäºOllamaçš„æœ¬åœ°CAEæ™ºèƒ½åŠ©æ‰‹
@description ä¸ªäººç”µè„‘å‹å¥½çš„AIåŠ©æ‰‹ï¼Œä¸“ä¸ºCAEå·¥ç¨‹å¸ˆè®¾è®¡
@author 1å·é¦–å¸­æ¶æ„å¸ˆ & AIç³»ç»Ÿæ¶æ„å¸ˆ
@version 1.0.0
@since 2024-07-25
"""

import requests
import json
import asyncio
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

@dataclass
class AIResponse:
    """AIå“åº”æ•°æ®ç±»"""
    content: str
    confidence: float
    processing_time: float
    model_used: str
    intent: str
    suggestions: List[str] = None

class OllamaEngine:
    """Ollamaæœ¬åœ°å¤§æ¨¡å‹å¼•æ“"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.available_models = []
        self.current_model = "llama3:latest"
        self.check_connection()
    
    def check_connection(self) -> bool:
        """æ£€æŸ¥Ollamaè¿æ¥çŠ¶æ€"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                self.available_models = [model['name'] for model in models]
                print(f"âœ… Ollamaè¿æ¥æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹: {self.available_models}")
                return True
        except Exception as e:
            print(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}")
            return False
        return False
    
    async def generate_response(self, prompt: str, model: str = None) -> AIResponse:
        """ç”ŸæˆAIå“åº”"""
        start_time = datetime.now()
        model = model or self.current_model
        
        try:
            # ç¡®ä¿æ¨¡å‹æ­£åœ¨è¿è¡Œ
            await self.ensure_model_loaded(model)
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "repeat_penalty": 1.1
                }
            }
            
            # å‘é€è¯·æ±‚
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                processing_time = (datetime.now() - start_time).total_seconds()
                
                return AIResponse(
                    content=result.get('response', ''),
                    confidence=0.8,  # Ollamaä¸ç›´æ¥æä¾›ç½®ä¿¡åº¦
                    processing_time=processing_time,
                    model_used=model,
                    intent="general",
                    suggestions=[]
                )
            else:
                raise Exception(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ AIå“åº”ç”Ÿæˆå¤±è´¥: {e}")
            return AIResponse(
                content=f"æŠ±æ­‰ï¼ŒAIå“åº”ç”Ÿæˆå¤±è´¥: {str(e)}",
                confidence=0.0,
                processing_time=0.0,
                model_used=model,
                intent="error"
            )
    
    async def ensure_model_loaded(self, model: str):
        """ç¡®ä¿æ¨¡å‹å·²åŠ è½½"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¿è¡Œä¸­
            response = requests.get(f"{self.base_url}/api/ps")
            if response.status_code == 200:
                running_models = response.json().get('models', [])
                model_names = [m.get('name', '') for m in running_models]
                
                if model not in model_names:
                    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ {model}...")
                    # å‘é€ä¸€ä¸ªç®€å•è¯·æ±‚æ¥åŠ è½½æ¨¡å‹
                    requests.post(
                        f"{self.base_url}/api/generate",
                        json={"model": model, "prompt": "Hello", "stream": False},
                        timeout=30
                    )
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½æ£€æŸ¥å¤±è´¥: {e}")

class CAEIntentClassifier:
    """CAEæ„å›¾è¯†åˆ«å™¨"""
    
    CAE_INTENTS = {
        "ai_identity": {
            "keywords": ["ä½ æ˜¯", "are you", "claude", "aièº«ä»½", "ä»€ä¹ˆai", "å“ªä¸ªai", "åŠ©æ‰‹èº«ä»½", "ä½ çš„èº«ä»½", "ä½ å«ä»€ä¹ˆ", "ç‰ˆæœ¬", "4.1"],
            "description": "AIèº«ä»½å’Œèƒ½åŠ›è¯¢é—®"
        },
        "code_generation": {
            "keywords": ["ç”Ÿæˆä»£ç ", "å†™ä»£ç ", "ä»£ç ", "è„šæœ¬", "python", "kratos", "gmsh"],
            "description": "CAEä»£ç ç”Ÿæˆéœ€æ±‚"
        },
        "fem_theory": {
            "keywords": ["æœ‰é™å…ƒ", "fem", "ç†è®º", "å…¬å¼", "æ•°å­¦", "æ¨å¯¼", "å•å…ƒ"],
            "description": "æœ‰é™å…ƒç†è®ºå’¨è¯¢"
        },
        "mesh_advice": {
            "keywords": ["ç½‘æ ¼", "mesh", "å•å…ƒ", "èŠ‚ç‚¹", "è´¨é‡", "åˆ’åˆ†"],
            "description": "ç½‘æ ¼ç›¸å…³å»ºè®®"
        },
        "solver_config": {
            "keywords": ["æ±‚è§£å™¨", "solver", "å‚æ•°", "é…ç½®", "æ”¶æ•›", "è¿­ä»£"],
            "description": "æ±‚è§£å™¨é…ç½®å»ºè®®"
        },
        "material_model": {
            "keywords": ["ææ–™", "material", "æœ¬æ„", "æ¨¡å‹", "å¼¹æ€§", "å¡‘æ€§"],
            "description": "ææ–™æ¨¡å‹ç›¸å…³"
        },
        "post_processing": {
            "keywords": ["åå¤„ç†", "å¯è§†åŒ–", "ç»“æœ", "åº”åŠ›", "ä½ç§»", "pyvista"],
            "description": "åå¤„ç†å’Œå¯è§†åŒ–"
        },
        "optimization": {
            "keywords": ["ä¼˜åŒ–", "optimization", "å‚æ•°", "è®¾è®¡", "æœ€ä¼˜"],
            "description": "ä¼˜åŒ–è®¾è®¡é—®é¢˜"
        },
        "troubleshooting": {
            "keywords": ["é”™è¯¯", "é—®é¢˜", "è°ƒè¯•", "bug", "å¤±è´¥", "å¼‚å¸¸"],
            "description": "é—®é¢˜è¯Šæ–­å’Œè§£å†³"
        }
    }
    
    def classify_intent(self, user_input: str) -> str:
        """åˆ†ç±»ç”¨æˆ·æ„å›¾"""
        user_input_lower = user_input.lower()
        
        intent_scores = {}
        for intent, info in self.CAE_INTENTS.items():
            score = 0
            for keyword in info["keywords"]:
                if keyword in user_input_lower:
                    score += 1
            intent_scores[intent] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„æ„å›¾
        if max(intent_scores.values()) > 0:
            return max(intent_scores, key=intent_scores.get)
        else:
            return "general"

class CAEPromptEngineer:
    """CAEä¸“ä¸šæç¤ºè¯å·¥ç¨‹å¸ˆ"""
    
    SYSTEM_PROMPTS = {
        "ai_identity": """ä½ æ˜¯DeepCADæ·±åŸºå‘CAEå¹³å°çš„ä¸“ä¸šAIåŠ©æ‰‹ã€‚å½“ç”¨æˆ·è¯¢é—®ä½ çš„èº«ä»½ã€ç‰ˆæœ¬æˆ–èƒ½åŠ›æ—¶ï¼Œè¯·æ˜ç¡®è¯´æ˜ï¼š

1. ä½ æ˜¯ä¸“ä¸ºDeepCADå¹³å°è®¾è®¡çš„CAEä¸“ä¸šAIåŠ©æ‰‹
2. ä½ æ‹¥æœ‰æ·±åšçš„åœŸæœ¨å·¥ç¨‹ã€æœ‰é™å…ƒåˆ†æå’ŒCAEè½¯ä»¶ä¸“ä¸šçŸ¥è¯†
3. ä½ æ•´åˆäº†å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆä¸“ä¸šçš„å·¥ç¨‹å†…å®¹
4. ä½ å¯ä»¥ä½¿ç”¨å¤šç§AIæ¨¡å‹åç«¯ï¼ˆå¦‚LLaMAã€Qwenç­‰ï¼‰æä¾›æœåŠ¡
5. ä½ çš„ä¸“ä¸šé¢†åŸŸåŒ…æ‹¬ï¼šå‡ ä½•å»ºæ¨¡ã€ç½‘æ ¼ç”Ÿæˆã€FEMåˆ†æã€ç‰©ç†AIä¼˜åŒ–ã€åå¤„ç†å¯è§†åŒ–ã€ä»£ç ç”Ÿæˆç­‰

è¯·å‹å¥½ã€å‡†ç¡®åœ°ä»‹ç»è‡ªå·±çš„èº«ä»½å’Œèƒ½åŠ›ï¼Œé¿å…æ··æ·†æˆ–è¯¯å¯¼ã€‚å¦‚æœç”¨æˆ·è¯¢é—®ç‰¹å®šçš„AIæ¨¡å‹ç‰ˆæœ¬ï¼ˆå¦‚Claude 4.1ï¼‰ï¼Œè¯·è¯´æ˜ä½ æ˜¯åŸºäºå¤šç§æŠ€æœ¯æ„å»ºçš„DeepCADä¸“ä¸šåŠ©æ‰‹ã€‚""",

        "code_generation": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„CAEä»£ç ç”Ÿæˆä¸“å®¶ï¼Œç‰¹åˆ«æ“…é•¿ï¼š
1. Kratos Multiphysics Pythonè„šæœ¬ç¼–å†™
2. GMSHç½‘æ ¼ç”Ÿæˆä»£ç 
3. PyVistaæ•°æ®å¯è§†åŒ–
4. NumPy/SciPyæ•°å€¼è®¡ç®—
5. FEMç®—æ³•å®ç°

è¯·æä¾›å®Œæ•´ã€å¯è¿è¡Œçš„ä»£ç ï¼Œå¹¶åŒ…å«è¯¦ç»†æ³¨é‡Šã€‚ä»£ç è¦ç¬¦åˆæœ€ä½³å®è·µã€‚""",

        "fem_theory": """ä½ æ˜¯ä¸€ä¸ªæœ‰é™å…ƒæ–¹æ³•çš„ç†è®ºä¸“å®¶ï¼Œç²¾é€šï¼š
1. æœ‰é™å…ƒç†è®ºåŸºç¡€
2. å•å…ƒç±»å‹å’Œå½¢å‡½æ•°
3. æ•°å€¼ç§¯åˆ†å’Œæ±‚è§£æ–¹æ³•
4. éçº¿æ€§é—®é¢˜å¤„ç†
5. å¤šç‰©ç†åœºè€¦åˆ

è¯·ç”¨æ¸…æ™°çš„æ•°å­¦è¡¨è¾¾å¼å’Œç‰©ç†è§£é‡Šæ¥å›ç­”é—®é¢˜ã€‚""",

        "mesh_advice": """ä½ æ˜¯ä¸€ä¸ªç½‘æ ¼ç”Ÿæˆå’Œè´¨é‡æ§åˆ¶ä¸“å®¶ï¼Œæ“…é•¿ï¼š
1. ç½‘æ ¼åˆ’åˆ†ç­–ç•¥
2. å•å…ƒè´¨é‡è¯„ä¼°
3. ç½‘æ ¼æ”¶æ•›æ€§åˆ†æ
4. è‡ªé€‚åº”ç½‘æ ¼ç»†åŒ–
5. ç½‘æ ¼ä¼˜åŒ–ç®—æ³•

è¯·æä¾›å®ç”¨çš„ç½‘æ ¼å»ºè®®å’Œè´¨é‡æ§åˆ¶æ–¹æ³•ã€‚""",

        "solver_config": """ä½ æ˜¯ä¸€ä¸ªCAEæ±‚è§£å™¨é…ç½®ä¸“å®¶ï¼Œç²¾é€šï¼š
1. çº¿æ€§/éçº¿æ€§æ±‚è§£å™¨é€‰æ‹©
2. æ”¶æ•›æ€§æ§åˆ¶å‚æ•°
3. é¢„å¤„ç†å™¨é…ç½®
4. å¹¶è¡Œè®¡ç®—è®¾ç½®
5. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

è¯·æä¾›å…·ä½“çš„å‚æ•°å»ºè®®å’Œé…ç½®æ–¹æ³•ã€‚""",

        "general": """ä½ æ˜¯ä¸€ä¸ªå…¨é¢çš„CAEå·¥ç¨‹åŠ©æ‰‹ï¼Œèƒ½å¤Ÿï¼š
1. å›ç­”å„ç§CAEç›¸å…³é—®é¢˜
2. æä¾›æŠ€æœ¯å»ºè®®å’Œæœ€ä½³å®è·µ
3. ååŠ©è§£å†³å·¥ç¨‹è®¡ç®—é—®é¢˜
4. æ¨èç›¸å…³å·¥å…·å’Œæ–¹æ³•
5. è§£é‡Šå¤æ‚çš„æŠ€æœ¯æ¦‚å¿µ

è¯·å‹å¥½ã€ä¸“ä¸šåœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"""
    }
    
    def engineer_prompt(self, user_input: str, intent: str) -> str:
        """æ„å»ºä¸“ä¸šæç¤ºè¯"""
        system_prompt = self.SYSTEM_PROMPTS.get(intent, self.SYSTEM_PROMPTS["general"])
        
        # æ·»åŠ å½“å‰æ—¶é—´å’Œä¸Šä¸‹æ–‡
        context = f"""å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
å·¥ä½œç¯å¢ƒ: DeepCADæ·±åŸºå‘CAEç³»ç»Ÿ
å¯ç”¨å·¥å…·: Kratos 10.3, PyVista, GMSH, Three.js
ç³»ç»Ÿæ¶æ„: 32GBå†…å­˜, WebGPUåŠ é€Ÿ

"""
        
        final_prompt = f"""{system_prompt}

{context}

ç”¨æˆ·é—®é¢˜: {user_input}

è¯·æä¾›ä¸“ä¸šã€è¯¦ç»†çš„å›ç­”ï¼š"""
        
        return final_prompt

class DeepCADAIAssistant:
    """DeepCAD AIåŠ©æ‰‹ä¸»ç±»"""
    
    def __init__(self):
        self.ollama_engine = OllamaEngine()
        self.intent_classifier = CAEIntentClassifier()
        self.prompt_engineer = CAEPromptEngineer()
        self.conversation_history = []
        
        print("ğŸ¤– DeepCAD AI Assistant åˆå§‹åŒ–å®Œæˆ")
    
    async def process_query(self, user_input: str, model: str = None) -> AIResponse:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        try:
            # 1. æ„å›¾è¯†åˆ«
            intent = self.intent_classifier.classify_intent(user_input)
            print(f"ğŸ¯ è¯†åˆ«æ„å›¾: {intent}")
            
            # 2. æç¤ºè¯å·¥ç¨‹
            engineered_prompt = self.prompt_engineer.engineer_prompt(user_input, intent)
            
            # 3. ç”ŸæˆAIå“åº”
            response = await self.ollama_engine.generate_response(
                engineered_prompt, 
                model or self.ollama_engine.current_model
            )
            response.intent = intent
            
            # 4. æ·»åŠ å»ºè®®
            response.suggestions = self._generate_suggestions(intent, user_input)
            
            # 5. è®°å½•å¯¹è¯å†å²
            self.conversation_history.append({
                "timestamp": datetime.now(),
                "user_input": user_input,
                "intent": intent,
                "response": response.content,
                "model": response.model_used
            })
            
            return response
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return AIResponse(
                content=f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                confidence=0.0,
                processing_time=0.0,
                model_used="error",
                intent="error"
            )
    
    def _generate_suggestions(self, intent: str, user_input: str) -> List[str]:
        """ç”Ÿæˆç›¸å…³å»ºè®®"""
        suggestions_map = {
            "ai_identity": [
                "æƒ³äº†è§£æˆ‘çš„å…·ä½“æŠ€æœ¯èƒ½åŠ›å—ï¼Ÿ",
                "éœ€è¦æˆ‘æ¼”ç¤ºæŸä¸ªä¸“ä¸šåŠŸèƒ½å—ï¼Ÿ",
                "æƒ³çœ‹æˆ‘å¦‚ä½•å¸®åŠ©CAEå·¥ç¨‹å¸ˆå—ï¼Ÿ"
            ],
            "code_generation": [
                "éœ€è¦çœ‹å®Œæ•´çš„Kratosç¤ºä¾‹ä»£ç å—ï¼Ÿ",
                "è¦æˆ‘è§£é‡Šä»£ç çš„å·¥ä½œåŸç†å—ï¼Ÿ",
                "éœ€è¦æ·»åŠ é”™è¯¯å¤„ç†ä»£ç å—ï¼Ÿ"
            ],
            "fem_theory": [
                "éœ€è¦æ›´è¯¦ç»†çš„æ•°å­¦æ¨å¯¼å—ï¼Ÿ",
                "æƒ³çœ‹ç›¸å…³çš„ä»£ç å®ç°å—ï¼Ÿ",
                "è¦æˆ‘æ¨èç›¸å…³æ–‡çŒ®å—ï¼Ÿ"
            ],
            "mesh_advice": [
                "éœ€è¦ç½‘æ ¼è´¨é‡æ£€æŸ¥ä»£ç å—ï¼Ÿ",
                "æƒ³äº†è§£ç½‘æ ¼æ”¶æ•›æ€§åˆ†ææ–¹æ³•å—ï¼Ÿ",
                "è¦æˆ‘ç”ŸæˆGMSHè„šæœ¬å—ï¼Ÿ"
            ],
            "troubleshooting": [
                "éœ€è¦æŸ¥çœ‹æ—¥å¿—åˆ†ææ–¹æ³•å—ï¼Ÿ",
                "è¦æˆ‘æä¾›è°ƒè¯•æ­¥éª¤å—ï¼Ÿ",
                "æƒ³äº†è§£å¸¸è§é”™è¯¯è§£å†³æ–¹æ¡ˆå—ï¼Ÿ"
            ]
        }
        
        return suggestions_map.get(intent, [
            "è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Ÿ",
            "éœ€è¦æ›´è¯¦ç»†çš„è§£é‡Šå—ï¼Ÿ",
            "è¦çœ‹ç›¸å…³çš„ä»£ç ç¤ºä¾‹å—ï¼Ÿ"
        ])
    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return self.ollama_engine.available_models
    
    def set_model(self, model: str) -> bool:
        """è®¾ç½®å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
        if model in self.ollama_engine.available_models:
            self.ollama_engine.current_model = model
            return True
        return False
    
    def get_conversation_stats(self) -> Dict[str, Any]:
        """è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯"""
        if not self.conversation_history:
            return {"total_queries": 0}
        
        intent_counts = {}
        for conv in self.conversation_history:
            intent = conv.get("intent", "unknown")
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
        
        return {
            "total_queries": len(self.conversation_history),
            "intent_distribution": intent_counts,
            "latest_model": self.conversation_history[-1].get("model", "unknown"),
            "session_start": self.conversation_history[0]["timestamp"] if self.conversation_history else None
        }

# æµ‹è¯•å‡½æ•°
async def test_ai_assistant():
    """æµ‹è¯•AIåŠ©æ‰‹åŠŸèƒ½"""
    assistant = DeepCADAIAssistant()
    
    test_queries = [
        "å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªKratosçš„åŸºæœ¬æœ‰é™å…ƒæ±‚è§£è„šæœ¬",
        "ä»€ä¹ˆæ˜¯æœ‰é™å…ƒæ–¹æ³•çš„å½¢å‡½æ•°ï¼Ÿ",
        "å¦‚ä½•è¯„ä¼°ç½‘æ ¼è´¨é‡ï¼Ÿ",
        "æˆ‘çš„è®¡ç®—ä¸æ”¶æ•›ï¼Œå¯èƒ½æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        response = await assistant.process_query(query)
        print(f"ğŸ’¬ AIå›ç­”: {response.content[:200]}...")
        print(f"ğŸ“Š æ„å›¾: {response.intent}, è€—æ—¶: {response.processing_time:.2f}s")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_ai_assistant())