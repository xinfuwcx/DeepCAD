"""
DeepCAD AI Assistant - ç®€åŒ–ç‰ˆUI
åŸºäºOllamaçš„æœ¬åœ°CAEæ™ºèƒ½åŠ©æ‰‹
"""

import streamlit as st
import requests
import json
import asyncio
import time
from datetime import datetime

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ğŸ¤– DeepCAD AI Assistant",
    page_icon="ğŸ¤–",
    layout="wide"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .main-title {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    
    .user-message {
        background-color: #f0f2f6;
        border-left: 4px solid #ff6b6b;
    }
    
    .assistant-message {
        background-color: #e8f4f8;
        border-left: 4px solid #4ecdc4;
    }
</style>
""", unsafe_allow_html=True)

class SimpleOllamaClient:
    """ç®€åŒ–çš„Ollamaå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.available_models = []
        self.check_connection()
    
    def check_connection(self):
        """æ£€æŸ¥è¿æ¥"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                self.available_models = [model['name'] for model in models]
                return True
        except:
            pass
        return False
    
    def generate_response(self, prompt, model="llama3:latest"):
        """ç”Ÿæˆå“åº”"""
        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
            else:
                return f"è¯·æ±‚å¤±è´¥: {response.status_code}"
                
        except Exception as e:
            return f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}"

def classify_cae_intent(user_input):
    """ç®€å•çš„CAEæ„å›¾åˆ†ç±»"""
    intents = {
        "code_generation": ["ä»£ç ", "ç”Ÿæˆ", "è„šæœ¬", "python", "kratos", "gmsh", "pyvista"],
        "fem_theory": ["æœ‰é™å…ƒ", "fem", "ç†è®º", "å…¬å¼", "æ•°å­¦", "æ¨å¯¼"],
        "mesh_advice": ["ç½‘æ ¼", "mesh", "å•å…ƒ", "èŠ‚ç‚¹", "è´¨é‡", "åˆ’åˆ†"],
        "solver_config": ["æ±‚è§£å™¨", "solver", "å‚æ•°", "é…ç½®", "æ”¶æ•›"],
        "optimization": ["ä¼˜åŒ–", "optimization", "å‚æ•°", "è®¾è®¡"],
        "troubleshooting": ["é”™è¯¯", "é—®é¢˜", "è°ƒè¯•", "å¤±è´¥", "å¼‚å¸¸"],
        "visualization": ["å¯è§†åŒ–", "visualization", "æ˜¾ç¤º", "å›¾å½¢", "ç»“æœ"]
    }
    
    user_lower = user_input.lower()
    for intent, keywords in intents.items():
        if any(keyword in user_lower for keyword in keywords):
            return intent
    return "general"

def create_cae_prompt(user_input, intent):
    """åˆ›å»ºCAEä¸“ä¸šæç¤ºè¯"""
    system_prompts = {
        "code_generation": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„CAEä»£ç ç”Ÿæˆä¸“å®¶ã€‚è¯·å¸®åŠ©ç”¨æˆ·ç”Ÿæˆå®Œæ•´ã€å¯è¿è¡Œçš„ä»£ç ï¼Œç‰¹åˆ«æ“…é•¿ï¼š
- Kratos Multiphysics Pythonè„šæœ¬ç¼–å†™
- GMSHç½‘æ ¼ç”Ÿæˆä»£ç 
- PyVistaæ•°æ®å¯è§†åŒ–ä»£ç 
- NumPy/SciPyæ•°å€¼è®¡ç®—ä»£ç 

è¯·æä¾›ä»£ç å¹¶åŒ…å«è¯¦ç»†æ³¨é‡Šã€‚""",

        "fem_theory": """ä½ æ˜¯ä¸€ä¸ªæœ‰é™å…ƒæ–¹æ³•çš„ç†è®ºä¸“å®¶ã€‚è¯·ç”¨æ¸…æ™°çš„è§£é‡Šæ¥å›ç­”é—®é¢˜ï¼š
- æœ‰é™å…ƒç†è®ºåŸºç¡€
- å•å…ƒç±»å‹å’Œå½¢å‡½æ•°
- æ•°å€¼ç§¯åˆ†å’Œæ±‚è§£æ–¹æ³•
- éçº¿æ€§é—®é¢˜å¤„ç†

è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µã€‚""",

        "mesh_advice": """ä½ æ˜¯ä¸€ä¸ªç½‘æ ¼ç”Ÿæˆä¸“å®¶ã€‚è¯·æä¾›ä¸“ä¸šçš„ç½‘æ ¼å»ºè®®ï¼š
- ç½‘æ ¼åˆ’åˆ†ç­–ç•¥
- å•å…ƒè´¨é‡è¯„ä¼°
- ç½‘æ ¼æ”¶æ•›æ€§åˆ†æ
- ç½‘æ ¼ä¼˜åŒ–æ–¹æ³•

è¯·ç»™å‡ºå®ç”¨çš„å»ºè®®å’Œæœ€ä½³å®è·µã€‚""",

        "solver_config": """ä½ æ˜¯ä¸€ä¸ªCAEæ±‚è§£å™¨ä¸“å®¶ã€‚è¯·æä¾›æ±‚è§£å™¨é…ç½®å»ºè®®ï¼š
- çº¿æ€§/éçº¿æ€§æ±‚è§£å™¨é€‰æ‹©
- æ”¶æ•›æ€§æ§åˆ¶å‚æ•°
- é¢„å¤„ç†å™¨é…ç½®
- æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

è¯·æä¾›å…·ä½“çš„å‚æ•°å»ºè®®å’Œé…ç½®æ–¹æ³•ã€‚""",

        "optimization": """ä½ æ˜¯ä¸€ä¸ªå·¥ç¨‹ä¼˜åŒ–ä¸“å®¶ã€‚è¯·æä¾›ä¼˜åŒ–è®¾è®¡å»ºè®®ï¼š
- ä¼˜åŒ–ç®—æ³•é€‰æ‹©
- è®¾è®¡å˜é‡å®šä¹‰
- çº¦æŸæ¡ä»¶è®¾ç½®
- å¤šç›®æ ‡ä¼˜åŒ–ç­–ç•¥

è¯·ç»™å‡ºç³»ç»Ÿæ€§çš„ä¼˜åŒ–æ–¹æ³•ã€‚""",

        "troubleshooting": """ä½ æ˜¯ä¸€ä¸ªCAEé—®é¢˜è¯Šæ–­ä¸“å®¶ã€‚è¯·å¸®åŠ©åˆ†æå’Œè§£å†³é—®é¢˜ï¼š
- æ”¶æ•›æ€§é—®é¢˜è¯Šæ–­
- é”™è¯¯ä¿¡æ¯è§£è¯»
- è°ƒè¯•æ–¹æ³•æŒ‡å¯¼
- è§£å†³æ–¹æ¡ˆæ¨è

è¯·æä¾›ç³»ç»Ÿæ€§çš„é—®é¢˜è§£å†³æ€è·¯ã€‚""",

        "visualization": """ä½ æ˜¯ä¸€ä¸ªç§‘å­¦å¯è§†åŒ–ä¸“å®¶ã€‚è¯·æä¾›å¯è§†åŒ–å»ºè®®ï¼š
- PyVistaå¯è§†åŒ–æŠ€æœ¯
- ç»“æœåå¤„ç†æ–¹æ³•
- å›¾è¡¨è®¾è®¡åŸåˆ™
- äº¤äº’å¼å¯è§†åŒ–

è¯·æä¾›å®ç”¨çš„å¯è§†åŒ–ä»£ç å’ŒæŠ€å·§ã€‚""",

        "general": """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„CAEå·¥ç¨‹åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”å„ç§CAEç›¸å…³é—®é¢˜å¹¶æä¾›ä¸“ä¸šå»ºè®®ã€‚è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ã€‚"""
    }
    
    system_prompt = system_prompts.get(intent, system_prompts["general"])
    
    return f"""{system_prompt}

å½“å‰ç¯å¢ƒ: DeepCADæ·±åŸºå‘CAEç³»ç»Ÿ
å¯ç”¨å·¥å…·: Kratos 10.3, PyVista, GMSH, Three.js, WebGPUä¼˜åŒ–

ç”¨æˆ·é—®é¢˜: {user_input}

è¯·æä¾›ä¸“ä¸šã€è¯¦ç»†çš„å›ç­”ï¼š"""

def main():
    """ä¸»ç•Œé¢"""
    
    # æ ‡é¢˜
    st.markdown("""
    <div class="main-title">
        <h1>ğŸ¤– DeepCAD AI Assistant</h1>
        <p>Your Intelligent CAE Companion - Powered by Ollama</p>
    </div>
    """, unsafe_allow_html=True)
    
    # åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯
    if 'ollama_client' not in st.session_state:
        st.session_state.ollama_client = SimpleOllamaClient()
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("ğŸ› ï¸ æ§åˆ¶é¢æ¿")
        
        # è¿æ¥çŠ¶æ€
        client = st.session_state.ollama_client
        if client.check_connection():
            st.success("ğŸŸ¢ Ollamaè¿æ¥æ­£å¸¸")
            
            # æ¨¡å‹é€‰æ‹©
            if client.available_models:
                selected_model = st.selectbox(
                    "é€‰æ‹©AIæ¨¡å‹",
                    client.available_models,
                    index=0
                )
            else:
                st.warning("âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°å¯ç”¨æ¨¡å‹")
                selected_model = "llama3:latest"
        else:
            st.error("ğŸ”´ Ollamaè¿æ¥å¤±è´¥")
            st.info("è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ")
            selected_model = "llama3:latest"
        
        st.divider()
        
        # CAEå¿«é€Ÿå·¥å…·
        st.header("ğŸ”§ CAEå¿«é€Ÿå·¥å…·")
        
        quick_prompts = {
            "ğŸ—ï¸ Kratosè„šæœ¬": "å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªåŸºæœ¬çš„Kratosæœ‰é™å…ƒæ±‚è§£è„šæœ¬",
            "ğŸ•¸ï¸ ç½‘æ ¼è´¨é‡": "å¦‚ä½•æ£€æŸ¥å’Œä¼˜åŒ–æœ‰é™å…ƒç½‘æ ¼è´¨é‡ï¼Ÿ",
            "ğŸ“Š ç»“æœå¯è§†åŒ–": "å¸®æˆ‘ç”ŸæˆPyVistaç»“æœå¯è§†åŒ–ä»£ç ",
            "ğŸ” æ”¶æ•›è¯Šæ–­": "æˆ‘çš„CAEè®¡ç®—ä¸æ”¶æ•›ï¼Œå¦‚ä½•è¯Šæ–­é—®é¢˜ï¼Ÿ",
            "âš™ï¸ æ±‚è§£å™¨é…ç½®": "å¦‚ä½•é…ç½®Kratosæ±‚è§£å™¨å‚æ•°ï¼Ÿ",
            "ğŸ¯ å‚æ•°ä¼˜åŒ–": "å¦‚ä½•è¿›è¡ŒCAEå‚æ•°ä¼˜åŒ–è®¾è®¡ï¼Ÿ"
        }
        
        for button_text, prompt in quick_prompts.items():
            if st.button(button_text):
                st.session_state.quick_prompt = prompt
        
        st.divider()
        
        # ä¼šè¯ç»Ÿè®¡
        st.header("ğŸ“Š ä¼šè¯ä¿¡æ¯")
        if 'messages' in st.session_state:
            message_count = len([m for m in st.session_state.messages if m['role'] == 'user'])
            st.metric("å¯¹è¯è½®æ•°", message_count)
        
        st.info(f"å½“å‰æ¨¡å‹: {selected_model}")
    
    # ä¸»èŠå¤©åŒºåŸŸ
    st.header("ğŸ’¬ AIå¯¹è¯")
    
    # åˆå§‹åŒ–æ¶ˆæ¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant",
                "content": """ä½ å¥½ï¼æˆ‘æ˜¯DeepCAD AIåŠ©æ‰‹ ğŸ¤–

æˆ‘å¯ä»¥å¸®åŠ©ä½ ï¼š
- ğŸ”§ ç”ŸæˆCAEä»£ç  (Kratos, GMSH, PyVista)
- ğŸ“š è§£ç­”æœ‰é™å…ƒç†è®ºé—®é¢˜
- ğŸ•¸ï¸ æä¾›ç½‘æ ¼åˆ’åˆ†å»ºè®®
- ğŸ” è¯Šæ–­è®¡ç®—é—®é¢˜
- ğŸ“Š åˆ›å»ºæ•°æ®å¯è§†åŒ–
- âš™ï¸ ä¼˜åŒ–æ±‚è§£å™¨é…ç½®

æœ‰ä»€ä¹ˆCAEé—®é¢˜å°½ç®¡é—®æˆ‘ï¼""",
                "intent": "greeting",
                "timestamp": datetime.now()
            }
        ]
    
    # æ˜¾ç¤ºæ¶ˆæ¯å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # æ˜¾ç¤ºAIå“åº”çš„å…ƒä¿¡æ¯
            if message["role"] == "assistant" and "intent" in message:
                col1, col2 = st.columns(2)
                with col1:
                    st.caption(f"ğŸ¯ æ„å›¾: {message.get('intent', 'unknown')}")
                with col2:
                    if "processing_time" in message:
                        st.caption(f"â±ï¸ è€—æ—¶: {message['processing_time']:.1f}s")
    
    # å¤„ç†å¿«é€Ÿæç¤º
    user_input = None
    if 'quick_prompt' in st.session_state:
        user_input = st.session_state.quick_prompt
        del st.session_state.quick_prompt
    
    # ç”¨æˆ·è¾“å…¥
    if not user_input:
        user_input = st.chat_input("è¾“å…¥ä½ çš„CAEé—®é¢˜...")
    
    if user_input:
        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({
            "role": "user",
            "content": user_input,
            "timestamp": datetime.now()
        })
        
        # ç”ŸæˆAIå“åº”
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– AIæ­£åœ¨æ€è€ƒä¸­..."):
                start_time = time.time()
                
                # åˆ†ç±»æ„å›¾
                intent = classify_cae_intent(user_input)
                
                # åˆ›å»ºä¸“ä¸šæç¤ºè¯
                prompt = create_cae_prompt(user_input, intent)
                
                # ç”Ÿæˆå“åº”
                response = client.generate_response(prompt, selected_model)
                
                processing_time = time.time() - start_time
                
                # æ˜¾ç¤ºå“åº”
                st.markdown(response)
                
                # æ˜¾ç¤ºå…ƒä¿¡æ¯
                col1, col2 = st.columns(2)
                with col1:
                    st.caption(f"ğŸ¯ æ„å›¾: {intent}")
                with col2:
                    st.caption(f"â±ï¸ è€—æ—¶: {processing_time:.1f}s")
                
                # æ·»åŠ AIå“åº”åˆ°å†å²
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "intent": intent,
                    "processing_time": processing_time,
                    "timestamp": datetime.now()
                })
        
        # è‡ªåŠ¨åˆ·æ–°ä»¥æ˜¾ç¤ºæ–°æ¶ˆæ¯
        st.rerun()
    
    # é¡µé¢åº•éƒ¨ä¿¡æ¯
    st.divider()
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.info("ğŸ”— åŸºäºOllamaæœ¬åœ°éƒ¨ç½²")
    with col2:
        st.info("ğŸ¯ ä¸“ä¸ºCAEå·¥ç¨‹å¸ˆè®¾è®¡") 
    with col3:
        st.info("ğŸš€ é›†æˆDeepCADä¼˜åŒ–ç³»ç»Ÿ")

if __name__ == "__main__":
    main()